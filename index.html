<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=EB Garamond:300,300italic,400,400italic,700,700italic|Cinzel Decorative:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wjykl22.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
<meta property="og:type" content="website">
<meta property="og:title" content="韭零后">
<meta property="og:url" content="https://wjykl22.github.io/index.html">
<meta property="og:site_name" content="韭零后">
<meta property="og:description" content="世界上有两样东西不可直视，一是太阳，二是人心">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Jiyang">
<meta property="article:tag" content="keywords">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wjykl22.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>
<link rel="stylesheet" type="text/css" href="/css/injector.css" />
  <title>韭零后</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="韭零后" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">韭零后</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">公元1996 - ?</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-目录">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>目录</a>

  </li>
        <li class="menu-item menu-item-结构">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>结构</a>

  </li>
        <li class="menu-item menu-item-站点地图">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-公益">

    <a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/88211b35.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/88211b35.html" class="post-title-link" itemprop="url">分布式机器学习量化通信综述</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-31 16:03:49" itemprop="dateCreated datePublished" datetime="2020-07-31T16:03:49+08:00">2020-07-31</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-24 21:40:43" itemprop="dateModified" datetime="2020-09-24T21:40:43+08:00">2020-09-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/88211b35.html" class="post-meta-item leancloud_visitors" data-flag-title="分布式机器学习量化通信综述" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/88211b35.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/88211b35.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>18 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          此文加密，请输入密码
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/archives/88211b35.html#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/2932556.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/2932556.html" class="post-title-link" itemprop="url">A Survey on Methods and Theories of Quantized Neural Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-29 14:30:28" itemprop="dateCreated datePublished" datetime="2020-07-29T14:30:28+08:00">2020-07-29</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-31 21:08:17" itemprop="dateModified" datetime="2020-07-31T21:08:17+08:00">2020-07-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/2932556.html" class="post-meta-item leancloud_visitors" data-flag-title="A Survey on Methods and Theories of Quantized Neural Networks" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/2932556.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/2932556.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>当下流行的深度神经网络具有非常复杂的结构，训练时需要消耗大量的内存和电源；在移动端和边缘设备等资源限制的情况下难以发挥作用，而量化则是解决上述问题的办法之一。原来的神经网络权重、激活和梯度都需要采用<span class="math inline">\(32bit\)</span>精度的浮点表示，但是采用量化表示只需要整型或者二进制即可，大大减少了模型尺寸和资源消耗。这是一篇综述，从不同方面给出了量化神经网络的一些方法，同时也罗列了目前在这些方面遇到的挑战。</p>
<h2 id="介绍">介绍</h2>
<h3 id="神经网络">神经网络</h3>
<p>本文介绍了下面几种神经网络，比较基础，这里就不做赘述</p>
<h4 id="前馈神经网络">前馈神经网络</h4>
<h4 id="卷积神经网络">卷积神经网络</h4>
<p>值得一提的是以下这些卷积神经网络结构：</p>
<ul>
<li>AlexNet[Krizhevsky et al., 2012<a href="#refer-anchor-1"><sup>1</sup></a>]</li>
<li>VGGNet[Simonyan and Zisserman, 2014<a href="#refer-anchor-2"><sup>2</sup></a>]</li>
<li>GoogleNet[Szegedy et al., 2015<a href="#refer-anchor-3"><sup>3</sup></a>]</li>
<li>ResNet[He et al., 2016a<a href="#refer-anchor-4"><sup>4</sup></a>]</li>
</ul>
<p>这四个架构非常广泛地用在比较不同压缩和量化方法性能比较实验过程中，常常作为基准（baseline）。</p>
<h4 id="循环神经网络和lstm">循环神经网络和LSTM</h4>
<h3 id="量化神经网络">量化神经网络</h3>
<h4 id="术语介绍">术语介绍</h4>
<ul>
<li><strong>低精度</strong>（Low precision）：可能是最通用的概念。常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；低精度则表示 FP16（半精度浮点），INT8（8位的定点整数）等等数值格式。不过目前低精度往往指代 INT8。</li>
<li><strong>混合精度</strong>（Mixed precision）在模型中使用 FP32 和 FP16 。 FP16 减少了一半的内存大小，但有些参数或操作符必须采用 FP32 格式才能保持准确度。如果您对该主题感兴趣，请查看 <a href="https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/" rel="external nofollow noreferrer">Mixed-Precision Training of Deep Neural Networks</a> 。</li>
<li><strong>量化</strong>一般指 INT8 。不过，根据存储一个权重元素所需的位数，还可以包括：
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.02830" rel="external nofollow noreferrer">二值神经网络</a>：在运行时权重和激活只取两种值（例如 +1，-1）的神经网络，以及在训练时计算参数的梯度。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.04711" rel="external nofollow noreferrer">三元权重网络</a>：权重约束为+1,0和-1的神经网络。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05279" rel="external nofollow noreferrer">XNOR网络</a>：过滤器和卷积层的输入是二进制的。 XNOR 网络主要使用二进制运算来近似卷积。</li>
</ul></li>
</ul>
<h4 id="正文">正文</h4>
<p>目前，很多技术用在了量化神经网络方面。粗略地来看可以分成确定性量化和随机量化。在确定量化中，在量化值和真实值之间有一一对应的映射，而随机量化权重，激活和梯度则是离散分布。量化值是从离散分布中采样得到的。</p>
<p>在神经网络中有三个部分是可以进行量化的：权重、激活和梯度。量化这些部分的动机和方法是不同的。量化权重和激活层，我们得到更小的模型尺寸。在分布式训练的花镜中，我们能够通过量化梯度的方式节省通信消耗。一般来说，量化梯度比量化权重和激活更加困难，因为训练往往需要精度更高的梯度来保证算法的收敛。</p>
<p>我们通常采用编码本（codebook）来表示代表真实值的离散值。从密码本的表示来看，现有的工作可以将量化神经网络粗略的分成两类：固定编码本量化和自适应编码本量化。</p>
<p>在固定编码本量化中，权重经常被量化成提前定义好的编码，二自适应编码本是从数据中学习而来。一些普遍应用的密码本包括<span class="math inline">\(\{-1,1\}\)</span>，<span class="math inline">\(\{-1,0,1\}\)</span>或者二数幂或者二进制网络和三元权重网络等。</p>
<p>训练量化模型需要不断调整，而且量化网络并不容易理解，寻找新的量化方法以及配合理论分析是量化神经网络非常重要的一点。</p>
<h2 id="量化技术">量化技术</h2>
<h3 id="确定性量化">确定性量化</h3>
<h4 id="取整rounding">取整（Rounding）</h4>
<h5 id="主要内容">主要内容</h5>
<p>取证可能是对真实值最简单的量化，例如[Courbariaux et al., 2015]提出下面这种取整方法： <span class="math display">\[
x^{b}=\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1 &amp; x \geq 0 \\
-1 &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中<span class="math inline">\(x^b\)</span>表示二进制量，<span class="math inline">\(x\)</span>是真实量。这个方法可以应用在量化权重，激活和梯度中。在前向传播中，真实值权重能够产生输出。然而，在反向传播过程中，我们不能够通过<code>Sign(x)</code>来进行，因为它是离散的，到处都是梯度为零。通常采用的方法是“直通估计（straight through estimator）”（STE）[Hinton et al., 2012b]，它采用启发式的方法估计随机神经元的梯度。假设<span class="math inline">\(E\)</span>是损失函数，STE的前向和反向计算可以看成如下方式： <span class="math display">\[
\begin{array}{l}
\text { Forward: } \quad x^{b}=\operatorname{Sign}(x) \\
\text { Backward: } \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}} \mathrm{I}_{|x| \leq 1}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{I}_{|x| \leq 1}\)</span>是定义如下的指示函数： <span class="math display">\[
\mathrm{I}_{|x| \leq 1}=\left\{\begin{array}{ll}
1 &amp; |x| \leq 1 \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 为了对双精度进行取整，[Gupta et al., 2015]作者提出了如下的取整方式： <span class="math display">\[
\operatorname{Round}(x,[\mathrm{IL}, \mathrm{FL}])=\left\{\begin{array}{ll}
\lfloor x\rfloor &amp; \text { if }\lfloor x\rfloor \leq x \leq\lfloor x\rfloor+\frac{\epsilon}{2} \\
\lfloor x\rfloor+\epsilon &amp; \text { if }\lfloor x\rfloor+\frac{\epsilon}{2}&lt;x \leq\lfloor x\rfloor+\epsilon
\end{array}\right.
\]</span> 在固定点表达中，IL代表整数位的个数，FL表示分数位的个数。<span class="math inline">\(\epsilon\)</span>表示在固定点表达中能够表达的最小正数。<span class="math inline">\(\lfloor x\rfloor\)</span>被定义为<span class="math inline">\(\epsilon\)</span>的最大整数倍。对于超出此固定点格式范围的值，作者将它们规范化为固定点表示的下界或上界[Rastegari et al., 2016]。将上式扩展： <span class="math display">\[
\begin{array}{ll}
\text { Forward: } &amp; x^{b}=\operatorname{Sign}(x) \times \mathrm{E}_{F}(|x|) \\
\text { Backward: } &amp; \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{E}_{F}(|x|)\)</span>表示每个输出通道的权值绝对值的平均值。</p>
<p>近期[Polino et al., 2018]提出了更加普遍的舍入函数： <span class="math display">\[
Q(x)=s c^{-1}(\hat{Q}(s c(x)))
\]</span> 其中<span class="math inline">\(sc(x)\)</span>是将值从任意范围缩放到<span class="math inline">\([0,1]\)</span>的缩放函数。<span class="math inline">\(\hat{Q}(x)\)</span>是实际的量化函数。给出量化等级参数<span class="math inline">\(s\)</span>，有<span class="math inline">\(s+1\)</span>等级的统一量化函数可以定义为： <span class="math display">\[
\hat{Q}(x, s)=\frac{\lfloor x s\rfloor}{s}+\frac{\xi}{s}
\]</span> 其中 <span class="math display">\[
\xi=\left\{\begin{array}{ll}
1 &amp; x s-\lfloor x s\rfloor&gt;\frac{1}{2} \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 这个量化函数的直觉是将<span class="math inline">\(x\)</span>分配到在<span class="math inline">\([0,1]\)</span>范围内<span class="math inline">\(s-1\)</span>个等间隔最接近的量化点。这是符号<span class="math inline">\(Sign(x)\)</span>函数的广义版本，能够将实值量化成多层。在[Shuang et al., 2018]中，作者提出了启发式摄入函数来量化一个市值为<span class="math inline">\(k\)</span>位的整数。 <span class="math display">\[
Q(x, k)=\operatorname{Clip}\left\{\sigma(k) \cdot \operatorname{round}\left[\frac{x}{\sigma(k)}\right],-1+\sigma(k), 1-\sigma(k)\right\}
\]</span> 想法是将真实值利用统一的距离<span class="math inline">\(\sigma(k)\)</span>进行量化，其中<span class="math inline">\(\sigma(k)=2^{1-k}\)</span>。<span class="math inline">\(Clip\)</span>将量化限制在<span class="math inline">\([-1+\sigma(k), 1-\sigma(k)]\)</span>范围内，<span class="math inline">\(round\)</span>用最近的离散点替换连续值。</p>
<h5 id="挑战">挑战</h5>
<p>挑战:使用四舍五入函数是将实值转换为量化值的简单方法。然而，每次四舍五入操作之后，网络性能可能会急剧下降。在训练过程中需要保持真实值作为参考，这会增加记忆开销。同时，由于使用离散值时参数空间要小得多，训练过程难以收敛。最后，舍入运算不能充分利用网络中权值的结构信息。</p>
<h4 id="向量量化">向量量化</h4>
<p>[Gong et al., 2014]是第一篇将向量量化考虑到神经网络压缩和量化中的。他主要的思想是将权重分组聚类，在推理时采用聚类中心代表每个组实际的权重。</p>
<p>[Han et al., 2015]，[Gong et al., 2014]等都对这种方式进行了改进，[Choi et al., 2016]指出这种方法有两个缺点，第一是不能够控制由于<code>k-means</code>算法造成的损失；第二是<code>k-means</code>算法不施加任何压缩比约束。为了解决这些问题，作者提出了一种Hessian加权k均值聚类方法。其基本思想是使用Hessian Weighted失真来测量因权值量化而导致的性能退化。这样可以防止那些对网络性能有较大影响的权值与原始值偏离太多。</p>
<p>有很多对向量量化的扩展方法，乘积量化[Gong et al., 2014]是一种将权重矩阵划分为许多不相交的子矩阵，并对每个子矩阵进行量化的方法。在[Wu et al., 2016]中，作者采用带误差修正的产品量化方法对网络参数进行量化，实现快速训练和测试。残差量化[Gong et al., 2014]将向量量化到k个聚类中，然后递归量化残差。在[Park et al., 2017]中，作者采用了类似于矢量量化的方法。他们使用了一个基于权重熵的想法[Guias¸u, 1971]来将权重分组到N个簇中。对于重要的权重范围有更多的簇。从而实现了自动灵活的多比特量化。</p>
<h5 id="挑战-1">挑战</h5>
<p>由于网络中权值的数量，k-means聚类的计算量很大。与四舍五入法相比，用向量化方法来实现二值权值比较困难。向量量化通常用于对预先训练的模型进行量化。因此，如果任务是从头训练量化网络，最好使用精心设计的四舍五入函数。向量量化忽略了网络的局部信息。</p>
<h4 id="量化最优化">量化最优化</h4>
<p>简单来说就是将量化作为最优化问题进行，此处暂时省略...</p>
<h3 id="随机量化">随机量化</h3>
<h4 id="随机舍入法random-rounding">随机舍入法（Random Rounding）</h4>
<p>在随机舍入法中，真实值和量化值有着一对一的对应。典型地，量化值的权重是从离散分布中采样而来，它是通过真实值进行参数化的。例如，[Courbariaux et al., 2015]提出了以下的随机近似方法： <span class="math display">\[
x^{b}=\left\{\begin{array}{ll}+1 &amp; \text { with probability } p=\sigma(x) \\ -1 &amp; \text { with probability } 1-p\end{array}\right.
\]</span> 其中<span class="math inline">\(\sigma\)</span>表示“hard sigmoid”函数 <span class="math display">\[
\sigma(x)=\operatorname{clip}\left(\frac{x+1}{2}, 0,1\right)=\max \left(0, \min \left(1, \frac{x+1}{2}\right)\right)
\]</span> 直观上来说，<span class="math inline">\(x\)</span>是一个正值，我们将以很高的概率量化到<span class="math inline">\(+1\)</span>，其他情况量化到<span class="math inline">\(-1\)</span>。这就给我们更加灵活的量化模式。在[Muller and Indi-veri, 2015]中，作者在整数规划中使用了这种思想。提出的随机四舍五入函数将每个实值概率映射到最近的离散点或第二最近的离散点，这取决于到对应点的距离。在[Lin et al., 2015]中，将二进随机四舍五入扩展到三元情形。</p>
<h5 id="挑战-2">挑战</h5>
<p>随机舍入提供了一种将噪声注入训练过程的方法。它可以作为一个正则化器和可提供条件计算。然而，使用随机的舍入方法，我们需要估计离散神经元的梯度。这样的估计往往有很高的方差。这一事实可能会导致训练过程中损失函数的振荡。[Bengio等人，2013]的工作提供了估计离散中性电子梯度的可能解决方案的概述。</p>
<h4 id="概率量化">概率量化</h4>
<p>暂无</p>
<h4 id="讨论">讨论</h4>
<p>上述量化技术使我们能够从不同的角度对网络进行量化。这些技术的优缺点可以指导我们在不同的情况下选择合适的技术。一般来说，如果我们想为硬件加速量化神经网络，确定性量化应该是首选，因为我们可以预先指定适当的量化级别，以便在专用硬件上运行量化的网络工作。这可以提高硬件的预测性能。四舍五入使我们能够以数据依赖的方式量化权重。这导致了条件计算[Bengio et al.， 2013]，可以增加神经网络的容量。概率量化与确定性量化的不同之处在于量化后的权重更具有可解释性。我们可以用概率量化的方法来理解权值的分布，并对网络的工作原理有更深入的了解。在概率量化的情况下，由于贝叶斯方法的正则化效果，我们也可以得到更稀疏的模型。</p>
<h2 id="量化对象分类">量化对象分类</h2>
<h3 id="权重量化">权重量化</h3>
<p>在[Zhou et al., 2017a]中，作者提出了增量式网络量化(INQ)，它包括三个步骤：权重划分、分组量化和再训练。他们以组的方式对权重进行组化，以允许某些权重组补偿由于其他组的量化而造成的准确性损失。[Gudovskiy and Rigazio, 2017]的工作将这种方法扩展到2次幂设置。</p>
<p>在[Lin et al., 2016]中，作者试图找到最优的不动点位宽跨层分配。他们研究了通过量化不同的层可以引入多少噪音。[Lin et al., 2017]使用多个二进制基的线性组合近似全精度权重。结果表明，二值神经网络在ImageNet数据集上首次取得了与全精度神经网络相当的预测精度。在[Moons et al., 2017]中，作者研究了如何发展节能量化神经网络。在[Guo et al., 2017]的工作中引入了网络素描来量化预先训练好的模型。其思想是使用二进制基来近似预先训练过的滤波器。他们首先提出了一种启发式算法来寻找二进制基础，然后提供了一个改进版本，以更好的近似。在[Mohamed Amer, 2018]中，作者提出了一种端到端训练框架来同时优化原始损失函数、量化误差和总比特数。然而，其精度无法与其他量化神经网络相比。</p>
<h3 id="梯度量化">梯度量化</h3>
<p>梯度量化的应用场景一般是为了在分布式训练过程中减小通信消耗。如下图就是为神经网络的并行训练：</p>
<p><img src="/archives/2932556/image-20200729203114640.png" alt="image-20200729203114640" style="zoom: 80%;"></p>
<p>[Seide et al., 2014<a href="#refer-anchor-5"><sup>5</sup></a>]提出了一种1-bit表示各个节点计算梯度。和常规的方法相比，它得到了10倍的加速比。[Strom, 2015<a href="#refer-anchor-6"><sup>6</sup></a>]作者提出了一种阈值量化方法。提前选定一个阈值，如果梯度大于这个阈值就量化为<span class="math inline">\(+1\)</span>，如果小于这个阈值就量化为<span class="math inline">\(0\)</span>。[Alistarh et al., 2016<a href="#refer-anchor-7"><sup>7</sup></a>]提出了QSGD方法，允许每个节点在精度、梯度和模型的精度之间进行权衡。QSGD利用随机四舍五入的思想将梯度量化为一组离散值，并利用无损编码产生高效的编码。[Dryden et al., 2016<a href="#refer-anchor-8"><sup>8</sup></a>]]作者提出一种简单的自适应量化方法来选择合适的梯度进行量化并发送。</p>
<p>[Wen et al., 2017<a href="#refer-anchor-9"><sup>9</sup></a>]]通过提出了TernGrad解决了并行过程中水平扩展的问题。他将梯度量化为<span class="math inline">\(\{-1,0,1\}\)</span>。在发送给中心参数服务器之前，每个梯度都将被如下量化： <span class="math display">\[
\tilde{\Delta}_{t}=\operatorname{ternarize}\left(\Delta_{t}\right)=s_{t} \cdot \operatorname{sign}\left(\Delta_{t}\right) \circ b_{t}
\]</span> 其中<span class="math inline">\(s_{t}=\max \left(\operatorname{abs}\left(\Delta_{t}\right)\right)\)</span>，其中<span class="math inline">\(\circ\)</span>是<a href="https://www.baidu.com/link?url=w-LVD0IIl4PbHY-Vzc-UEKgvX7_8m_uRUWXH56DVxRjY77fZbsptVsF2pO7Gmxx3cRBReXg7sz3JTvIakfzCprx8m2RSMUNzT_E2Ppx3wyfA6RMWuAA7E66zF4btnsYK&amp;wd=&amp;eqid=8509435200010e68000000055f216602" target="_blank" rel="noopener external nofollow noreferrer">哈达玛积</a>，<span class="math inline">\(b_t\)</span>是遵循如下伯努利分布的随机二进制向量： <span class="math display">\[
\left\{\begin{array}{l}
P\left(b_{t k}=1 \mid \Delta_{t}\right)=\left|\Delta_{t k}\right| / s_{t} \\
P\left(b_{t k}=0 \mid \Delta_{t}\right)=1-\left|\Delta_{t k}\right| / s_{t}
\end{array}\right.
\]</span> 采用这种方法，服务器和工作节点之间的通信开销可以降低接近20倍.</p>
<p>在单一节点的环境中，我们也能够通过量化梯度获得益处。为了减小反向传播的计算开销，[Rastegari et al., 2016<a href="#refer-anchor-11"><sup>11</sup></a>]将梯度量化为2-bits来进行高性能通信。[Zhou et al., 2016<a href="#refer-anchor-10"><sup>10</sup></a>]他还量化了反向传播过程中的梯度。他们发现使用随机四舍五入的方法是非常重要的，使量程梯度工作得很好。他们设计了如下的k比特量化函数， <span class="math display">\[
\tilde{f}_{\gamma}^{k}(d r)=2 \max _{0}(|d r|)\left[\text { quantize }_{k}\left(\frac{d r}{2 \max _{0}(|d r|)+\frac{1}{2}}\right)-\frac{1}{2}\right]
\]</span> 其中<span class="math inline">\(d r=\frac{\partial c}{\partial r}\)</span>是在某些层输出<span class="math inline">\(r\)</span>的梯度，<span class="math inline">\(quantize_k\)</span>被用于量化一个实数输入<span class="math inline">\(r_{i} \in[0,1]\)</span>到k-bit输出值<span class="math inline">\(r_{0} \in[0,1]\)</span>， <span class="math display">\[
r_{o}=\frac{1}{2^{k}-1} \operatorname{round}\left(\left(2^{k}-1\right) r_{i}\right)
\]</span> 他们还在训练过程中加入额外的噪声，以弥补量化造成的准确性损失。</p>
<h5 id="挑战-3">挑战</h5>
<ul>
<li><p>梯度的大小和符号对于更新权重都很重要。为了量化梯度，我们必须解决如何将这两个因素纳入计算的问题。</p></li>
<li><p>一种简单的量化梯度的方法可能在实践中并不奏效，因为它可能违反随机梯度下降算法的收敛条件。在这种情况下需要更复杂的方法。</p></li>
</ul>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener external nofollow noreferrer">Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener external nofollow noreferrer">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="noopener external nofollow noreferrer">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" target="_blank" rel="noopener external nofollow noreferrer">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-7">

</div></li>
<li>[7] <a href="https://www.isca-speech.org/archive/interspeech_2015/i15_1488.html" target="_blank" rel="noopener external nofollow noreferrer">Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. 2015.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="https://ieeexplore.ieee.org/abstract/document/7835789/" target="_blank" rel="noopener external nofollow noreferrer">Dryden N, Moon T, Jacobs S A, et al. Communication quantization for data-parallel training of deep neural networks[C]//2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC). IEEE, 2016: 1-8.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/pdf/1606.06160" target="_blank" rel="noopener external nofollow noreferrer">Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients[J]. arXiv preprint arXiv:1606.06160, 2016.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="https://link.springer.com/chapter/10.1007/978-3-319-46493-0_32" target="_blank" rel="noopener external nofollow noreferrer">Rastegari M, Ordonez V, Redmon J, et al. Xnor-net: Imagenet classification using binary convolutional neural networks[C]//European conference on computer vision. Springer, Cham, 2016: 525-542.</a></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/fcc7e450.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/fcc7e450.html" class="post-title-link" itemprop="url">Error Compensated Quantized SGD and its Applications to Large-scale</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-28 14:55:28 / 修改时间：19:37:43" itemprop="dateCreated datePublished" datetime="2020-07-28T14:55:28+08:00">2020-07-28</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/fcc7e450.html" class="post-meta-item leancloud_visitors" data-flag-title="Error Compensated Quantized SGD and its Applications to Large-scale" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/fcc7e450.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/fcc7e450.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>本文提出了一种错误补偿的随机梯度量化方法来提高训练效率。量化本地梯度来减小通信负担，但是累积的量化误差会影响收敛的速度。此外，本文对收敛行为进行了理论分析，并证明了其相对于其他算法的优势。实验表明该算法能够再不降级影响收敛性能的情况下，将梯度压缩两个数量级。</p>
<h2 id="介绍">介绍</h2>
<p>一些方法注重将梯度量化为固定值，这样可以用更少的bits来进行通信传输（Zhou et al 2016<a href="#refer-anchor-1"><sup>1</sup></a>）；还有更加激进的方法，比如二进制或者三元组的表达（Seide et al., 2014<a href="#refer-anchor-2"><sup>2</sup></a>;Strom, 2015<a href="#refer-anchor-3"><sup>3</sup></a>; Wen et al., 2017<a href="#refer-anchor-4"><sup>4</sup></a>）；其他方法是在通信的过程中进行稀疏化，其中，每次迭代只有梯度的一小部分在节点之间通信传输（Wangni et al., 2017<a href="#refer-anchor-5"><sup>5</sup></a>; Lin et al., 2018<a href="#refer-anchor-6"><sup>6</sup></a>）</p>
<p>本文提出了误差补偿随机梯度量化方法，称为EC-SGD。该算法和<span class="math inline">\(1Bits\)</span>算法<a href="#refer-anchor-2"><sup>2</sup></a>不太相同，它将之前量化的所有梯度误差都考虑在内，并不只是使用最新一轮的量化误差。</p>
<p>在（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）中，作者证明了所提出的QSGD算法达到某次最优间隙所需的迭代次数与随机量化梯度的方差界成正比。然而这不能够解释我们方法的收敛行为，因为我们量化梯度是有偏估计，并不像QSGD那样。事实上，量化梯度的方差边界比在QSGD当中的更大，因为量化误差的累计会更大。为了解决这个问题，我们从另一个角度给出了收敛性分析，并且证明了我们的算法比QSGD算法有着更加严格的最坏情况的错误边界。结果表明，我们提出的误差反馈方案可以很好地抑制量化误差对误差界的影响，我们在实验中观察到，与QSGD相比，次最优性间隙更小。</p>
<h2 id="相关工作">相关工作</h2>
<h3 id="异步sgd">异步SGD</h3>
<p>Hogwild!（Recht et al. 2011）（其他工作相对来说时间比较久远，这里就不罗列了）</p>
<h3 id="梯度量化">梯度量化</h3>
<p>（Seide et al., 2014<a href="#refer-anchor-2"><sup>2</sup></a>）<span class="math inline">\(1Bit-SGD\)</span>用于对梯度的量化，将<span class="math inline">\(0\)</span>作为阈值，量化为<span class="math inline">\(1\)</span>或者<span class="math inline">\(-1\)</span>。在量化过程中引入上一轮的量化误差作为反馈。相似的想法在（Strom, 2015<a href="#refer-anchor-3"><sup>3</sup></a>）中被采纳，它通过迭代累计局部梯度并且仅传输超过预先选择的阈值的梯度分量。（Wen et al., 2017<a href="#refer-anchor-4"><sup>4</sup></a>）扩展了这一想法，并且将梯度压缩至了三元组来保证其无偏性。QSGD（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）采用均匀分布的方式随机量化梯度，更加细致地分析其收敛性。ZipML（Zhang et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）介绍了一种优化的量化策略，在分布式状态下动态选择量化节点。（Zhou et al 2016<a href="#refer-anchor-1"><sup>1</sup></a>）提出了DoReFa-Net来训练卷积神经网络，将输入、权重和梯度都进行定点的量化。</p>
<h3 id="梯度稀疏化">梯度稀疏化</h3>
<p>梯度丢弃方法是由（Aji &amp; Heafield, 2017<a href="#refer-anchor-10"><sup>10</sup></a>）将稀疏化方法引入到梯度当中，来减小通信误差。在（Wangni et al., 2017<a href="#refer-anchor-11"><sup>11</sup></a>）将梯度量化抽象成了线性规划问题，目的就是最小化量化梯度的方差增长。（Lin et al., 2018<a href="#refer-anchor-6"><sup>6</sup></a>）提出了深度梯度压缩算法，利用动量校正，梯度剪裁，动量因子掩饰，热身训练，以实现更高的稀疏性而不失去准确性。</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>（比较容易理解，暂时不翻译）</p>
<h2 id="误差压缩量化sgd方法">误差压缩量化SGD方法</h2>
<p>在每一轮迭代，之前每一轮的累计的量化误差都会对当前本地梯度进行补偿，再通过随机量化函数进行压缩。</p>
<p>令<span class="math inline">\(Q: \mathbb{R}^{d} \rightarrow \mathcal{C}^{d}\)</span>是一个无偏的随机量化函数，它将每部分的<span class="math inline">\(d\)</span>个维度向量映射到量化密码本<span class="math inline">\(\mathcal{C}\)</span>中。密码本通常只包含有限数量的元素，因此量化向量能够高效的编码。在每次迭代中，每个节点在广播之前量化他们的本地梯度： <span class="math display">\[
\tilde{\mathbf{g}}_{p}^{(t)}=Q\left(\mathbf{g}_{p}^{(t)}\right)
\]</span> 其中<span class="math inline">\(\mathbf{g}_{p}^{(t)}\)</span>是第<span class="math inline">\(p\)</span>个节点和第<span class="math inline">\(t\)</span>次迭代的本地梯度，<span class="math inline">\(\tilde{\mathbf{g}}_{p}^{(t)}\)</span>表示他的量化产物。</p>
<p>当节点接收到所有从其他节点发送过来的本地梯度之后，它将计算全局梯度以及更新它本地的模型，采用以下式子： <span class="math display">\[
\mathbf{w}^{(t+1)}=\mathbf{w}^{(t)}-\eta \cdot \tilde{\mathbf{g}}^{(t)}=\mathbf{w}^{(t)}-\frac{\eta}{P} \sum_{p=1}^{P} \tilde{\mathbf{g}}_{p}^{(t)}
\]</span></p>
<p>其中<span class="math inline">\(\eta&gt;0\)</span>表示学习率。</p>
<p>ECQ-SGD的核心思想是当量化本地梯度时，当前梯度和之前累加的量化误差都会考虑在内。特别的，我们使用<span class="math inline">\(\mathbf{h}_{p}^{(t)}\)</span>代表第<span class="math inline">\(p\)</span>个节点的第<span class="math inline">\(t\)</span>次迭代的累计量化误差： <span class="math display">\[
\mathbf{h}_{p}^{(t)}=\sum_{t^{\prime}=0}^{t-1} \beta^{t-1-t^{\prime}}\left(\mathbf{g}_{p}^{\left(t^{\prime}\right)}-\tilde{\mathbf{g}}_{p}^{\left(t^{\prime}\right)}\right)
\]</span> 其中<span class="math inline">\(\beta\)</span>是时间减弱因子。注意到累计量化误差的更新式如下： <span class="math display">\[
\mathbf{h}_{p}^{(t)}=\beta \mathbf{h}_{p}^{(t-1)}+\left(\mathbf{g}_{p}^{(t-1)}-\tilde{\mathbf{g}}_{p}^{(t-1)}\right)
\]</span> 其中<span class="math inline">\(\mathbf{h}_{p}^{(0)}=\mathbf{0}\)</span>。量化本地梯度将会通过量化函数计算出误差补偿梯度： <span class="math display">\[
\tilde{\mathbf{g}}_{p}^{(t)}=Q\left(\mathbf{g}_{p}^{(t)}+\alpha \mathbf{h}_{p}^{(t)}\right)
\]</span> 其中<span class="math inline">\(\alpha\)</span>代表补偿系数。</p>
<p>这里我们采用服从均匀分布的随机量化函数，类似于QSGD算法（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>），其中第<span class="math inline">\(i\)</span>个维度将会量化为： <span class="math display">\[
\tilde{g}_{i}=\|\mathbf{g}\| \cdot \operatorname{sgn}\left(g_{i}\right) \cdot \xi\left(\left|g_{i}\right| ;\|\mathbf{g}\|\right)
\]</span> 其中$|| <span class="math inline">\(表示扩展因子（可以选择\)</span>l_2<span class="math inline">\(或者\)</span>l_{}<span class="math inline">\(这两种方式），\)</span>()<span class="math inline">\(是随机函数，是映射到如下元素中\)</span>{0, , , 1}$： <span class="math display">\[
\xi\left(\left|g_{i}\right| ;\|\mathbf{g}\|\right)=\left\{\begin{array}{ll}
\frac{l}{s}, &amp; \text { with probability } l+1-s \cdot \frac{\left|g_{i}\right|}{\|\mathbf{g}\|} \\
\frac{l+1}{s}, &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中<span class="math inline">\(\left|g_{i}\right| /\|\mathbf{g}\|\)</span>是落在<span class="math inline">\(\left[\frac{l}{s}, \frac{l+1}{s}\right)\)</span>范围之内。超参数<span class="math inline">\(s\)</span>表示非零量化等级：更大的<span class="math inline">\(s\)</span>会导致更加细粒度的量化，同时导致更多的通信消耗。我们采用<span class="math inline">\(Q_{s}(\cdot)\)</span>代表量化函数，<span class="math inline">\(s\)</span>表示非零量化等级。</p>
<p>在量化之后，我们只需要使用<span class="math inline">\(r=\left\lceil\log _{2}(2 s+1)\right\rceil\)</span>bits来编码每个梯度<span class="math inline">\(\tilde{g}_{i}\)</span>，一个完整的精度表示扩展因子$ ||$。全部的通信需要花费<span class="math inline">\(32+d\)</span>bits（<span class="math inline">\(r \ll 32\)</span>）,原始的梯度需要每个维度32-bit的全精度来表示。更加有效的编码模式，例如Huffman编码，能够进一步地减小通信量。令<span class="math inline">\(d_k\)</span>表示分配各<span class="math inline">\(k\)</span>个量化级别的维数，之后整个编码长度最多只需要<span class="math inline">\(\sum_{k=1}^{2 s+1} d_{k} \log _{2} \frac{d}{d_{k}}\)</span>个bits。</p>
<p>算法1概括了上述的整个过程：</p>
<p><img src="/archives/fcc7e450/image-20200728190425444.png" alt="image-20200728190425444" style="zoom:67%;"></p>
<h2 id="理论分析">理论分析</h2>
<p>（暂时省略）</p>
<h2 id="实验分析">实验分析</h2>
<h3 id="线性模型">线性模型</h3>
<p>使用三个人造数据集：Syn-256、Syn-512和Syn-1024。每个数据集包含10k个训练样本，后缀表示特征维度<span class="math inline">\(d\)</span>。训练样本通过<span class="math inline">\(y_{i}=\mathbf{w}^{* T} \mathbf{x}_{i}+\epsilon_{i}\)</span>生成，其中<span class="math inline">\(\mathbf{w}^{*} \in \mathbb{R}^{d}\)</span>是我们希望获得的潜在模型参数，<span class="math inline">\(\left\{\epsilon_{i}\right\}\)</span>是服从独立同分布的随机噪声。学习率是0.02，QSGD和ECQ-SGD都采用<span class="math inline">\(l_2\)</span>作为扩展因子，采用<span class="math inline">\(4\)</span>作为量化等级。</p>
<p>下图我们比较了损失函数值（上方）和距离最优解的距离（下方）。对于这三个数据集，ECQ-SGD损失函数的收敛性更加接近于<span class="math inline">\(32-bit\)</span>全精度的SGD算法，比<span class="math inline">\(QSGD\)</span>的训练速度明显更快。另一方面，QSGD(或ECQ-SGD)与32Bit-FP在最优解距离上的差距度量了量化误差对(21)中定义的误差界的贡献。ECQ-SGD的距离差明显小于QSGD，说明量化误差对误差界的贡献得到了很好的抑制。</p>
<p>下面我们比较了在大数据集上，QSGD和ECQ-SGD运行时训练速率，Syn-20k，它包括了50k训练样本和20k维度的特征。在图三中，我们多维度展示了在1k轮迭代之后各种方法的时间消耗和测试误差。我们发现ECQ-SGD达到和32Bit-FP相似的测试误差，比32Bit-FP和QSGD所用的时间更短。虽然ECQ-SGD需要额外的编码好解码时间，由于通信量的降低，整个训练速度依然得到了提高：</p>
<p><img src="/archives/fcc7e450/image-20200728192202073.png" alt="image-20200728192202073" style="zoom:67%;"></p>
<p>其次，本文还对两个公开的数据集<code>YearPredictionMSD</code>（回归）和<code>gisette</code>分类。在不同的量化方法下对这两个数据集的逻辑回归和线性回归做了测评：</p>
<p><img src="/archives/fcc7e450/image-20200728192421592.png" alt="image-20200728192421592" style="zoom: 80%;"></p>
<h3 id="卷积神经网络">卷积神经网络</h3>
<p>本文还在卷积神经网络上做了测试。CIFAR-10和ResNet-20模型，采用不同的量化方法，结果如图所示：</p>
<figure>
<img src="/archives/fcc7e450/image-20200728192625094.png" alt="image-20200728192625094"><figcaption aria-hidden="true">image-20200728192625094</figcaption>
</figure>
<p>在第一列中，比较了各种方法的整体通信负担和损失函数值的情况。与32位全精度基准方法相比，每个模型的超参数被分离，以达到可以忽略的精度损失。我们发现所有的方法都以相似的速度收敛，但ECQ-SGD在通信成本上降低了80倍以上，并且显著优于其他梯度量化方法。</p>
<p>在第二列和第三列中，我们比较了它和QSGD的细节，因为它和我们的方法最为相关。我们采用了不同的扩展因子：第二列是<span class="math inline">\(l_2\)</span>，第三列是<span class="math inline">\(l_{\infty}\)</span>。我们发现我们观察到，ECQ-SGD在收敛速度和分类精度方面始终优于QSGD，而在相同的超参数设置下，这两种方法在降低通信成本方面是相似的。</p>
<h3 id="性能模型">性能模型</h3>
<p>我们采用（Yan et al., 2015）提出的性能评估模型对ECQ-SGD算法进行评估。对计算量和通信时间进行轻量级分析，以估计较大集群的学习效率。主要硬件规格如下:Intel Xeon E5-2680 CPU, Nvidia Tesla P40 GPU(每个节点8个单元)，Mellanox ConnectX-3 Pro网卡(40Gb/s连通性)。</p>
<p>下图中，我们展示了采用ResNet-50模型训练ILSVRC-12数据集。训练512个GPU，ECQ-SGD先比于普通的SGD达到了143.5%的加速比。</p>
<p><img src="/archives/fcc7e450/image-20200728193508865.png" alt="image-20200728193508865" style="zoom:67%;"></p>
<h2 id="结论">结论</h2>
<p>为了提高大规模分布式优化的学习效率，本文提出了误差补偿量化SGD算法。通过引入误差反馈机制，ECQ-SGD算法可以有效地抑制量化误差对误差界的贡献。我们从理论的角度分析了它的收敛行为，并证明了它比最先进的QSGD算法的优势。在线性模型和非凸卷积神经网络上的实验证明了该算法的有效性。</p>
<h2 id="部分参考文献">部分参考文献</h2>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="https://arxiv.org/pdf/1606.06160" target="_blank" rel="noopener external nofollow noreferrer">Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients[J]. arXiv preprint arXiv:1606.06160, 2016.</a></li>
</ul>
<div id="refer-anchor-2">

</div>
<ul>
<li>[2] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a></li>
</ul>
<div id="refer-anchor-3">

</div>
<ul>
<li>[3] <a href="https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1488.pdf" target="_blank" rel="noopener external nofollow noreferrer">Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. 2015.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wangni J, Wang J, Liu J, et al. Gradient sparsification for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2018: 1299-1309.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="https://arxiv.org/pdf/1712.01887" target="_blank" rel="noopener external nofollow noreferrer">Lin Y, Han S, Mao H, et al. Deep gradient compression: Reducing the communication bandwidth for distributed training[J]. arXiv preprint arXiv:1712.01887, 2017.</a>
<div id="refer-anchor-7">

</div></li>
<li>[7] <a href="http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf" target="_blank" rel="noopener external nofollow noreferrer">Recht B, Re C, Wright S, et al. Hogwild: A lock-free approach to parallelizing stochastic gradient descent[C]//Advances in neural information processing systems. 2011: 693-701.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="http://proceedings.mlr.press/v70/zhang17e.html" target="_blank" rel="noopener external nofollow noreferrer">Zhang H, Li J, Kara K, et al. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning[C]//International Conference on Machine Learning. 2017: 4035-4043.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/pdf/1704.05021" target="_blank" rel="noopener external nofollow noreferrer">Aji A F, Heafield K. Sparse communication for distributed gradient descent[J]. arXiv preprint arXiv:1704.05021, 2017.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wangni J, Wang J, Liu J, et al. Gradient sparsification for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2018: 1299-1309.</a>
<div id="refer-anchor-12">

</div></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/d06ef2e3.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/d06ef2e3.html" class="post-title-link" itemprop="url">QSGD Communication-Efficient SGD via Gradient Quantization and Encoding</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-28 09:06:20" itemprop="dateCreated datePublished" datetime="2020-07-28T09:06:20+08:00">2020-07-28</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-29 20:47:10" itemprop="dateModified" datetime="2020-07-29T20:47:10+08:00">2020-07-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/d06ef2e3.html" class="post-meta-item leancloud_visitors" data-flag-title="QSGD Communication-Efficient SGD via Gradient Quantization and Encoding" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/d06ef2e3.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/d06ef2e3.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>本文提出了量化SGD（QSGD）方法，可以帮助使用者平滑地权衡通信带宽和收敛时间：在方差成本可能较高的情况下，节点可以调节每轮迭代发送的比特数。本文证明了这种权衡是内在的，某种意义上，提高它超过某个阈值将违反信息理论的下界。QSGD保证了在凸和非凸目标函数上的收敛性，在异步条件下，能够使用随机方差减小的方法进行拓展。</p>
<h2 id="简介">简介</h2>
<p>比较流行的减少通信量通常采用有损压缩的方法<a href="#refer-anchor-1"><sup>1</sup></a><a href="#refer-anchor-2"><sup>2</sup></a><a href="#refer-anchor-3"><sup>3</sup></a>。一种方式是简单地降低表示精度，在凸和稀疏的条件下证明了收敛性<a href="#refer-anchor-4"><sup>4</sup></a>。<span class="math inline">\(1BitSGD\)</span>量化方法为梯度的量化压缩奠定了基础。</p>
<h3 id="本文贡献">本文贡献</h3>
<p>本文重点关注数据并行SGD算法的通信开销和收敛速率保证。我们提出有损的梯度压缩方法：QSGD算法，它可以在每一次迭代中在通信量和方差之间做平衡。</p>
<p>QSGD建立在两种算法思想上。第一种是直观的随机量化模式：在某进程中给出一个梯度向量，我们通过四舍五入到一组离散的值来量化每个部分，有原则地保持原始数据的统计性质。第二种是量化梯度的有效无损代码，利用他们的统计属性来生成有效的编码。我们分析给出了QSGD引起的精度-方差权衡的严格界限。</p>
<p>QSGD非常普遍，在假设条件下，对于非凸目标和异步迭代，能够收敛到局部最小。在非平凡扩展的情况下，我们创新了一种随机方差减小的QSGD算法，称为QSVRG，它具有指数级的收敛速率。</p>
<p>一个关键的问题是QSGD的压缩方差平衡是否是固有的：例如，任何算法保证最多恒定方差放大每次迭代需要传输<span class="math inline">\(\Omega(n)\)</span>bits。回答是肯定的：在此基础上的渐近改进将打破分布式平均估计的通信复杂度下限（不是很懂这块内容）。</p>
<p>与1BitSGD相比，QSGD具有渐近高压缩性能，在标准假设下可证明收敛性，在某些情况下具有较好的实际性能。</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>（暂时省略）</p>
<h2 id="量化随机梯度下降qsgd">量化随机梯度下降（QSGD）</h2>
<h3 id="一般性的随机量化和编码">一般性的随机量化和编码</h3>
<h4 id="随机量化">随机量化</h4>
<p>我们现在思考随机梯度向量一般参数有损压缩方案。压缩函数用<span class="math inline">\(Q_s(v)\)</span>表示，其中<span class="math inline">\(s \geq 1\)</span>调节参数，代表我们所要实现的量化的程度。直观上来看，我们定义<span class="math inline">\(s\)</span>服从<span class="math inline">\(0-1\)</span>的均匀分布，每个值都以一种保持预期值并引入最小方差的方式进行量化，例如下图：</p>
<p><img src="/archives/d06ef2e3/image-20200728104606218.png" alt="image-20200728104606218" style="zoom:67%;"></p>
<p>上图是5级的广义随机量化的示例</p>
<p>对于任意的<span class="math inline">\(\boldsymbol{v} \in \mathbb{R}^{n} \text { with } \boldsymbol{v} \neq \mathbf{0})\)</span>，<span class="math inline">\(Q_{s}(\boldsymbol{v})\)</span>定义如下： <span class="math display">\[
Q_{s}\left(v_{i}\right)=\|\boldsymbol{v}\|_{2} \cdot \operatorname{sgn}\left(v_{i}\right) \cdot \xi_{i}(\boldsymbol{v}, s)
\]</span> 其中<span class="math inline">\(\xi_{i}(\boldsymbol{v}, s)\)</span>是一个独立随机变量，定义如下。令<span class="math inline">\(0 \leq \ell&lt;s\)</span>是一个整数，例如<span class="math inline">\(\left|v_{i}\right| /\|\boldsymbol{v}\|_{2} \in[\ell / s,(\ell+1) / s]\)</span>。<span class="math inline">\([\ell / s,(\ell+1) / s]\)</span>是与$|v_{i}| /||_{2} $相符的量化间隔。定义： <span class="math display">\[
\xi_{i}(\boldsymbol{v}, s)=\left\{\begin{array}{ll}
\ell / s &amp; \text { 以如下概率 } 1-p\left(\frac{\left|v_{i}\right|}{\|\boldsymbol{v}\|_{2}}, s\right) \\
(\ell+1) / s &amp; \text { 其他情况. }
\end{array}\right.
\]</span> 这里<span class="math inline">\(p(a, s)=a s-\ell\)</span>对于任何的<span class="math inline">\(a \in[0,1]\)</span>。如果<span class="math inline">\(v=0\)</span>，我们定义<span class="math inline">\(Q(v,s)=0\)</span>。</p>
<p><span class="math inline">\(\xi_{i}(\boldsymbol{v}, s)\)</span>的分布具有最小的方差，它的期望满足<span class="math inline">\(\mathbb{E}\left[\xi_{i}(\boldsymbol{v}, s)\right]=\left|v_{i}\right| /\|\boldsymbol{v}\|_{2}\)</span>。我们可以证明如下：</p>
<p><strong>引理</strong> 对于任意<span class="math inline">\(\boldsymbol{v} \in \mathbb{R}^{n}\)</span>，我们有（1）<span class="math inline">\(\mathbb{E}\left[Q_{s}(\boldsymbol{v})\right]=\boldsymbol{v} \text { (无偏) }\)</span>；（2）<span class="math inline">\(\mathbb{E}\left[\left\|Q_{s}(\boldsymbol{v})-\boldsymbol{v}\right\|_{2}^{2}\right] \leq \min \left(n / s^{2}, \sqrt{n} / s\right)\|\boldsymbol{v}\|_{2}^{2}\)</span>（方差最小边界）；（3）<span class="math inline">\(\mathbb{E}\left[\left\|Q_{s}(\boldsymbol{v})\right\|_{0}\right] \leq s(s+\sqrt{n})\)</span>稀疏性</p>
<h4 id="梯度的高效编码">梯度的高效编码</h4>
<p>对于任意的向量<span class="math inline">\(v\)</span>，输出<span class="math inline">\(Q_s(v)\)</span>可以表达为三元组<span class="math inline">\(\left(\|v\|_{2}, \sigma, \zeta\right)\)</span>，其中<span class="math inline">\(\sigma\)</span>是向量中<span class="math inline">\(v_i\)</span>的符号，<span class="math inline">\(\zeta\)</span>是整数<span class="math inline">\(s \cdot \xi_{i}(\boldsymbol{v}, s)\)</span>的向量。我们可以发现，大的整数出现的频率很低。我们将通过专门的<code>Eilias</code>整数编码来利用这一点。</p>
<p>直觉上，对于任何正整数<span class="math inline">\(k\)</span>，他的编码用<span class="math inline">\(Elias(k)\)</span>表示<span class="math inline">\(k\)</span>的二进制，并且在它的前面加上长度表示。然后递归地编码这个前缀。我们发现对于任何整数<span class="math inline">\(k\)</span>，结果编码的长度为<span class="math inline">\(\operatorname{Elias}(k) \mid=\log k+\log \log k+\ldots+1 \leq(1+o(1)) \log k+1\)</span>。编码和解码过程非常高效。</p>
<p>给定的梯度向量可以表示为三元组<span class="math inline">\(\left(\|\boldsymbol{v}\|_{2}, \boldsymbol{\sigma}, \boldsymbol{\zeta}\right)\)</span>，具有<span class="math inline">\(s\)</span>压缩等级，我们的编码输出为字符串<span class="math inline">\(S\)</span>，定义如下：第一使用<span class="math inline">\(32bits\)</span>编码<span class="math inline">\(\|\boldsymbol{v}\|_{2}\)</span>。它继续使用<span class="math inline">\(Elias\)</span>递归编码读第一个非零项<span class="math inline">\(\boldsymbol{\zeta}\)</span>的位置进行编码。接下来附加一个bit代表<span class="math inline">\(\boldsymbol{\sigma}_i\)</span>，遵循<span class="math inline">\(s \cdot \xi_{i}(\boldsymbol{v}, s)\)</span>。迭代进行，它继续从当前的写入坐标到下一个非零的距离，并以同样的方式对坐标的<span class="math inline">\(\boldsymbol{\sigma}_i\)</span>和<span class="math inline">\(\boldsymbol{\zeta}_i\)</span>进行编码。解码方案很简单：我们首先读取<span class="math inline">\(32bits\)</span>来构建<span class="math inline">\(\|\boldsymbol{v}\|_{2}\)</span>，接下来，迭代使用<span class="math inline">\(Elias\)</span>递归编码的译码方案读取非零项<span class="math inline">\(\boldsymbol{\zeta}\)</span>和<span class="math inline">\(\boldsymbol{\sigma}\)</span>的位置和值。</p>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" target="_blank" rel="noopener external nofollow noreferrer">Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks[C]//Advances in neural information processing systems. 2012: 1223-1231.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1603.04467.pdf" target="_blank" rel="noopener external nofollow noreferrer">Abadi M, Agarwal A, Barham P, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems[J]. arXiv preprint arXiv:1603.04467, 2016.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="http://papers.nips.cc/paper/5717-taming-the-wild-a-unified-analysis-of-hogwild-style-algorithms.pdf" target="_blank" rel="noopener external nofollow noreferrer">De Sa C M, Zhang C, Olukotun K, et al. Taming the wild: A unified analysis of hogwild-style algorithms[C]//Advances in neural information processing systems. 2015: 2674-2682.</a></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/487a2936.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/487a2936.html" class="post-title-link" itemprop="url">粉笔公考-判断推理-图形推理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-27 20:39:49" itemprop="dateCreated datePublished" datetime="2020-07-27T20:39:49+08:00">2020-07-27</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-28 21:11:16" itemprop="dateModified" datetime="2020-07-28T21:11:16+08:00">2020-07-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%AC%E5%8A%A1%E5%91%98%E8%80%83%E8%AF%95/" itemprop="url" rel="index"><span itemprop="name">公务员考试</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%85%AC%E5%8A%A1%E5%91%98%E8%80%83%E8%AF%95/%E5%88%A4%E6%96%AD%E6%8E%A8%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">判断推理</span></a>
                </span>
            </span>

          
            <span id="/archives/487a2936.html" class="post-meta-item leancloud_visitors" data-flag-title="粉笔公考-判断推理-图形推理" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/487a2936.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/487a2936.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>851</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="判断推理题型">判断推理题型</h2>
<ul>
<li>图形推理</li>
<li>类比推理</li>
<li>定义判断</li>
<li>逻辑判断</li>
</ul>
<h2 id="图形推理">图形推理</h2>
<h3 id="命题形式">命题形式</h3>
<h4 id="一组图">一组图</h4>
<p>大多数：从左到右整体去看</p>
<p>有时候：跳着看，考地相对较少</p>
<p><img src="/archives/487a2936/image-20200727205210815.png" alt="image-20200727205210815" style="zoom:67%;"></p>
<h4 id="两组图">两组图</h4>
<p>第一组用于找规律，第二组用规律（模仿第一组的规律即可，细节变化以第二组图为准）</p>
<p><img src="/archives/487a2936/image-20200727205147584.png" alt="image-20200727205147584" style="zoom:67%;"></p>
<h4 id="九宫格">九宫格</h4>
<p>优先横着看，其次再是竖着看（很少斜着、S型和米子型）</p>
<h4 id="分组分类">分组分类</h4>
<p>一般分成两组，在组内找出各自的规律（找出两个规律）</p>
<p><img src="/archives/487a2936/image-20200727205400400.png" alt="image-20200727205400400" style="zoom:67%;"></p>
<h4 id="空间类折纸盒">空间类：折纸盒</h4>
<p>六面体为主，转化成平面</p>
<p><img src="/archives/487a2936/image-20200727205533123.png" alt="image-20200727205533123" style="zoom:67%;"></p>
<h5 id="特殊题型">特殊题型</h5>
<ul>
<li>截面图</li>
<li>三视图</li>
<li>立体拼合</li>
</ul>
<h2 id="六大规律">六大规律</h2>
<p>重点通过识别图形特征，来识别考察什么规律</p>
<ol type="1">
<li>位置规律</li>
<li>样式规律</li>
<li>属性规律</li>
<li>特殊规律</li>
<li>数量规律</li>
<li>空间规律</li>
</ol>
<h3 id="位置规律">位置规律</h3>
<h4 id="特征">特征</h4>
<p>位置类识别特征：各图元素组成相同</p>
<p><img src="/archives/487a2936/image-20200727210022937.png" alt="image-20200727210022937" style="zoom:67%;"></p>
<h4 id="考点">考点</h4>
<ul>
<li>平移</li>
<li>旋转、翻转（常结合考察）</li>
</ul>
<h5 id="考点一平移">考点一：平移</h5>
<ol type="1">
<li><p>方向：直线（上下、左右、对角线）、绕圈（顺/逆时针）</p>
<p><img src="/archives/487a2936/image-20200727210259617.png" alt="image-20200727210259617" style="zoom:67%;"></p></li>
<li><p>步数：恒定、递增（等差）、周期（考的少）</p></li>
</ol>
<h6 id="宫格形黑块平移">宫格形黑块平移</h6>
<ol type="1">
<li>个别黑块重合
<ul>
<li>题干和选项大部分元素组成完全一致，个别一两副图少黑块</li>
<li>题干第一幅图的黑块一般不会重合</li>
</ul></li>
</ol>
<p><img src="/archives/487a2936/image-20200727210936304.png" alt="image-20200727210936304" style="zoom:67%;"></p>
<ol start="2" type="1">
<li><p>黑块走到头后</p>
<ul>
<li><p>循环走：从头开始</p>
<p><img src="/archives/487a2936/image-20200727211118319.png" alt="image-20200727211118319" style="zoom:67%;"></p></li>
<li><p>折返走：直接弹回</p>
<p><img src="/archives/487a2936/image-20200727211135729.png" alt="image-20200727211135729" style="zoom:67%;"></p></li>
<li><p>“双胞胎”黑块如何分辨：就近走原则</p>
<p><img src="/archives/487a2936/image-20200727211331662.png" alt="image-20200727211331662" style="zoom:67%;"></p></li>
</ul></li>
</ol>
<h6 id="多宫格方向判定">多宫格方向判定</h6>
<p>题型特征：16宫格图形多个黑块平移</p>
<ol type="1">
<li><p>直线走：</p>
<ul>
<li>横行黑块数量相同（左右走）</li>
<li>竖行黑块数量相同（上下走）</li>
</ul>
<p><img src="/archives/487a2936/image-20200727211823656.png" alt="image-20200727211823656" style="zoom:67%;"></p></li>
<li><p>绕圈走：</p>
<ul>
<li>中间颜色数量相同，有限考虑内外圈分开看</li>
</ul>
<p><img src="/archives/487a2936/image-20200727212331968.png" alt="image-20200727212331968" style="zoom:67%;"></p></li>
</ol>
<h5 id="考点二旋转与翻转">考点二：旋转与翻转</h5>
<h6 id="旋转">旋转</h6>
<ol type="1">
<li>方向：顺时针、逆时针</li>
<li>常见角度：45°、60°、90°、180°等</li>
</ol>
<blockquote>
<p>TIPS：难题可以采用两两相邻比较</p>
<p>钟表类：麦面一个框，中间有一个点，饶了一圈线——常考旋转</p>
</blockquote>
<h6 id="翻转">翻转</h6>
<ol type="1">
<li>左右翻转</li>
<li>上下翻转（可能存在视觉误差，需要警觉）</li>
</ol>
<blockquote>
<p>TIPS：先看容易看懂的</p>
</blockquote>
<p>这道题目需要注意上下翻转，很难看出来（第二张图到第三张图）</p>
<p><img src="/archives/487a2936/image-20200727214018551.png" alt="image-20200727214018551" style="zoom:67%;"></p>
<h6 id="区分旋转和翻转">区分旋转和翻转</h6>
<ul>
<li><p>只有左右互换（上下不变）——左右翻</p>
<p><img src="/archives/487a2936/image-20200727214319546.png" alt="image-20200727214319546" style="zoom:67%;"></p></li>
<li><p>只有上下互换（左右不变）——上下翻</p>
<p><img src="/archives/487a2936/image-20200727214353319.png" alt="image-20200727214353319" style="zoom:67%;"></p></li>
<li><p>上下、左右都互换——旋转180°</p>
<p><img src="/archives/487a2936/image-20200727214429063.png" alt="image-20200727214429063" style="zoom:67%;"></p></li>
</ul>
<h3 id="样式规律">样式规律</h3>
<h4 id="特征-1">特征</h4>
<p>样式识别特征：元素组成相似</p>
<h4 id="考点-1">考点</h4>
<ol type="1">
<li>遍历</li>
<li>加减同异</li>
<li>黑白运算</li>
</ol>
<h5 id="考点一遍历">考点一：遍历</h5>
<p>图形特征：小元素重复出现</p>
<p>解题思路：缺啥补啥（遍历包括空白/阴影等很多方面）</p>
<p><img src="/archives/487a2936/image-20200727215015137.png" alt="image-20200727215015137" style="zoom:67%;"></p>
<h5 id="考点二加减同异"><strong>考点二：加减同异</strong></h5>
<p>识别特征：相同线条重复出现</p>
<ol type="1">
<li>相加、相减</li>
<li>求异（保留不同）</li>
<li>求同（保留相同）</li>
</ol>
<blockquote>
<p>对比选项，从特殊线条入手（横线、竖线、最长最短线）</p>
</blockquote>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/493c0bc7.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/493c0bc7.html" class="post-title-link" itemprop="url">DOUBLESQUEEZE Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-24 20:45:07" itemprop="dateCreated datePublished" datetime="2020-07-24T20:45:07+08:00">2020-07-24</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-03 16:04:16" itemprop="dateModified" datetime="2020-08-03T16:04:16+08:00">2020-08-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/493c0bc7.html" class="post-meta-item leancloud_visitors" data-flag-title="DOUBLESQUEEZE Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/493c0bc7.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/493c0bc7.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>目前已有类似于QSGD和稀疏化SGD的通信优化算法，但参数服务器在实际应用中在收到工作节点量化梯度并聚合后，需要将聚合梯度从新分发给工作节点。本论文同时对工作节点和参数服务器梯度，采用误差补偿的方式进行梯度压缩。该算法有三大优势：</p>
<ol type="1">
<li>它兼容众多“粗暴”的压缩技术</li>
<li>它与没有误差补偿的压缩算法（例如QSGD和稀疏化SGD）相比，收敛性更好</li>
<li>达到了线性收敛</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(<span class="string">"111"</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>dfdfadf</p>
</blockquote>
<h2 id="背景介绍">背景介绍</h2>
<p>提高分布式机器学习性能的三个方向：</p>
<p>（1）高通信效率的学习</p>
<ul>
<li>QSGD: Communication-efficient SGD via gradient quantization and encoding<a href="#refer-anchor-1"><sup>1</sup></a>（量化为三元组表示）</li>
<li>signSGD: Compressed optimisation for non-convex problems<a href="#refer-anchor-2"><sup>2</sup></a></li>
<li>1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns<a href="#refer-anchor-3"><sup>3</sup></a>（提出一种误差补偿的量化方法）</li>
</ul>
<p>（2）去中心化学习；</p>
<ul>
<li>He L, Bian A, Jaggi M. Cola: Decentralized linear learning[C]//Advances in Neural Information Processing Systems. 2018: 4536-4546.</li>
<li>Lian X, Zhang C, Zhang H, et al. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent[C]//Advances in Neural Information Processing Systems. 2017: 5330-5340.</li>
</ul>
<p>（3）异步学习</p>
<ul>
<li>Agarwal A, Duchi J C. Distributed delayed stochastic optimization[C]//Advances in Neural Information Processing Systems. 2011: 873-881.</li>
<li>Lian X, Huang Y, Li Y, et al. Asynchronous parallel stochastic gradient for nonconvex optimization[C]//Advances in Neural Information Processing Systems. 2015: 2737-2745.</li>
<li>Recht B, Re C, Wright S, et al. Hogwild: A lock-free approach to parallelizing stochastic gradient descent[C]//Advances in neural information processing systems. 2011: 693-701.</li>
</ul>
<h3 id="量化压缩基本模型">量化压缩基本模型</h3>
<p>作者对分布式机器学习（特别是参数服务器架构）和量化压缩数学模型简单做了介绍</p>
<h4 id="分布式机器学习基本模型">分布式机器学习基本模型</h4>
<p><span class="math display">\[
\min _{\boldsymbol{x}} f(\boldsymbol{x})=\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{\boldsymbol{\zeta} \sim \mathcal{D}_{i}} F(\boldsymbol{x} ; \boldsymbol{\zeta})
\]</span></p>
<p>其中<span class="math inline">\(n\)</span>表示工作节点数量，<span class="math inline">\(\mathcal{D}_{i}\)</span>本地节点<span class="math inline">\(i\)</span>的数据分布，<span class="math inline">\(F(\boldsymbol{x} ; \boldsymbol{\zeta})\)</span>为本地损失函数。 <span class="math display">\[
\boldsymbol{g}^{(i)}=\nabla F\left(\boldsymbol{x} ; \boldsymbol{\zeta}^{(i)}\right)
\]</span> 各工作节点计算梯度 <span class="math display">\[
\boldsymbol{g}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{g}^{(i)}
\]</span> 参数服务器对梯度进行聚合，以上是对分布式SGD算法的简单建模</p>
<h4 id="量化压缩">量化压缩</h4>
<p><span class="math inline">\(Q_{\omega}[\cdot]\)</span>代表压缩操作，以<span class="math inline">\(1Bits\)</span>方法为例，利用递归的方法更新压缩误差： <span class="math display">\[
\boldsymbol{\delta}^{(i)}=\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}-Q_{\omega}\left[\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}\right]
\]</span> 其中<span class="math inline">\(\left[\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}\right]\)</span>表示本轮计算得到的梯度<span class="math inline">\(g^{(i)}\)</span>和上一轮压缩误差<span class="math inline">\(\boldsymbol{\delta}^{(i)}\)</span>的和，上式子是对本轮量化误差的重新计算，这也是误差补偿的由来。</p>
<h4 id="主要贡献">主要贡献</h4>
<ol type="1">
<li>比其他没有错误补偿的压缩方法具有更好的收敛性</li>
<li>进一步优化了通信效率</li>
<li>第一次给出了误差补偿SGD相关算法的速率分析</li>
<li>在非凸情况下的加速证明</li>
</ol>
<h2 id="相关工作">相关工作</h2>
<h4 id="分布式学习">分布式学习</h4>
<h5 id="中心化并行训练">中心化并行训练</h5>
<h6 id="参数服务器架构">参数服务器架构</h6>
<ol type="1">
<li>Abadi M, Barham P, Chen J, et al. Tensorflow: A system for large-scale machine learning[C]//12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16). 2016: 265-283.</li>
<li>Li M, Andersen D G, Park J W, et al. Scaling distributed machine learning with the parameter server[C]//11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 2014: 583-598.</li>
</ol>
<h6 id="去中心化训练">去中心化训练</h6>
<ol type="1">
<li>固定拓扑
<ul>
<li>Jin P H, Yuan Q, Iandola F, et al. How to scale distributed deep learning?[J]. arXiv preprint arXiv:1611.04581, 2016.</li>
<li>Lian X, Zhang C, Zhang H, et al. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent[C]//Advances in Neural Information Processing Systems. 2017: 5330-5340.</li>
<li>Shen Z, Mokhtari A, Zhou T, et al. Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication[J]. arXiv preprint arXiv:1805.09969, 2018.</li>
<li>Tang H, Lian X, Yan M, et al. D <span class="math inline">\(^ 2\)</span>: Decentralized Training over Decentralized Data[J]. arXiv preprint arXiv:1803.07068, 2018.</li>
</ul></li>
<li>随机拓扑
<ul>
<li>Lian X, Zhang W, Zhang C, et al. Asynchronous decentralized parallel stochastic gradient descent[C]//International Conference on Machine Learning. 2018: 3043-3052.</li>
<li>Nedić A, Olshevsky A. Distributed optimization over time-varying directed graphs[J]. IEEE Transactions on Automatic Control, 2014, 60(3): 601-615.</li>
<li>Nedic A, Olshevsky A, Shi W. Achieving geometric convergence for distributed optimization over time-varying graphs[J]. SIAM Journal on Optimization, 2017, 27(4): 2597-2633.</li>
</ul></li>
</ol>
<h6 id="不同角度实现分布式训练">不同角度实现分布式训练</h6>
<ol type="1">
<li>隐私分布式优化
<ul>
<li>Jayaraman B, Wang L, Evans D, et al. Distributed learning without distress: Privacy-preserving empirical risk minimization[C]//Advances in Neural Information Processing Systems. 2018: 6343-6354.</li>
</ul></li>
<li>自适应分布式ADMM
<ul>
<li>Xu Z, Taylor G, Li H, et al. Adaptive consensus ADMM for distributed optimization[J]. arXiv preprint arXiv:1706.02869, 2017.</li>
</ul></li>
<li>非平滑分布式优化
<ul>
<li>Scaman K, Bach F, Bubeck S, et al. Optimal algorithms for non-smooth distributed optimization in networks[C]//Advances in Neural Information Processing Systems. 2018: 2740-2749.</li>
</ul></li>
<li>分布式近端原对称对偶算法
<ul>
<li>Hong M, Hajinezhad D, Zhao M M. Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex optimization and learning over networks[C]//International Conference on Machine Learning. 2017: 1529-1538.</li>
</ul></li>
<li>投影-free的分布式在线学习
<ul>
<li>Zhang W, Zhao P, Zhu W, et al. Projection-free distributed online learning in networks[C]//International Conference on Machine Learning. 2017: 4054-4062.</li>
</ul></li>
<li>平行倒推
<ul>
<li>Huo Z, Gu B, Yang Q, et al. Decoupled parallel backpropagation with convergence guarantee[J]. arXiv preprint arXiv:1804.10574, 2018.</li>
</ul></li>
</ol>
<h5 id="压缩通信学习">压缩通信学习</h5>
<ol type="1">
<li><p>稀疏化模型</p>
<ul>
<li>Wang J, Kolar M, Srebro N, et al. Efficient distributed learning with sparsity[C]//International Conference on Machine Learning. 2017: 3636-3645.</li>
</ul></li>
<li><p>梯度量化</p>
<ul>
<li>Shen Z, Mokhtari A, Zhou T, et al. Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication[J]. arXiv preprint arXiv:1805.09969, 2018.</li>
</ul>
<p>QSGD</p>
<ul>
<li>Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</li>
</ul>
<p>PCA压缩</p>
<ul>
<li>Garber D, Shamir O, Srebro N. Communication-efficient algorithms for distributed stochastic principal component analysis[J]. arXiv preprint arXiv:1702.08169, 2017.</li>
</ul>
<p><span class="math inline">\(1Bits\)</span>量化</p>
<ul>
<li>Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</li>
<li>Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</li>
</ul></li>
</ol>
<h5 id="错误补偿压缩">错误补偿压缩</h5>
<h6 id="bits量化"><span class="math inline">\(1Bits\)</span>量化</h6>
<ul>
<li>Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</li>
</ul>
<h6 id="二次优化">二次优化</h6>
<ul>
<li>Wu J, Huang W, Huang J, et al. Error compensated quantized SGD and its applications to large-scale distributed optimization[J]. arXiv preprint arXiv:1806.08054, 2018.</li>
</ul>
<h6 id="signsgd">SignSGD</h6>
<ul>
<li>Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed optimisation for non-convex problems[J]. arXiv preprint arXiv:1802.04434, 2018.</li>
<li>Alistarh D, Hoefler T, Johansson M, et al. The convergence of sparsified gradient methods[C]//Advances in Neural Information Processing Systems. 2018: 5973-5983.</li>
</ul>
<h2 id="算法介绍">算法介绍</h2>
<h3 id="算法描述">算法描述</h3>
<p>本文采用参数服务器架构描述该算法，但是算法的应用场景不仅限于参数服务器架构，在第<span class="math inline">\(t\)</span>次迭代，我们将该算法的关键步骤描述如下：</p>
<ul>
<li><strong>工作节点计算</strong></li>
</ul>
<p>每个节点<span class="math inline">\(i\)</span>计算本地随机梯度<span class="math inline">\(\nabla F\left(\boldsymbol{x}_{t} ; \boldsymbol{\zeta}_{t}^{(i)}\right)\)</span>，该梯度基于全局模型<span class="math inline">\(x_t\)</span>以及本地样本<span class="math inline">\(\boldsymbol{\zeta}_{t}^{(i)}\)</span>。这里的<span class="math inline">\(i\)</span>代表工作节点<span class="math inline">\(i\)</span>的索引，<span class="math inline">\(t\)</span>表示本轮的迭代次数</p>
<ul>
<li><strong>工作节点压缩</strong></li>
</ul>
<p>每个工作节点<span class="math inline">\(i\)</span>计算误差补偿随机梯度 <span class="math display">\[
\boldsymbol{\delta}_{t}^{(i)}=\boldsymbol{v}_{t}^{(i)}-Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]
\]</span> 其中<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>表示压缩误差补偿随机梯度</p>
<ul>
<li><strong>参数服务器压缩</strong></li>
</ul>
<p>所有节点将计算所得的<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>量化梯度发送给参数服务器，参数服务器聚合所有量化梯度<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>，并且更新全局误差补偿随机梯度<span class="math inline">\(v_t\)</span>，根据以下式子对梯度误差<span class="math inline">\(\boldsymbol{\delta}_{t}\)</span>进行更新 <span class="math display">\[
\begin{array}{l}
\boldsymbol{v}_{t}=\boldsymbol{\delta}_{t-1}+\frac{1}{n} \sum_{i=1}^{n} Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right] \\
\boldsymbol{\delta}_{t}=\boldsymbol{v}_{t}-Q_{\omega_{t}}\left[\boldsymbol{v}_{t}\right]
\end{array}
\]</span></p>
<ul>
<li><strong>工作节点更新</strong></li>
</ul>
<p>参数服务器将<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>发送给所有工作节点，所有工作节点更新本地模型： <span class="math display">\[
\boldsymbol{x}_{t+1}=\boldsymbol{x}_{t}-\gamma Q_{\omega_{t}}\left[\boldsymbol{v}_{t}\right]
\]</span> 其中<span class="math inline">\(\gamma\)</span>表示学习率</p>
<h3 id="压缩选择">压缩选择</h3>
<p>该方法不像当前存在的方法，并不需要无偏压缩的限制（也就是<span class="math inline">\(\mathbb{E}_{\omega} Q_{\omega}[\boldsymbol{x}]=\boldsymbol{x}\)</span>）。所以选择压缩的方法是非常灵活的。论文例举了多种较为常用的压缩选项：</p>
<h4 id="随机量化">随机量化</h4>
<p>对于任意真实值<span class="math inline">\(z \in[a, b]\)</span>，其中<span class="math inline">\((a,b)\)</span>是定义好的低bit数值，<span class="math inline">\(z\)</span>会有<span class="math inline">\(\frac{b-z}{b-a}\)</span>的概率被压缩到<span class="math inline">\(a\)</span>,有<span class="math inline">\(\frac{z-a}{b-a}\)</span>的概率压缩到<span class="math inline">\(b\)</span>。这种压缩操作是无偏的。</p>
<h4 id="bits量化-1"><span class="math inline">\(1Bits\)</span>量化</h4>
<p>将<span class="math inline">\(x\)</span>向量压缩到<span class="math inline">\(\|x\| \operatorname{sign}(x)\)</span>，其中<span class="math inline">\(sign(x)\)</span>是其中<span class="math inline">\(x\)</span>向量对应元素的符号。这种压缩是有偏的</p>
<h4 id="clipping">Clipping</h4>
<p>对于真实值<span class="math inline">\(z\)</span>，直接设置低于<span class="math inline">\(k\)</span>bis的部分压缩到<span class="math inline">\(0\)</span>。例如，将<span class="math inline">\(1.23456\)</span>压缩为d<span class="math inline">\(1.2\)</span>，直接将其较低的四位变成<span class="math inline">\(0\)</span>。这种压缩是有偏的。</p>
<h4 id="top-k稀疏化">Top-k稀疏化</h4>
<p>对于向量<span class="math inline">\(x\)</span>，将其最大的<span class="math inline">\(k\)</span>个元素进行保留，其余的设置为<span class="math inline">\(0\)</span>。这种操作是有偏的。</p>
<h4 id="随机稀疏化">随机稀疏化</h4>
<p>对于真实值<span class="math inline">\(z\)</span>，有<span class="math inline">\(p\)</span>的概率将<span class="math inline">\(z\)</span>设置为<span class="math inline">\(0\)</span>，以及<span class="math inline">\(p\)</span>的概率设置为<span class="math inline">\(z/p\)</span>。这样的方法是无偏的</p>
<h2 id="数学证明和收敛性分析">数学证明和收敛性分析</h2>
<p>待补充...</p>
<h2 id="实验">实验</h2>
<h3 id="实验设置">实验设置</h3>
<h4 id="数据集和模型">数据集和模型</h4>
<ul>
<li>ResNet-18以及CIFAR-10</li>
</ul>
<h4 id="实现对照组">实现对照组</h4>
<h5 id="doublesqueeze">DOUBLESQUEEZE</h5>
<h6 id="bit压缩"><span class="math inline">\(1-bit\)</span>压缩</h6>
<p>将梯度压缩到<span class="math inline">\(1-bit\)</span>，只包含符号。基于向量考虑，它的比例因子表示为： <span class="math display">\[
\frac{\text { magnitude of compensated gradient }}{\text { magnitude of quantized gradient }}
\]</span></p>
<h6 id="top-k压缩">Top-k压缩</h6>
<h5 id="qsgd">QSGD</h5>
<p>工作节点将梯度压缩成三元表示，其中每个元素用<span class="math inline">\(\{-1,0,1\}\)</span>表示。假设在这个梯度向量各个元素中的最大绝对值为<span class="math inline">\(m\)</span>，对于任意一个元素<span class="math inline">\(e\)</span>，它都以<span class="math inline">\(|e| /|m|\)</span>的可能性压缩到<span class="math inline">\(sign(e)\)</span>，以<span class="math inline">\(1-|e| /|m|\)</span>的可能性压缩到<span class="math inline">\(0\)</span>。扩展因子可以记为： <span class="math display">\[
\frac{\text { magnitude of compensated gradient }}{\text { magnitude of quantized gradient }}
\]</span> 采用这种方法时，参数服务器将梯度分发的时候不会讲梯度再次压缩</p>
<h5 id="vanilla-sgd">Vanilla SGD</h5>
<p>并不采用任何压缩处理</p>
<h5 id="mem-sgd">MEM-SGD</h5>
<p>和DEOUBLESQUEEZE的区别是从参数服务器进行分发的梯度不进行压缩，对于此种方法，本文也去使用了<span class="math inline">\(1-bit\)</span>二和<span class="math inline">\(top-k\)</span>这两中压缩方法。</p>
<h5 id="top-k-sgd">Top-k SGD</h5>
<p>该方法不涉及误差补偿机制</p>
<h3 id="实验结果">实验结果</h3>
<ol type="1">
<li>将<span class="math inline">\(1-bit\)</span>压缩作为DEUBLESQUEEZE的压缩方法，与MEM-SGD, QSGD这些压缩方法做对比</li>
</ol>
<p><img src="/archives/493c0bc7/image-20200724191830893.png" alt="image-20200724191830893" style="zoom:67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192014337.png" alt="image-20200724192014337" style="zoom:67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192043966.png" alt="image-20200724192043966" style="zoom:67%;"></p>
<ol start="2" type="1">
<li>将Top-k压缩作为DEUBLESQUEEZE的压缩方法，与MEM-SGD, QSGD这些压缩方法做对比</li>
</ol>
<p><img src="/archives/493c0bc7/image-20200724192139291.png" alt="image-20200724192139291" style="zoom: 67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192150444.png" alt="image-20200724192150444" style="zoom: 67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192202959.png" alt="image-20200724192202959" style="zoom: 67%;"></p>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1802.04434.pdf" target="_blank" rel="noopener external nofollow noreferrer">Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed Optimisation for Non-Convex Problems[C]//International Conference on Machine Learning. 2018: 560-569.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a>
<div id="refer-anchor-4">

</div>
<div id="refer-anchor-5">

</div>
<div id="refer-anchor-6">

</div>
<div id="refer-anchor-7">

</div>
<div id="refer-anchor-8">

</div>
<div id="refer-anchor-9">

</div></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/5cf57987.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/5cf57987.html" class="post-title-link" itemprop="url">分布式学习通信优化综述（2020香港大学）</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-08-26 14:13:54" itemprop="dateCreated datePublished" datetime="2020-08-26T14:13:54+08:00">2020-08-26</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-08-28 19:30:10" itemprop="dateModified" datetime="2020-08-28T19:30:10+08:00">2020-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/5cf57987.html" class="post-meta-item leancloud_visitors" data-flag-title="分布式学习通信优化综述（2020香港大学）" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/5cf57987.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/5cf57987.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>10k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>9 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>综述名</strong>：Communication-Efficient Distributed Deep Learning: A Comprehensive Survey</p>
<p><strong>发表年限：</strong>2020</p>
<p><strong>简介：</strong>从以下四个维度对分布式训练算法进行介绍</p>
<ol type="1">
<li><p>通信同步和频次</p>
<p>主要有以下两个方向：</p>
<p>（1）宽松同步条件为异步并行（ASGD）或延迟同步并行（SSP）</p>
<p>（2）本地多次迭代后进行通信</p></li>
<li><p>系统架构和梯度聚合</p>
<p>（1）宽松参数服务器架构中的拥塞问题，MPI的去中心化拓扑</p>
<p>（2）Gossip架构</p>
<blockquote>
<p>工作节点能够从听一个或多个节点中获取模型，解决拥塞问题，并且减少了通信量</p>
</blockquote></li>
<li><p>压缩技术</p>
<p>（1）量化</p>
<p>（2）编码</p>
<p>（3）稀疏化</p></li>
<li><p>通信和计算的并行</p>
<p>简称为流水线算法，使得通信和计算充分并行。</p></li>
</ol>
<p>分布式SGD通信优化——以表格方式归纳如下：</p>
<table>
<thead>
<tr class="header">
<th>维度</th>
<th>方法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>通信同步和频次</td>
<td>同步方法（SGD）<br>延迟受限同步<br>异步方法（ASGD）<br>本地SGD</td>
</tr>
<tr class="even">
<td>系统架构</td>
<td>参数服务器架构<br>All-Reduce架构<br>Gossip架构</td>
</tr>
<tr class="odd">
<td>压缩技术</td>
<td>量化<br>编码<br>稀疏化</td>
</tr>
<tr class="even">
<td>计算和同行并行</td>
<td>流水线方法<br>调度方法</td>
</tr>
</tbody>
</table>
<h2 id="分布式学习分类">分布式学习分类</h2>
<h3 id="通信同步和频次">通信同步和频次</h3>
<h4 id="同步并行">同步并行</h4>
<h4 id="受限同步并行">受限同步并行</h4>
<p>[Chen et al.] <a href="#refer-anchor-1"><sup>[1]</sup></a>提出了候补节点<span class="math inline">\(n_e\)</span>。主要思想是当有<span class="math inline">\(n\)</span>个节点到达之后，就不再等待梯度还未到达的<span class="math inline">\(n_e\)</span>个节点。它们的梯度值将会被舍去。</p>
<p>[Ho et al.] <a href="#refer-anchor-2"><sup>[2]</sup></a>提出了SSP模型，设定了一个阈值，允许在阈值之内，计算较快的工作节点迭代更多次数并更新全局参数，但是当最快节点和最慢节点到达阈值时，快速的节点需要等待慢速的节点回归合理阈值。</p>
<p>对于<code>拥塞问题</code>，[Chen et al] 提出了Round-Robin同步并行方法，该算法在整个训练过程中均匀交错工作节点更新，并以固定的循环顺序协调工作节点更新梯度。</p>
<h4 id="异步并行">异步并行</h4>
<p>[Mote et al.] <a href="#refer-anchor-3"><sup>[3]</sup></a>提出了高通信效率的分布式交替方向乘数法（D-ADMM），他是对ADMM算法的扩展。[Wei et al.] <a href="#refer-anchor-6"><sup>[6]</sup></a><a href="#refer-anchor-7"><sup>[7]</sup></a>将其拓展为去中心化的ADMM算法。</p>
<p>之后有更多的学者对ADMM算法进行优化 <a href="#refer-anchor-7"><sup>[7]</sup></a>，[Li et al.] <a href="#refer-anchor-8"><sup>[8]</sup></a>提出了延迟闭塞近端梯度法（Delayed Block Proximal Gradient Method），该算法只在每次迭代过程中异步更新参数中的一块内容，因此在通信的过程中只需要上传一部分参数。</p>
<p>[Grishchenko et al.]提出了一种异步分配算法，其特点是向上通信(从工人到主人)的稀疏化，他们通过对本地更新条目的统一抽样选择来实现稀疏化，这使得通信更加高效。</p>
<p>但是异步的方法往往收敛性得不到保证。</p>
<h4 id="本地sgd">本地SGD</h4>
<p>基本思想是在多轮迭代之后进行通信<a href="#refer-anchor-9"><sup>[9]</sup></a><a href="#refer-anchor-10"><sup>[10]</sup></a><a href="#refer-anchor-11"><sup>[11]</sup></a><a href="#refer-anchor-12"><sup>[12]</sup></a><a href="#refer-anchor-13"><sup>[13]</sup></a><a href="#refer-anchor-14"><sup>[14]</sup></a><a href="#refer-anchor-15"><sup>[15]</sup></a><a href="#refer-anchor-16"><sup>[16]</sup></a><a href="#refer-anchor-17"><sup>[17]</sup></a>。</p>
<p>[Yu et al.] <a href="#refer-anchor-18"><sup>[18]</sup></a>结合了分布式动量的SGD方法和PR-SGD<a href="#refer-anchor-19"><sup>[19]</sup></a>方法来提高了本地SGD方法的性能，并且证明了线性加速比。</p>
<p>[Jiang et al.] <a href="#refer-anchor-20"><sup>[20]</sup></a>将量化方法和本地SGD方法做了结合，降低了通信的复杂度。</p>
<p>其他能够减少通信数据量的方法是提高批量大小。[Yu et al.] <a href="#refer-anchor-21"><sup>[21]</sup></a>提出了Catalyst-like<a href="#refer-anchor-22"><sup>[22]</sup></a><a href="#refer-anchor-23"><sup>[23]</sup></a>算法，能够在每轮迭代之后动态提高批量大小，并达到和SSP方法下沟通的收敛速率。</p>
<p>大批量的SGD方法会使得泛化性能降低。[Lin et al.] <a href="#refer-anchor-24"><sup>[24]</sup></a>提出了post-local SGD方法来解决这个问题。该算法将整个训练过程分为两个阶段，第一阶段采用小批量SGD，第二阶段采用局部SGD。</p>
<h3 id="中心化和去中心化架构">中心化和去中心化架构</h3>
<h3 id="量化方法">量化方法</h3>
<h4 id="方法一">方法一</h4>
<p>数据并行下的梯度量化方法其实是一种分布式平均估计问题<a href="#refer-anchor-25"><sup>[25]</sup></a><a href="#refer-anchor-26"><sup>[26]</sup></a>。[Suresh et al] <a href="#refer-anchor-25"><sup>[25]</sup></a>和[Jakub et al] <a href="#refer-anchor-26"><sup>[26]</sup></a>对分布式平均估计进行了通信效率算法的分析。他们使用均方误差的方法，并提出了一种在给定通信成本下的编码策略以达到最佳的MSE。为了减少通信成本，[Suresh et al.] <a href="#refer-anchor-25"><sup>[25]</sup></a> 提出了两种方法，随机旋转量化（Stochastic Rotated Quantization），所有工作节点和中央服务器生成一个全局随机旋转矩阵，并尝试找到一个正交矩阵<span class="math inline">\(\mathbb{R}\)</span>。变长编码（Variable Length Coding）采用对应每个量化值出现次数的霍夫曼编码算法。</p>
<h4 id="方法二">方法二</h4>
<p>[Sei et al.] <a href="#refer-anchor-27"><sup>[27]</sup></a> 为了降低量化误差带来的负面影响，作者使用了误差补偿技术：每次量化时，把上一次迭代的量化误差加到本次迭代的梯度上，然后再进行量化，接着求出本次量化操作的误差。这种误差补偿机制可以确保所有的梯度都会再一定程度上对模型更新产生作用，只不过这种作用分散在不同的迭代中——类似于一种延迟更新的形式。作者指出，使用误差补偿后，就可以在几乎不损失模型精度的情况下将梯度由32位量化成1位。 <span class="math display">\[
\begin{aligned}
G_{i j \ell}^{\text {quant }}(t) &amp;=\mathcal{Q}\left(G_{i j \ell}(t)+\Delta_{i j \ell}(t-N)\right) \\
\Delta_{i j \ell}(t) &amp;=G_{i j \ell}(t)-\mathcal{Q}^{-1}\left(G_{i j \ell}^{\text {quant }}(t)\right)
\end{aligned}
\]</span> 其中<span class="math inline">\(\mathcal{Q}(\cdot)\)</span>表示量化函数，<span class="math inline">\(G_{i j \ell}^{\text {quant }}(t)\)</span>表示量化之后的整型数值。我们在量化过程中会保证<span class="math inline">\(\Delta_{i j \ell}(t)\)</span>被加到下一轮的梯度过程中（也称为了误差补偿机制）。</p>
<p>举个例子，在具体的实现上，比较简单的方法是将大于<span class="math inline">\(0\)</span>的梯度值编码成为<span class="math inline">\(1\)</span>，小于等于<span class="math inline">\(0\)</span>的梯度值编码为<span class="math inline">\(0\)</span>。在解码的时候，将<span class="math inline">\(1\)</span>编码为<span class="math inline">\(+1\)</span>，将<span class="math inline">\(0\)</span>解码为<span class="math inline">\(-1\)</span>，在进行聚合操作。</p>
<p>其他一些研究采用不精确的近端梯度提出了自适应的量化方法[168][169]，但是这些方法缺乏在深度学习模型种的实践。</p>
<p>考虑到需要同时具备高通信效率和好的收敛性，[Alistarh et al.] <a href="#refer-anchor-28"><sup>[28]</sup></a> 提出了一种基于量化的算法，而不仅仅只是量化方法。这种量化算法叫做QSGD，可以平衡传输的比特数与压缩梯度的方差。</p>
<h4 id="方法三">方法三</h4>
<p>[Wen et al.] <a href="#refer-anchor-29"><sup>[29]</sup></a> 提出了另一种叫做TernGrad的压缩通信量的模式，它可以使用三元梯度来加速分布式深度学习。在TernGrad中，梯度<span class="math inline">\(G(x)\)</span>被量化为三元组<span class="math inline">\(\{-1,0,1\}\)</span>来减少通信量化大小。梯度<span class="math inline">\(G(x)\)</span>量化如下： <span class="math display">\[
\tilde{Q}_{t}\left(G\left(\mathbf{x}_{t}\right)\right)=\operatorname{ternarize}\left(G\left(\mathbf{x}_{t}\right)\right)=s_{t} \cdot \operatorname{sign}\left(G\left(\mathbf{x}_{t}\right)\right) \circ \mathbf{b}_{t}
\]</span> 其中<span class="math inline">\(s_{t}:=\max \left(a b s\left(G\left(\mathbf{x}_{t}\right)\right)\right)\)</span>以及$ <span class="math inline">\(表示哈达玛积。\)</span>()<span class="math inline">\(和SGD中的\)</span>()<span class="math inline">\(是一致的。每个\)</span>b_t$元素遵循如下分布： <span class="math display">\[
\begin{array}{l}
P\left(b_{t, j}=1 \mid G_{t}\left(\mathbf{x}_{t}\right)\right)=\left|G_{t, j}\left(\mathbf{x}_{t}\right) / s_{t}\right| \\
P\left(b_{t, j}=0 \mid G_{t}\left(\mathbf{x}_{t}\right)\right)=1-\left|G_{t, j}\left(\mathbf{x}_{t}\right) / s_{t}\right|
\end{array}
\]</span></p>
<h4 id="方法四">方法四</h4>
<p>Sign-SGD是另外一种量化方法 <a href="#refer-anchor-30"><sup>[30]</sup></a>。在Sign-SGD中，每个工作节点将梯度量化为二进制值，它是梯度向量的每个坐标的符号。[Bernstein et al.] <a href="#refer-anchor-31"><sup>[31]</sup></a>提供了基于该方法在非凸优化上的理论分析。他们证明了当梯度与随机度和曲率一样稠密或更密集时，Sign-SGD可以以一个理论速率收敛。[Bernstein et al.] <a href="#refer-anchor-31"><sup>[31]</sup></a>还提出了一种具有大多数投票的Sign-SGD方法。在工作节点将他们的梯度向量的符号交换到服务器后，整体的更新由多数人投票决定。通过这种方法，将通信成本降低了32倍。</p>
<h4 id="方法五">方法五</h4>
<p>[Wang et al.] <a href="#refer-anchor-32"><sup>[32]</sup></a>提出了一种新的方法叫做原子稀疏化方法（Atomic Sparsification (ATOMO)）。他证明了梯度稀疏化和量化是原子分解过程中梯度稀疏化的一般方法的一部分，例如QSGD、奇异值分解（SVD）、傅里叶分解等。ATOMO的目标是最小化在原子基础上稀疏的稀疏梯度的方差，并保持它作为原始梯度的无偏估计量。它们说明了1位的QSGD和TernGrad是ATOMO的特殊情况。此外，他们用SVD改进了ATOMO，称为Spectral-ATOMO。在他们的实验中，与QSGD和TernGrad相比，Spectral-ATOMO分别减少了2倍和3倍的训练时间。</p>
<h4 id="方法六">方法六</h4>
<p>[Mishchenko et al.] <a href="#refer-anchor-33"><sup>[33]</sup></a>介绍了DIANA这种创新方法，它扩展了QSGD和Terngrad这两种方法，将这个梯度向量划分成多个子向量，并将每个子向量进行独立压缩。</p>
<h4 id="方法七">方法七</h4>
<p>[Sun et al.] <a href="#refer-anchor-34"><sup>[34]</sup></a>的LAQ方法</p>
<h4 id="方法八">方法八</h4>
<p>[Horvth et al.] <a href="#refer-anchor-36"><sup>[36]</sup></a>提出了一种新的压缩方法，叫做自然压缩(Natural Compression)。这种压缩方法定义如下： <span class="math display">\[
C_{n a t}(t):=\left\{\begin{array}{ll}
\operatorname{sign}(t) \cdot 2^{\left\lfloor\log _{2}|t|\right\rfloor}, &amp; \text { with probability } p(t) \\
\operatorname{sign}(t) \cdot 2^{\left\lfloor\log _{2}|t|\right\rfloor}, &amp; \text { with probability } 1-p(t)
\end{array}\right.
\]</span></p>
<p>其中<span class="math inline">\(p(t):=\frac{2^{\left\lceil\log _{2}|t|-|t|\right\rceil}}{2^{\left\lfloor\log _{2}|t|\right\rfloor}}\)</span>。他们提出的这种压缩方法能够将方差忽略不计，因此有较好的收敛性。这个方法的一个优势是<span class="math inline">\(C_{nat}\)</span>能够去掉二进制表示中的尾数。他们提出了与QSGD的抖动是类似的。</p>
<p>[Yu et al.] <a href="#refer-anchor-35"><sup>[35]</sup></a>提出了名为AsyLPGd低精度算法，在异步框架中使用，同时量化梯度和模型参数。它使用额外的要求来限制量化级别。他们将稀疏化方法和AsyLPG相结合，进一步降低通信的复杂度。</p>
<h3 id="稀疏化方法">稀疏化方法</h3>
<h3 id="计算和通信流水线调度">计算和通信流水线调度</h3>
<h2 id="未来优化方向">未来优化方向</h2>
<h2 id="参考文献">参考文献</h2>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="https://arxiv.org/abs/1604.00981" target="_blank" rel="noopener external nofollow noreferrer">Chen J, Pan X, Monga R, et al. Revisiting distributed synchronous SGD[J]. arXiv preprint arXiv:1604.00981, 2016.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="http://papers.nips.cc/paper/4894-more-effective-distributed-ml-via-as" target="_blank" rel="noopener external nofollow noreferrer">Ho Q, Cipar J, Cui H, et al. More effective distributed ml via a stale synchronous parallel parameter server[C]//Advances in neural information processing systems. 2013: 1223-1231.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://home.cse.ust.hk/~weiwa/papers/chen-infocom19.pdf" target="_blank" rel="noopener external nofollow noreferrer">Chen C, Wang W, Li B. Round-robin synchronization: Mitigating communication bottlenecks in parameter servers[C]//IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 2019: 532-540.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="https://ieeexplore.ieee.org/abstract/document/6484993/" target="_blank" rel="noopener external nofollow noreferrer">Mota J F C, Xavier J M F, Aguiar P M Q, et al. D-ADMM: A communication-efficient distributed algorithm for separable optimization[J]. IEEE Transactions on Signal Processing, 2013, 61(10): 2718-2723.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="https://ieeexplore.ieee.org/abstract/document/6425904/" target="_blank" rel="noopener external nofollow noreferrer">Wei E, Ozdaglar A. Distributed alternating direction method of multipliers[C]//2012 IEEE 51st IEEE Conference on Decision and Control (CDC). IEEE, 2012: 5445-5450.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="https://ieeexplore.ieee.org/abstract/document/7472585/" target="_blank" rel="noopener external nofollow noreferrer">Chang T H, Hong M, Liao W C, et al. Asynchronous distributed alternating direction method of multipliers: Algorithm and convergence analysis[C]//2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016: 4781-4785.</a></li>
</ul>
<div id="refer-anchor-7">

</div>
<ul>
<li>[7] <a href="http://www.jmlr.org/proceedings/papers/v32/zhange14.pdf" target="_blank" rel="noopener external nofollow noreferrer">Zhang R, Kwok J. Asynchronous distributed ADMM for consensus optimization[C]//International conference on machine learning. 2014: 1701-1709.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="http://papers.nips.cc/paper/5597-communication-efficient-distributed-machine-learning-with-the-parameter-server" target="_blank" rel="noopener external nofollow noreferrer">Li M, Andersen D G, Smola A J, et al. Communication efficient distributed machine learning with the parameter server[C]//Advances in Neural Information Processing Systems. 2014: 19-27.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="https://arxiv.org/abs/1403.7550" target="_blank" rel="noopener external nofollow noreferrer">Zhang C, Ré C. Dimmwitted: A study of main-memory statistical analytics[J]. arXiv preprint arXiv:1403.7550, 2014.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/abs/1603.04379" target="_blank" rel="noopener external nofollow noreferrer">Bijral A S, Sarwate A D, Srebro N. On data dependence in distributed stochastic optimization[J]. arXiv preprint arXiv:1603.04379, 2016.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="https://arxiv.org/abs/1606.07365" target="_blank" rel="noopener external nofollow noreferrer">Zhang J, De Sa C, Mitliagkas I, et al. Parallel SGD: When does averaging help?[J]. arXiv preprint arXiv:1606.07365, 2016.</a>
<div id="refer-anchor-12">

</div></li>
<li>[12] <a href="http://papers.nips.cc/paper/9288-local-sgd-with-periodic-averaging-tighter-analysis-and-adaptive-synchronization" target="_blank" rel="noopener external nofollow noreferrer">Haddadpour F, Kamani M M, Mahdavi M, et al. Local SGD with periodic averaging: Tighter analysis and adaptive synchronization[C]//Advances in Neural Information Processing Systems. 2019: 11082-11094.</a>
<div id="refer-anchor-13">

</div></li>
<li>[13] <a href="https://www.aclweb.org/anthology/N10-1069.pdf" target="_blank" rel="noopener external nofollow noreferrer">McDonald R, Hall K, Mann G. Distributed training strategies for the structured perceptron[C]//Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics. 2010: 456-464.</a>
<div id="refer-anchor-14">

</div></li>
<li>[14] <a href="http://papers.nips.cc/paper/3881-efficient-large-scale-distributed-training-of-conditional-maximum-entropy-models" target="_blank" rel="noopener external nofollow noreferrer">Mcdonald R, Mohri M, Silberman N, et al. Efficient large-scale distributed training of conditional maximum entropy models[C]//Advances in neural information processing systems. 2009: 1231-1239.</a>
<div id="refer-anchor-15">

</div></li>
<li>[15] <a href="https://ieeexplore.ieee.org/abstract/document/6853589/" target="_blank" rel="noopener external nofollow noreferrer">Zhang X, Trmal J, Povey D, et al. Improving deep neural network acoustic models using generalized maxout networks[C]//2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014: 215-219.</a>
<div id="refer-anchor-16">

</div></li>
<li>[16] <a href="http://papers.nips.cc/paper/5761-deep-learning-with-elastic-averaging-sgd" target="_blank" rel="noopener external nofollow noreferrer">Zhang S, Choromanska A E, LeCun Y. Deep learning with elastic averaging SGD[C]//Advances in neural information processing systems. 2015: 685-693.</a>
<div id="refer-anchor-17">

</div></li>
<li>[17] <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4514" target="_blank" rel="noopener external nofollow noreferrer">Yu H, Yang S, Zhu S. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33: 5693-5700.</a>
<div id="refer-anchor-18">

</div></li>
<li>[18] <a href="https://arxiv.org/abs/1905.03817" target="_blank" rel="noopener external nofollow noreferrer">Yu H, Jin R, Yang S. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization[J]. arXiv preprint arXiv:1905.03817, 2019.</a>
<div id="refer-anchor-19">

</div></li>
<li>[19] <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4514" target="_blank" rel="noopener external nofollow noreferrer">Yu H, Yang S, Zhu S. Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33: 5693-5700.</a>
<div id="refer-anchor-20">

</div></li>
<li>[20] <a href="http://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication" target="_blank" rel="noopener external nofollow noreferrer">Jiang P, Agrawal G. A linear speedup analysis of distributed deep learning with sparse and quantized communication[C]//Advances in Neural Information Processing Systems. 2018: 2525-2536.</a>
<div id="refer-anchor-21">

</div></li>
<li>[21] <a href="https://arxiv.org/abs/1905.04346" target="_blank" rel="noopener external nofollow noreferrer">Yu H, Jin R. On the computation and communication complexity of parallel SGD with dynamic batch sizes for stochastic non-convex optimization[J]. arXiv preprint arXiv:1905.04346, 2019.</a>
<div id="refer-anchor-22">

</div></li>
<li>[22] <a href="http://papers.nips.cc/paper/5928-a-universal-catalyst-for-first-order-optimization" target="_blank" rel="noopener external nofollow noreferrer">Lin H, Mairal J, Harchaoui Z. A universal catalyst for first-order optimization[C]//Advances in neural information processing systems. 2015: 3384-3392.</a>
<div id="refer-anchor-23">

</div></li>
<li>[23] <a href="https://hal.inria.fr/hal-01773296/" target="_blank" rel="noopener external nofollow noreferrer">Paquette C, Lin H, Drusvyatskiy D, et al. Catalyst for gradient-based nonconvex optimization[C]. 2018.</a>
<div id="refer-anchor-24">

</div></li>
<li>[24] <a href="https://arxiv.org/abs/1808.07217" target="_blank" rel="noopener external nofollow noreferrer">Lin T, Stich S U, Patel K K, et al. Don't Use Large Mini-Batches, Use Local SGD[J]. arXiv preprint arXiv:1808.07217, 2018.</a>
<div id="refer-anchor-25">

</div></li>
<li>[25] <a href="http://proceedings.mlr.press/v70/suresh17a.html" target="_blank" rel="noopener external nofollow noreferrer">Suresh A T, Felix X Y, Kumar S, et al. Distributed mean estimation with limited communication[C]//International Conference on Machine Learning. 2017: 3329-3337.</a>
<div id="refer-anchor-26">

</div></li>
<li>[26] <a href="https://www.frontiersin.org/articles/10.3389/fams.2018.00062/full" target="_blank" rel="noopener external nofollow noreferrer">Konečný J, Richtárik P. Randomized distributed mean estimation: Accuracy vs. communication[J]. Frontiers in Applied Mathematics and Statistics, 2018, 4: 62.</a>
<div id="refer-anchor-27">

</div></li>
<li>[27] <a href="https://www.isca-speech.org/archive/interspeech_2014/i14_1058.html" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a></li>
<li><div id="refer-anchor-28">

</div></li>
<li>[28] <a href="https://scholar.google.com.hk/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;q=Qsgd%3A+Randomized+quantization+for+communication-optimal+stochastic+gradient+descent&amp;btnG=" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Li J, Tomioka R, et al. Qsgd: Randomized quantization for communication-optimal stochastic gradient descent[J]. arXiv preprint arXiv:1610.02132, 2016.</a>
<div id="refer-anchor-29">

</div></li>
<li>[29] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-30">

</div></li>
<li>[30] <a href="https://arxiv.org/abs/1802.04434" target="_blank" rel="noopener external nofollow noreferrer">Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed optimisation for non-convex problems[J]. arXiv preprint arXiv:1802.04434, 2018.</a>
<div id="refer-anchor-31">

</div></li>
<li>[31] <a href="https://arxiv.org/abs/1810.05291" target="_blank" rel="noopener external nofollow noreferrer">Bernstein J, Zhao J, Azizzadenesheli K, et al. signSGD with majority vote is communication efficient and fault tolerant[J]. arXiv preprint arXiv:1810.05291, 2018.</a>
<div id="refer-anchor-32">

</div></li>
<li>[32] <a href="http://papers.nips.cc/paper/8191-atomo-communication-efficient-learning-via-atomic-sparsification" target="_blank" rel="noopener external nofollow noreferrer">Wang H, Sievert S, Liu S, et al. Atomo: Communication-efficient learning via atomic sparsification[C]//Advances in Neural Information Processing Systems. 2018: 9850-9861.</a>
<div id="refer-anchor-33">

</div></li>
<li>[33] <a href="https://arxiv.org/abs/1901.09269" target="_blank" rel="noopener external nofollow noreferrer">Mishchenko K, Gorbunov E, Takáč M, et al. Distributed learning with compressed gradient differences[J]. arXiv preprint arXiv:1901.09269, 2019.</a>
<div id="refer-anchor-34">

</div></li>
<li>[34] <a href="http://papers.nips.cc/paper/8598-communication-efficient-distributed-learning-via-lazily-aggregated-quantified-grades" target="_blank" rel="noopener external nofollow noreferrer">Sun J, Chen T, Giannakis G, et al. Communication-efficient distributed learning via lazily aggregated quantized gradients[C]//Advances in Neural Information Processing Systems. 2019: 3370-3380.</a>
<div id="refer-anchor-35">

</div></li>
<li>[35] <a href="http://papers.nips.cc/paper/8694-double-quantization-for-communication-efficient-distributed-optimization" target="_blank" rel="noopener external nofollow noreferrer">Yu Y, Wu J, Huang L. Double quantization for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2019: 4438-4449.</a>
<div id="refer-anchor-36">

</div></li>
<li>[36] <a href="https://arxiv.org/abs/1905.10988" target="_blank" rel="noopener external nofollow noreferrer">Horvath S, Ho C Y, Horvath L, et al. Natural compression for distributed deep learning[J]. arXiv preprint arXiv:1905.10988, 2019.</a>
<div id="refer-anchor-37">

</div></li>
</ul>
<div id="refer-anchor-38">

</div>
<div id="refer-anchor-27">

</div>
<div id="refer-anchor-27">

</div>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/fbb6b1b9.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/fbb6b1b9.html" class="post-title-link" itemprop="url">Kong和Keycloak第三方认证和权限管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-07-27 10:48:15 / 修改时间：15:35:27" itemprop="dateCreated datePublished" datetime="2020-07-27T10:48:15+08:00">2020-07-27</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/API%E7%BD%91%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">API网关</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/API%E7%BD%91%E5%85%B3/Kong-Konga/" itemprop="url" rel="index"><span itemprop="name">Kong&Konga</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/API%E7%BD%91%E5%85%B3/Kong-Konga/%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81/" itemprop="url" rel="index"><span itemprop="name">权限认证</span></a>
                </span>
            </span>

          
            <span id="/archives/fbb6b1b9.html" class="post-meta-item leancloud_visitors" data-flag-title="Kong和Keycloak第三方认证和权限管理" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/fbb6b1b9.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/fbb6b1b9.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="介绍">介绍</h2>
<p>使用Keycloak作为第三方认证服务，Kong作为API网关与Keycloak进行对接，打通两者用户数据库，并使用Kong当中的ACL插件进行接口权限的设计。主要涉及到如下框架：</p>
<ul>
<li>Kong——开源API网关</li>
<li>Keycloak——一个OpenID认证服务</li>
<li>Konga——API网关后台管理可视化界面</li>
</ul>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-17.29.03-1595831384730.png" alt="Screen-Shot-2018-11-15-at-17.29.03"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-17.29.03</figcaption>
</figure>
<p>下文将记录Kong、Konga和Keycloak三者的安装和对接过程，并介绍权限打通的设计思路。以下是主要步骤：</p>
<ol type="1">
<li>创建Dockerfile，创建带有kong-oidc插件的镜像</li>
<li>构建上述镜像</li>
<li>创建<code>docker-compose.yml</code>文件，配置Kong，Konga和Keycloak的相关信息</li>
<li>启动kong-db服务</li>
<li>运行<code>migrations</code></li>
<li>启动Kong服务</li>
<li>验证kong-oidc插件是否可用</li>
<li>使用StreamSet打通Keycloak和Kong用户数据库，进行数据同步</li>
<li>添加kong-oidc插件和ACL插件</li>
<li>测试认证是否可行</li>
</ol>
<h2 id="安装过程">安装过程</h2>
<h3 id="创建dockerfile文件">创建Dockerfile文件</h3>
<p>第一，我们需要创建有关于Kong的镜像。我们还需要在这个镜像的基础上安装kong-oidc插件。我们可以采用以下两种方法进行:</p>
<ol type="1">
<li>修改现有的、正在运行的容器并提交更改</li>
<li>创建Dockerfile文件，并镜像构建</li>
</ol>
<p>我们将采用第二种方法，以下是Dockerfile文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p docker/kong</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> touch docker/kong/Dockerfile</span></span><br></pre></td></tr></table></figure>
<p>使用vim创建并打开文件，写入以下内容：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> kong:<span class="number">1.4</span>.<span class="number">2</span>-centos</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> description=<span class="string">"Centos 7 + Kong 1.4.2 + kong-oidc plugin"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum install -y git unzip &amp;&amp; yum clean all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> luarocks install kong-oidc</span></span><br></pre></td></tr></table></figure>
<p>以上代码将会将安装kong1.4.2版本，以及在此基础上安装kong-oidc插件，接下来构建该文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build -t kong:1.4.2-centos-oidc docker/kong/</span></span><br></pre></td></tr></table></figure>
<p>如果遇到<code>Warning: The directory '/root/.cache/luarocks' or its parent directory is not owned by the current user</code>就忽略。</p>
<h3 id="安装及配置kong">安装及配置Kong</h3>
<p>接下来创建<code>docker-compose.yml</code>文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> touch docker-compose.yml</span></span><br></pre></td></tr></table></figure>
<p>打开这个文件，并进行如下配置</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">version: <span class="string">'3.4'</span></span><br><span class="line"></span><br><span class="line">networks: </span><br><span class="line">  kong-net:</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  kong-datastore:</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  kong-db:</span><br><span class="line">    image: postgres:<span class="number">9.6</span></span><br><span class="line">    volumes:</span><br><span class="line">      - kong-datastore:/var/lib/postgresql/data</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"15432:5432"</span></span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_DB:       api-gw</span><br><span class="line">      POSTGRES_USER:     kong</span><br><span class="line">      POSTGRES_PASSWORD: kong</span><br><span class="line"></span><br><span class="line">  kong:</span><br><span class="line">    image: kong:<span class="number">1.4</span>.<span class="number">2</span>-centos-oidc</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-db</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8000:8000"</span> <span class="comment"># Listener</span></span><br><span class="line">      - <span class="string">"8001:8001"</span> <span class="comment"># Admin API</span></span><br><span class="line">      - <span class="string">"8443:8443"</span> <span class="comment"># Listener  (SSL)</span></span><br><span class="line">      - <span class="string">"8444:8444"</span> <span class="comment"># Admin API (SSL)</span></span><br><span class="line">    environment:</span><br><span class="line">      KONG_DATABASE:         postgres</span><br><span class="line">      KONG_PG_HOST:          kong-db</span><br><span class="line">      KONG_PG_PORT:          <span class="number">5432</span></span><br><span class="line">      KONG_PG_DATABASE:      api-gw</span><br><span class="line">      KONG_PROXY_ACCESS_LOG: /dev/stdout</span><br><span class="line">      KONG_ADMIN_ACCESS_LOG: /dev/stdout</span><br><span class="line">      KONG_PROXY_ERROR_LOG:  /dev/stderr</span><br><span class="line">      KONG_ADMIN_ERROR_LOG:  /dev/stderr</span><br><span class="line">      KONG_PROXY_LISTEN:     <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8000</span>, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8443</span> ssl</span><br><span class="line">      KONG_ADMIN_LISTEN:     <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8001</span>, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8444</span> ssl</span><br><span class="line">      KONG_PLUGINS:          bundled,oidc</span><br></pre></td></tr></table></figure>
<p>接下来，采用下面的命令启动kong-db服务，其中<code>-d</code>是告诉docker在后台运行Docker Compose进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d kong-db</span></span><br></pre></td></tr></table></figure>
<p>验证服务是否已经启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>接下来将迁移kong-db数据库，采用<code>migrations</code>命令。下面这个命令将驱动一个kong服务，<code>-rm</code>表示该服务将在命令运行之后关闭。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose run --rm kong kong migrations up</span></span><br></pre></td></tr></table></figure>
<p>最后，我们将Kong启动，并且检查其运行状态</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d kong</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>检查Kong的管理API，检查OIDC插件是否可以在服务器上使用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://localhost:8001 | jq .plugins.available_on_server.oidc</span></span><br></pre></td></tr></table></figure>
<p>将返回<code>true</code>。但是，虽然OIDC插件在Kong上可用，但是还没有配置Keycloak，实际上还不能进行API的保护。</p>
<p>至此为止，就完成了Kong的安装</p>
<hr>
<h3 id="安装konga可视化终端">安装Konga可视化终端</h3>
<p>Konga是开源的Kong可视化界面，方面API网关管理人员对Kong进行可视化管理，安装方法和Kong类似。</p>
<p>配置<code>docker-compose.yml</code>文件，添加如下内容：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">konga-prepare:</span><br><span class="line">     image: pantsel/konga:next</span><br><span class="line">     command: <span class="string">"-c prepare -a postgres -u postgresql://kong:kong@kong-db:5432/konga_db"</span></span><br><span class="line">     networks:</span><br><span class="line">       - kong-net</span><br><span class="line">     restart: on-failure</span><br><span class="line">     links:</span><br><span class="line">       - kong-db</span><br><span class="line">     depends_on:</span><br><span class="line">       - kong-db</span><br></pre></td></tr></table></figure>
<p>首先将konga的postgresql数据启动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d konga-prepare</span></span><br></pre></td></tr></table></figure>
<p>检查是否启动成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>接下来在<code>docker-compose.yml</code>再配置Konga镜像：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">konga:</span><br><span class="line">    image: pantsel/konga:latest</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    environment:</span><br><span class="line">      DB_ADAPTER: postgres</span><br><span class="line">      DB_HOST: kong-db</span><br><span class="line">      DB_USER: kong</span><br><span class="line">      DB_DATABASE: konga_db</span><br><span class="line">      NODE_ENV: production</span><br><span class="line">      DB_PASSWORD: kong</span><br><span class="line">    depends_on: </span><br><span class="line">      - kong-db</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"1337:1337"</span></span><br></pre></td></tr></table></figure>
<p>再次启动konga</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d konga</span></span><br></pre></td></tr></table></figure>
<p>检查是否启动成功：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>用浏览器访问“IP地址:1337”端口，注册后台管理用户名密码，检查是否能够正常访问，进入后台管理界面后，配置和Kong的对接：</p>
<p><img src="/archives/fbb6b1b9/image-20200727141443389.png" alt="image-20200727141443389" style="zoom:67%;"></p>
<p>对接完成后，可以用Konga查看并配置Kong网关的信息了。</p>
<h3 id="安装keycloak">安装Keycloak</h3>
<p>本节欸将重点配置Keycloak的安装。我们还是采用docker对Keycloak进行安装。</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-19.38.47.png" alt="Screen-Shot-2018-11-15-at-19.38.47"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-19.38.47</figcaption>
</figure>
<p>在本节中，我们将运行以下步骤：</p>
<ol type="1">
<li>修改<code>docker-compose.yml</code>来添加Keycloak和它的可视化终端服务</li>
<li>注册Keycloak数据库服务</li>
<li>注册Keycloak服务</li>
<li>登录Keycloak</li>
<li>为Kong添加Keycloak终端</li>
<li>添加新用户</li>
</ol>
<h4 id="使用docker安装keycloak">使用Docker安装Keycloak</h4>
<p>重新打开<code>docker-compose.yml</code>.为Keycloak添加网络：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">networks: </span><br><span class="line">  kong-net:</span><br><span class="line">  keycloak-net:</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  kong-datastore:</span><br><span class="line">  keycloak-datastore:</span><br></pre></td></tr></table></figure>
<p>接下来将Keycloak数据库添加到<code>docker-compose.yml</code>中：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">services:</span><br><span class="line">  ...</span><br><span class="line">  keycloak-db:</span><br><span class="line">    image: postgres:<span class="number">9.6</span></span><br><span class="line">    volumes: </span><br><span class="line">      - keycloak-datastore:/var/lib/postresql/data</span><br><span class="line">    networks:</span><br><span class="line">      - keycloak-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"25432:5432"</span></span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_DB:       keycloak</span><br><span class="line">      POSTGRES_USER:     keycloak</span><br><span class="line">      POSTGRES_PASSWORD: password</span><br></pre></td></tr></table></figure>
<p>启动服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d keycloak-db</span></span><br></pre></td></tr></table></figure>
<p>验证是否可以使用（确保它的状态是启动的）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>下一步，将Keycloak添加到<code>docker-compose.yml</code>的services中：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">services:</span><br><span class="line">  ...</span><br><span class="line">  keycloak:</span><br><span class="line">    image: jboss/keycloak:<span class="number">4.5</span>.<span class="number">0</span>.Final</span><br><span class="line">    depends_on:</span><br><span class="line">      - keycloak-db</span><br><span class="line">    networks:</span><br><span class="line">      - keycloak-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8180:8080"</span></span><br><span class="line">    environment:</span><br><span class="line">      DB_VENDOR:   POSTGRES</span><br><span class="line">      DB_ADDR:     keycloak-db</span><br><span class="line">      DB_PORT:     <span class="number">5432</span></span><br><span class="line">      DB_DATABASE: keycloak</span><br><span class="line">      DB_USER:     keycloak</span><br><span class="line">      DB_PASSWORD: password</span><br><span class="line">      KEYCLOAK_USER:     admin</span><br><span class="line">      KEYCLOAK_PASSWORD: admin</span><br></pre></td></tr></table></figure>
<p>最后启动Keycloak服务：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d keycloak</span></span><br></pre></td></tr></table></figure>
<p>验证服务是否可用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<h4 id="添加客户端到keycloak中">添加客户端到Keycloak中</h4>
<p>点击Administration Console进入到控制台当中，此时需要输入用户名和密码：</p>
<p>在<code>Master</code>域下，创建客户端：</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.38.29.png" alt="Screen-Shot-2018-11-15-at-20.38.29"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-20.38.29</figcaption>
</figure>
<p>在添加客户端页面，填写“客户端ID”为kong，点击保存按钮。</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.43.23.png" alt="Screen-Shot-2018-11-15-at-20.43.23"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-20.43.23</figcaption>
</figure>
<p>在详情页面中，可以看到“Access Type”，我们需要选择的是“Confidential”，其中“Root URL”是“IP地址:8000”（Kong所接管的端口），以及“Valid Redirect URIs”填写“/*”就可以了，我们的设置如下：</p>
<figure>
<img src="/archives/fbb6b1b9/image-20200727135258550.png" alt="image-20200727135258550"><figcaption aria-hidden="true">image-20200727135258550</figcaption>
</figure>
<p>点击保存之后，在“Credentials”页面中会有对应的“Secret”，这个密钥是需要在Kong当中进行配置的</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.52.51.png" alt="Screen-Shot-2018-11-15-at-20.52.51"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-20.52.51</figcaption>
</figure>
<h4 id="为keycloak添加用户">为Keycloak添加用户</h4>
<p>要添加用户，单击左侧侧边栏的“Users”选项卡，然后单击右侧的“add user”按钮</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.58.07.png" alt="Screen-Shot-2018-11-15-at-20.58.07"><figcaption aria-hidden="true">Screen-Shot-2018-11-15-at-20.58.07</figcaption>
</figure>
<p>在下一页，将“Username”设置为“用户”，并将“Email Enable”开关设置为“On”。然后，点击“Save”按钮。</p>
<p><img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-21.00.07.png" alt="Screen-Shot-2018-11-15-at-21.00.07" style="zoom:67%;"></p>
<p>点击“Credentials”选项卡，输入密码，确认，确保“Temporary”开关设置为“关闭”。然后，点击“Save”按钮。</p>
<p>至此，在Keycloak当中的配置已经完成了。</p>
<hr>
<h3 id="将kong和keycloak进行对接">将Kong和Keycloak进行对接</h3>
<p>接下来将配置Keycloak和Kong的对接部分</p>
<p><img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-21.06.44.png" alt="Screen-Shot-2018-11-15-at-21.06.44" style="zoom:67%;"></p>
<ol type="1">
<li><p>创建服务和路由（此处省略）</p></li>
<li><p>在全局范围内安装oidc插件</p>
<p>点击左侧“Plugin”后，再点击“ADD GLOBAL PLUGINS”，在Other当中，又OIDC插件，找到并点击“ADD PLUGIN”，会弹出如下表单，表单当中的条目信息可以参考：https://github.com/nokia/kong-oidc在GitHub上的首页。</p>
<p>其中以下几个需要重点关注：</p>
<ul>
<li>client_id：需要和Keycloak中创建的client_id对应一致</li>
<li>client_secret：需要需要和Keycloak中自动生成的secret一致</li>
<li>realm：默认就是Keycloak中的master域</li>
<li>redirect after logout uri：/</li>
<li>discovery：根据OIDC填写：http://XXXXXX:8180/auth/realms/master/.well-known/openid-configuration</li>
</ul></li>
<li><p>配置完成后，访问Kong接管的API接口，会自动跳转到Keycloak的登录界面，说明Keycloak和Kong对接完成</p>
<p><img src="/archives/fbb6b1b9/image-20200727142400435.png" alt="image-20200727142400435" style="zoom:67%;"></p></li>
</ol>
<h2 id="存在问题">存在问题</h2>
<p>官方给出了以上配置虽然打通了Kong和Keycloak，但是由于登录是Keycloak进行管理，采用的是Keycloak中的用户，而这些用户和Kong当中的“Consumer”是分离的，而且此时不能够通过Keycloak进行用户权限的管理。</p>
<p>如果将用户管理交给Keycloak，就会架空Kong当中的Consumer，最好是有办法将Kong中的Consumer和Keycloak中的User打通</p>
<h3 id="目标">目标</h3>
<ul>
<li>打通Keycloak和Kong中的用户表</li>
<li>能够使得ACL插件进行白名单和黑名单的访问控制</li>
</ul>
<h3 id="方法">方法</h3>
<ol type="1">
<li>使用ETL工具两者数据表，将ID和用户名进行同步</li>
<li>修改ACL插件使其能够获取到认证过后的<code>authenticated_consumer</code></li>
</ol>
<h3 id="操作步骤">操作步骤</h3>
<h4 id="etl工具同步">ETL工具同步</h4>
<p>此处过程省略</p>
<h4 id="修改acl插件">修改ACL插件</h4>
<p>在kong-oidc插件官方文档中，给出了<code>X-Userinfo</code>是被注入在请求头当中的，例如：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-Userinfo: &#123;"preferred_username":"alice","id":"60f65308-3510-40ca-83f0-e9c0151cc680","sub":"60f65308-3510-40ca-83f0-e9c0151cc680"&#125;</span><br></pre></td></tr></table></figure>
<p>而该插件同样在<code>ngx.ctx.authenticated_consumer</code>中设置了变量，他能够支持其他插件对认证通过用户进行操作，因此，可以让ACL获取该信息，利用该信息和现有“Consumer”做匹配。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ngx.ctx.authenticated_credential = &#123;</span><br><span class="line">    id = "60f65308-3510-40ca-83f0-e9c0151cc680",   -- sub field from Userinfo</span><br><span class="line">    username = "alice"                             -- preferred_username from Userinfo</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改如下：</p>
<ul>
<li><p>进入docker容器当中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo docker <span class="built_in">exec</span> -it  容器ID /bin/bash</span></span><br></pre></td></tr></table></figure></li>
<li><p>进入Kong插件所在目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/share/lua/5.1/kong/plugins/acl</span></span><br></pre></td></tr></table></figure></li>
<li><p>打开group.lua文件，对<code>get_current_consumer_id</code>做修改</p>
<figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local function get_current_consumer_id()</span><br><span class="line">  kong.log.err("oidc info:", ngx.ctx.authenticated_credential.id)</span><br><span class="line">  return (kong.client.get_consumer() or EMPTY).id or</span><br><span class="line">         (kong.client.get_credential() or EMPTY).consumer_id or</span><br><span class="line"><span class="addition">+        (ngx.ctx.authenticated_credential or EMPTY).id</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>这样就可以做到Kong和Keycloak真正打通。</p>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/5c29dc6b.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/5c29dc6b.html" class="post-title-link" itemprop="url">AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-10-07 10:33:24 / 修改时间：19:59:42" itemprop="dateCreated datePublished" datetime="2020-10-07T10:33:24+08:00">2020-10-07</time>
            </span>
            
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文阅读笔记</span></a>
                </span>
            </span>

          
            <span id="/archives/5c29dc6b.html" class="post-meta-item leancloud_visitors" data-flag-title="AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/5c29dc6b.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/5c29dc6b.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="accpar针对异构深度学习加速器的张量划分方法">AccPar：针对异构深度学习加速器的张量划分方法</h2>
<ul>
<li>作者：Linghao Song 南加利福尼亚大学 杜克大学</li>
<li>IEEE HPCA（High Performance Computer Architecture）</li>
</ul>
<h3 id="概括">概括</h3>
<p>提出了AccPar，在异构加速阵列上划分张量的方法，和其他方法相比，AccPar有着更加完备的张量划分空间（tensor partition space）同时能够产生其他方法未知的并行策略。</p>
<p>AccPar依赖于异构环境下，计算和通信消耗（computation and communication costs）。而且AccPar可以根据加速器的异构情况，使用灵活的划分比率来适应异构环境。</p>
<p>相比于OWT（one weird trick）方法、HYPAR方法以及传统的数据并行方法，分别提升了2.98倍、3.78倍和6.30倍。</p>
<h3 id="背景介绍">背景介绍</h3>
<h5 id="现有方法">现有方法</h5>
<h6 id="one-weird-trickowt">One Weird Trick（OWT）</h6>
<p>对于卷积层采用数据并行，对于全连接层采用模型并行</p>
<h6 id="hypar">HYPAR</h6>
<p>提出了一种最小化数据通信的并行方法，能够比OWT拥有更好的效果。</p>
<p>这些方法存在如下问题：</p>
<ol type="1">
<li>并行方法的搜索基于不完备的设计空间</li>
<li>只能处理线性结构的DNN体系结构</li>
<li>它们只考虑了通信而没有考虑计算</li>
<li>假设是在设备同构的环境下</li>
</ol>
<h5 id="accpar方法">AccPar方法</h5>
<ol type="1">
<li>提出了完备的张量划分空间
<ul>
<li>批量（batch size）</li>
<li>数据数据尺寸（imput data size）</li>
<li>输出数据尺寸（output data size）</li>
</ul></li>
<li>提出了异构环境下communication cost和computation cost</li>
<li>灵活变化划分比率</li>
</ol>
<blockquote>
<p>总的来说，该方法能够感知计算资源的异构性和网络带宽能力，从而实现张量的划分。</p>
</blockquote>
<p>作者先对现有DNN加速器和加速框架和它们不同的细分领域做了梳理：</p>
<figure>
<img src="/archives/5c29dc6b/image-20201007185047869.png" alt="image-20201007185047869"><figcaption aria-hidden="true">image-20201007185047869</figcaption>
</figure>
<blockquote>
<p>包括FPGA加速、数据流处理（Dataflow）、PIM（Processing-in-memory）以及其他新型的并行框架</p>
</blockquote>
<h3 id="张量划分空间">张量划分空间</h3>
<ul>
<li>批量（batch size）</li>
<li>数据数据尺寸（imput data size）</li>
<li>输出数据尺寸（output data size）</li>
</ul>
<p>符号简要介绍：</p>
<table>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbf{F}_{l}\)</span></td>
<td>输入到第<span class="math inline">\(l\)</span>层的特征矩阵（第<span class="math inline">\(l-1\)</span>层输出的特征向量）</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{E}_{l-1}\)</span></td>
<td>输入到第<span class="math inline">\(l\)</span>层的误差矩阵（第<span class="math inline">\(l+1\)</span>层输出的误差矩阵）</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{W}_{l}\)</span></td>
<td>第<span class="math inline">\(l\)</span>层的参数</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\triangle \mathbf{W}_{l}\)</span></td>
<td>第<span class="math inline">\(l\)</span>层的梯度</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(B\)</span></td>
<td>Mini-batch size</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{D}_{i,l}\)</span></td>
<td>第<span class="math inline">\(l\)</span>层输入数据尺寸</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{D}_{0,l}\)</span></td>
<td>第<span class="math inline">\(l\)</span>层输出数据尺寸</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha, \beta\)</span></td>
<td>划分比率</td>
</tr>
</tbody>
</table>
<h4 id="问题阐述">问题阐述</h4>
<p>一个DNN网络涉及下列三个重要环节：</p>
<ol type="1">
<li>前传：Forward <span class="math inline">\(\mathbf{F}_{l+1}=f\left(\mathbf{F}_{l} \times \mathbf{W}_{l}\right)\)</span></li>
<li>后传：Backward <span class="math inline">\(\mathbf{E}_{l}=\left(\mathbf{E}_{l+1} \times \mathbf{W}_{l}^{\top}\right) \odot f^{\prime}\left(\mathbf{F}_{l}\right)\)</span></li>
<li>求梯度：Gradient <span class="math inline">\(\Delta \mathbf{W}_{l}=\mathbf{F}_{l}^{\top} \times \mathbf{E}_{l+1}\)</span></li>
</ol>
<p>用数据尺寸可表示为：</p>
<ol type="1">
<li>前传：Forward <span class="math inline">\(\left(B, D_{o, l}\right) \leftarrow\left(B, D_{i, l}\right) \times\left(D_{i, l}, D_{o, l}\right)\)</span></li>
<li>后传：Backward <span class="math inline">\(\left(B, D_{i, l}\right) \leftarrow\left(B, D_{o, l}\right) \times\left(D_{o, l}, D_{i, l}\right)\)</span></li>
<li>求梯度：Gradient <span class="math inline">\(\left(D_{i, l}, D_{o, l}\right) \leftarrow\left(D_{i, l}, B\right) \times\left(B, D_{o, l}\right)\)</span></li>
</ol>
<h4 id="划分维度">划分维度</h4>
<p>假设1：<span class="math inline">\(\mathbf{F}_{l}\)</span>和<span class="math inline">\(\mathbf{E}_{l}\)</span>采用相同的划分方式</p>
<blockquote>
<p>原因：如果采用不相同的划分方式，会导致引入额外的通信，这和最小化communication costs相悖。</p>
</blockquote>
<p>假设2：这几个维度并不是独立的，所以每次只有一个维度能够划分</p>
<blockquote>
<p>举例：以前传：Forward为例<span class="math inline">\(\left(B, D_{o, l}\right) \leftarrow\left(B, D_{i, l}\right) \times\left(D_{i, l}, D_{o, l}\right)\)</span>，如果我们选定要划分<span class="math inline">\(B\)</span>这个维度</p>
<ol type="1">
<li>其中<span class="math inline">\(\mathbf{D}_{i,l}\)</span>不能划分，如果划分了矩阵乘法就无法进行了。</li>
<li><span class="math inline">\(D_{o, l}\)</span>也不能划分，如果划分，最后得到的矩阵只有左上右下或者右上左下的部分矩阵计算</li>
</ol>
</blockquote>
<p>以下内容只以Type-1作为例子进行详细阐述，其余的划分方法原理相同。（假设两块加速卡）</p>
<h5 id="type-1划分b维度">Type-1：划分B维度</h5>
<h6 id="前传forward">前传：Forward</h6>
<p><span class="math display">\[
\mathbf{F}_{l+1}[b, q o]=\sum_{q i \in\left\{1, \cdots, D_{i, l}\right\}} \mathbf{F}_{l}[b, q i] \times \mathbf{W}_{l}[q i, q o]
\]</span></p>
<p>其中一块数据划分<span class="math inline">\(\mathbf{F}_{l}[1: \alpha B,:]\)</span>，另一块<span class="math inline">\(\mathbf{F}_{l}[\alpha B+1: B,:]\)</span></p>
<figure>
<img src="/archives/5c29dc6b/image-20201007191707379.png" alt="image-20201007191707379"><figcaption aria-hidden="true">image-20201007191707379</figcaption>
</figure>
<h6 id="后传backward">后传：Backward</h6>
<p><span class="math display">\[
\mathbf{E}_{l}[b, q i]=\sum_{q o \in\left\{1, \cdots, D_{o, l}\right\}} \mathbf{E}_{l+1}[b, q o] \times \mathbf{W}_{l}^{\top}[q o, q i]
\]</span></p>
<p>其中一块误差划分<span class="math inline">\(\mathbf{E}_{l+1}[1: \alpha B,:]\)</span>，另一块：<span class="math inline">\(\mathbf{E}_{l}[\alpha B+1: B,:]\)</span></p>
<figure>
<img src="/archives/5c29dc6b/image-20201007191855397.png" alt="image-20201007191855397"><figcaption aria-hidden="true">image-20201007191855397</figcaption>
</figure>
<h6 id="梯度计算">梯度计算</h6>
<p><span class="math display">\[
\Delta \mathbf{W}_{l}[q i, q o]=\sum_{b \in\{1, \cdots, B\}} \mathbf{F}_{l}^{\top}[q i, b] \times \mathbf{E}_{l+1}[b, q o]
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\Delta \mathbf{W}_{l}[q i, q o]=&amp; \sum_{b \in\{1, \cdots, \alpha B\}} \mathbf{F}_{l}^{\top}[q i, b] \times \mathbf{E}_{l+1}[b, q o] \\
&amp;+\sum_{b \in\{\alpha B+1, \cdots, B\}} \mathbf{F}_{l}^{\top}[q i, b] \times \mathbf{E}_{l+1}[b, q o]
\end{aligned}
\]</span></p>
<figure>
<img src="/archives/5c29dc6b/image-20201007192005535.png" alt="image-20201007192005535"><figcaption aria-hidden="true">image-20201007192005535</figcaption>
</figure>
<blockquote>
<p>这里将产生层内通信（intra-layer），因为最后的结果必须将两块加速卡上计算出的部分梯度相加得到梯度结果。</p>
</blockquote>
<h5 id="type-2划分mathbfd_il维度">Type-2：划分<span class="math inline">\(\mathbf{D}_{i,l}\)</span>维度</h5>
<p>同样的分析方式，只不过划分<span class="math inline">\(\mathbf{D}_{i,l}\)</span>维度的层内（intra-layer）通信是在前传：Forward过程中产生，因为： <span class="math display">\[
\begin{aligned}
\mathbf{F}_{l+1}[b, q o]=&amp; \sum_{q i \in\left\{1, \cdots, \alpha D_{i, l}\right\}} \mathbf{F}_{l}[b, q i] \times \mathbf{W}_{l}[q i, q o] \\
&amp;+\sum_{q i \in\left\{\alpha D_{i, l}+1, \cdots, D_{i, l}\right\}} \mathbf{F}_{l}[b, q i] \times \mathbf{W}_{l}[q i, q o]
\end{aligned}
\]</span></p>
<h5 id="type-3划分mathbfd_ol维度">Type-3：划分<span class="math inline">\(\mathbf{D}_{o,l}\)</span>维度</h5>
<p>在后传：Backward的过程中产生层内（intra-layer）通信 <span class="math display">\[
\begin{aligned}
\mathbf{E}_{l}[b, q i] &amp;=\sum_{q o \in\left\{1, \cdots, \alpha D_{o, l}\right\}} \mathbf{E}_{l+1}[b, q o] \times \mathbf{W}_{l}^{\top}[q o, q i] \\
&amp;+\sum_{q o \in\left\{\alpha D_{o, l}+1, \cdots, D_{o, l}\right\}} \mathbf{E}_{l+1}[b, q o] \times \mathbf{W}_{l}^{\top}[q o, q i]
\end{aligned}
\]</span></p>
<h4 id="和hypar以及owt对比">和Hypar以及OWT对比</h4>
<p>OWT是特殊的数据和模型并行</p>
<p>HyPar是不完备的搜索空间，只考虑了Type-1和Type-2</p>
<h3 id="accpar的cost-model">AccPar的COST MODEL</h3>
<h5 id="communication-cost-model">Communication Cost Model</h5>
<p><span class="math display">\[
E_{\mathrm{cm}}=\frac{\mathbb{A}(\mathbf{T})}{b_{i}}
\]</span></p>
<p><span class="math inline">\(\mathbb{A}(\mathbf{T})\)</span>是虽有维度的长度，例如：shape是（2,2,4,4）的张量，其<span class="math inline">\(\mathbb{A}(\mathbf{T})\)</span>就是为<span class="math inline">\(2*2*4*4\)</span>，<span class="math inline">\(b_i\)</span>代表加速器<span class="math inline">\(i\)</span>的带宽。</p>
<h6 id="层内通信intra-layer">层内通信（Intra-layer）</h6>
<p>以Type-1中的梯度计算为例：如果划分比例是<span class="math inline">\(\alpha\)</span>；</p>
<p>对于每个<span class="math inline">\(b \in\{1, \cdots, \alpha B\}\)</span>，都有<span class="math inline">\(\mathbb{A}\left(\mathbf{F}_{l}^{\top}[:, b] \times \mathbf{E}_{l+1}[b,:]\right)=D_{i, l} \cdot D_{o, l}=\mathbb{A}\left(\triangle \mathbf{W}_{l}\right)=\mathbb{A}\left(\mathbf{W}_{l}\right)\)</span></p>
<h6 id="层间通信inter-layer">层间通信（Inter-layer）</h6>
<p>总共有Type1-3种划分方式，这一层到下一层排列组合就有9种层间通信的方式，举例来说：</p>
<figure>
<img src="/archives/5c29dc6b/image-20201007193609409.png" alt="image-20201007193609409"><figcaption aria-hidden="true">image-20201007193609409</figcaption>
</figure>
<ol type="1">
<li><p>（a）Type1 到 2；（f）Type2 到 3 以及 （h）Type 3 到 2</p>
<p>层间的通信为0，由于从<span class="math inline">\(l\)</span>（绿）到<span class="math inline">\(l+1\)</span>层（蓝）的划分是完全相同的，所以不需要额外的通信。</p></li>
<li><p>（c）Type1 到 3；（d）Type2 到 1；（e）Type 2 到 2 以及（i）Type 3 到 3</p>
<p>以（c）图为例，在前传的过程中，加速器<span class="math inline">\(i\)</span>的划分比率是<span class="math inline">\(\alpha\)</span>，它具有绿色阴影部分的张量，但是下一层这个加速器采用Type4的划分方式，需要完整的蓝色的张量，他们之间相差了黑色部分，需要进行通信，该通信称为层间通信，<span class="math inline">\((\beta B) \times D_{o, l}=\beta \mathbb{A}\left(\mathbf{F}_{l+1}\right)\)</span>。</p></li>
<li><p>（b）Type1 到 2；（g）Type3 到 1</p>
<p>同理，此处不再赘述。</p></li>
</ol>
<h5 id="computation-cost-model">Computation Cost Model</h5>
<p><span class="math display">\[
E_{\mathrm{cp}}=\frac{\alpha \cdot \mathbb{C}\left(\mathbf{T}_{1} \times \mathbf{T}_{2}\right)}{c_{i}}
\]</span></p>
<p>分子是划分后张量所需的精度位数（bits），分母是加速器计算密度（tensor computation density）</p>
<p>并不难理解</p>
<blockquote>
<p>以上都是以全连接层举例的，同样的可以扩展到卷积层，只不过卷积层采用4维的张量进行表示，分析方法相同，此处不再赘述。</p>
</blockquote>
<h3 id="accpar的划分算法">AccPar的划分算法</h3>
<p>采用动态规划的方法，采用递归定义的方式，定义第<span class="math inline">\(L_i\)</span>层的累计cost，公式如下： <span class="math display">\[
c\left(L_{i+1}, t\right)=\min _{t t \in \mathscr{T}}\left\{c\left(L_{i}, t t\right)+E_{\mathrm{cp}}(t)+E_{\mathrm{cm}}(t t, t)\right\}
\]</span> 其中<span class="math inline">\(t \in \mathscr{T}=\{\text { Type-I, Type-II, Type-III }\}\)</span>，<span class="math inline">\(E_{\mathrm{cp}}(t)\)</span>表示当前层<span class="math inline">\(L_{i+1}\)</span>采用<span class="math inline">\(t\)</span>这种划分方式产生的computation cost，<span class="math inline">\(E_{\mathrm{cm}}(t t, t)\)</span>表示层间和层内的cost和。</p>
<p>最小化通信代价和计算代价即可</p>
<figure>
<img src="/archives/5c29dc6b/image-20201007195435976.png" alt="image-20201007195435976"><figcaption aria-hidden="true">image-20201007195435976</figcaption>
</figure>
<p>对于每个基本的分区类型，在算法执行期间，我们需要记录到前一层的路径，以便在遍历所有层之后回溯，如图中的黑色箭头所示。</p>
<h4 id="划分比率">划分比率</h4>
<p>划分比率是为了适用异构环境下的加速阵列，基本思路是使得两个加速器通信和计算时间尽量接近，以缩短同步等待时间，因此可以采用如下方法： <span class="math display">\[
\begin{array}{r}
\alpha \cdot E_{\mathrm{cp}}\left(p_{i, l}\right)+\alpha \cdot E_{\mathrm{cm}}\left(p_{i, l}\right) \\
=\beta \cdot E_{\mathrm{cp}}\left(p_{j, l}\right)+\beta \cdot E_{\mathrm{cm}}\left(p_{j, l}\right)
\end{array}
\]</span></p>
<h3 id="实验">实验</h3>
<p>主要和数据并行、OWT以及HyPar进行加速比的对比实验，如下图所示：</p>
<ul>
<li><p>同构</p>
<figure>
<img src="/archives/5c29dc6b/image-20201007195855700.png" alt="image-20201007195855700"><figcaption aria-hidden="true">image-20201007195855700</figcaption>
</figure></li>
<li><p>异构</p>
<figure>
<img src="/archives/5c29dc6b/image-20201007195910275.png" alt="image-20201007195910275"><figcaption aria-hidden="true">image-20201007195910275</figcaption>
</figure></li>
</ul>

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/740c8dcf.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/archives/740c8dcf.html" class="post-title-link" itemprop="url">Alpha策略</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-22 19:35:06" itemprop="dateCreated datePublished" datetime="2020-09-22T19:35:06+08:00">2020-09-22</time>
            </span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-29 18:41:40" itemprop="dateModified" datetime="2020-09-29T18:41:40+08:00">2020-09-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%95%E8%B5%84/" itemprop="url" rel="index"><span itemprop="name">投资</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%95%E8%B5%84/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/" itemprop="url" rel="index"><span itemprop="name">量化投资</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%8A%95%E8%B5%84/%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84/Alpha%E7%AD%96%E7%95%A5/" itemprop="url" rel="index"><span itemprop="name">Alpha策略</span></a>
                </span>
            </span>

          
            <span id="/archives/740c8dcf.html" class="post-meta-item leancloud_visitors" data-flag-title="Alpha策略" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/740c8dcf.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/740c8dcf.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>0</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          

      
    </div>

    
    
    
      

      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
    

            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc">
                文章目录
              </li>
              <li class="sidebar-nav-overview">
                站点概览
              </li>
            </ul>
    



      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiyang"
      src="/img/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiyang</p>
  <div class="site-description" itemprop="description">世界上有两样东西不可直视，一是太阳，二是人心</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wjykl22" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wjykl22" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wjykl22@gmail.com" title="E-Mail → mailto:wjykl22@gmail.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



    <div class="links-of-blogroll motion-element links-of-blogroll-block">
      <div class="links-of-blogroll-title">
        <!-- modify icon to fire by szw -->
        <i class="fa fa-history fa-" aria-hidden="true"></i>
        近期文章
      </div>
      <ul class="links-of-blogroll-list">
        
        
          <li>
            <a href="/archives/5c29dc6b.html" title="AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators" target="_blank">AccPar: Tensor Partitioning for Heterogeneous Deep Learning Accelerators</a>
          </li>
        
          <li>
            <a href="/archives/740c8dcf.html" title="Alpha策略" target="_blank">Alpha策略</a>
          </li>
        
          <li>
            <a href="/archives/40cbe9ff.html" title="Device Placement Optimization with Reinforcement Learning" target="_blank">Device Placement Optimization with Reinforcement Learning</a>
          </li>
        
          <li>
            <a href="/archives/60fdd68b.html" title="Communication Optimal Parallel Multiplication of Sparse Random Matrices" target="_blank">Communication Optimal Parallel Multiplication of Sparse Random Matrices</a>
          </li>
        
          <li>
            <a href="/archives/62b3642.html" title="Beyond Data and Model Parallelism for Deep Neural Networks" target="_blank">Beyond Data and Model Parallelism for Deep Neural Networks</a>
          </li>
        
      </ul>
    </div>


      </div>
      <div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-eye"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiyang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">99k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:30</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'owd5pfJvVjoBkwE0F3w7Oqc5-gzGzoHsz',
      appKey     : 'pQbK5JId2AmEfxG3oSiFXAFP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div class="moon-menu-item" onclick="back2bottom()">
      <svg aria-hidden="true" focusable="false" data-prefix="fa" data-icon="chevron-down" class="svg-inline--fa fa-chevron-down fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"></path></svg>    </div>
    
    <div class="moon-menu-item" onclick="back2top()">
      <svg aria-hidden="true" focusable="false" data-prefix="fa" data-icon="chevron-up" class="svg-inline--fa fa-chevron-up fa-w-14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M240.971 130.524l194.343 194.343c9.373 9.373 9.373 24.569 0 33.941l-22.667 22.667c-9.357 9.357-24.522 9.375-33.901.04L224 227.495 69.255 381.516c-9.379 9.335-24.544 9.317-33.901-.04l-22.667-22.667c-9.373-9.373-9.373-24.569 0-33.941L207.03 130.525c9.372-9.373 24.568-9.373 33.941-.001z"></path></svg>    </div>
    
  </div>
  <div class="moon-menu-button" onclick="moonMenuClick()">
    <svg class="moon-menu-svg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
      <g class="moon-menu-points">
        <circle class="moon-menu-point" r=".2rem" cx="0" cy="-.8rem"></circle>
        <circle class="moon-menu-point" r=".2rem"></circle>
        <circle class="moon-menu-point" r=".2rem" cx="0" cy=".8rem"></circle>
      </g>
    </svg>
    <div class="moon-menu-icon">
    </div>
    <div class="moon-menu-text">
    </div>
  </div>
</div>
<script src="/js/injector.js"></script>
</body>
</html>
