<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=EB Garamond:300,300italic,400,400italic,700,700italic|Cinzel Decorative:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wjykl22.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="摘要 当下流行的深度神经网络具有非常复杂的结构，训练时需要消耗大量的内存和电源；在移动端和边缘设备等资源限制的情况下难以发挥作用，而量化则是解决上述问题的办法之一。原来的神经网络权重、激活和梯度都需要采用\(32bit\)精度的浮点表示，但是采用量化表示只需要整型或者二进制即可，大大减少了模型尺寸和资源消耗。这是一篇综述，从不同方面给出了量化神经网络的一些方法，同时也罗列了目前在这些方面遇到的挑">
<meta property="og:type" content="article">
<meta property="og:title" content="A Survey on Methods and Theories of Quantized Neural Networks">
<meta property="og:url" content="https://wjykl22.github.io/archives/2932556.html">
<meta property="og:site_name" content="韭零后">
<meta property="og:description" content="摘要 当下流行的深度神经网络具有非常复杂的结构，训练时需要消耗大量的内存和电源；在移动端和边缘设备等资源限制的情况下难以发挥作用，而量化则是解决上述问题的办法之一。原来的神经网络权重、激活和梯度都需要采用\(32bit\)精度的浮点表示，但是采用量化表示只需要整型或者二进制即可，大大减少了模型尺寸和资源消耗。这是一篇综述，从不同方面给出了量化神经网络的一些方法，同时也罗列了目前在这些方面遇到的挑">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wjykl22.github.io/archives/2932556/image-20200729203114640.png">
<meta property="article:published_time" content="2020-07-29T06:30:28.000Z">
<meta property="article:modified_time" content="2020-07-31T13:08:17.954Z">
<meta property="article:author" content="Jiyang">
<meta property="article:tag" content="分布式机器学习">
<meta property="article:tag" content="通信优化">
<meta property="article:tag" content="梯度压缩">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wjykl22.github.io/archives/2932556/image-20200729203114640.png">

<link rel="canonical" href="https://wjykl22.github.io/archives/2932556.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>A Survey on Methods and Theories of Quantized Neural Networks | 韭零后</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="韭零后" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">韭零后</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">公元1996 - ?</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-主页">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>主页</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-目录">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>目录</a>

  </li>
        <li class="menu-item menu-item-结构">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>结构</a>

  </li>
        <li class="menu-item menu-item-站点地图">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-公益">

    <a href="/404.html" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wjykl22.github.io/archives/2932556.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Jiyang">
      <meta itemprop="description" content="世界上有两样东西不可直视，一是太阳，二是人心">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="韭零后">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          A Survey on Methods and Theories of Quantized Neural Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-07-29 14:30:28" itemprop="dateCreated datePublished" datetime="2020-07-29T14:30:28+08:00">2020-07-29</time>
            </span>
            
                <i class="fa fa-thumb-tack"></i>
                <font color=7D26CD>置顶</font>
                <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-07-31 21:08:17" itemprop="dateModified" datetime="2020-07-31T21:08:17+08:00">2020-07-31</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">科研</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">分布式机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" itemprop="url" rel="index"><span itemprop="name">通信优化</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A7%91%E7%A0%94/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" itemprop="url" rel="index"><span itemprop="name">梯度压缩</span></a>
                </span>
            </span>

          
            <span id="/archives/2932556.html" class="post-meta-item leancloud_visitors" data-flag-title="A Survey on Methods and Theories of Quantized Neural Networks" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/archives/2932556.html#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/archives/2932556.html" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="摘要">摘要</h2>
<p>当下流行的深度神经网络具有非常复杂的结构，训练时需要消耗大量的内存和电源；在移动端和边缘设备等资源限制的情况下难以发挥作用，而量化则是解决上述问题的办法之一。原来的神经网络权重、激活和梯度都需要采用<span class="math inline">\(32bit\)</span>精度的浮点表示，但是采用量化表示只需要整型或者二进制即可，大大减少了模型尺寸和资源消耗。这是一篇综述，从不同方面给出了量化神经网络的一些方法，同时也罗列了目前在这些方面遇到的挑战。</p>
<h2 id="介绍">介绍</h2>
<h3 id="神经网络">神经网络</h3>
<p>本文介绍了下面几种神经网络，比较基础，这里就不做赘述</p>
<h4 id="前馈神经网络">前馈神经网络</h4>
<h4 id="卷积神经网络">卷积神经网络</h4>
<p>值得一提的是以下这些卷积神经网络结构：</p>
<ul>
<li>AlexNet[Krizhevsky et al., 2012<a href="#refer-anchor-1"><sup>1</sup></a>]</li>
<li>VGGNet[Simonyan and Zisserman, 2014<a href="#refer-anchor-2"><sup>2</sup></a>]</li>
<li>GoogleNet[Szegedy et al., 2015<a href="#refer-anchor-3"><sup>3</sup></a>]</li>
<li>ResNet[He et al., 2016a<a href="#refer-anchor-4"><sup>4</sup></a>]</li>
</ul>
<p>这四个架构非常广泛地用在比较不同压缩和量化方法性能比较实验过程中，常常作为基准（baseline）。</p>
<h4 id="循环神经网络和lstm">循环神经网络和LSTM</h4>
<h3 id="量化神经网络">量化神经网络</h3>
<h4 id="术语介绍">术语介绍</h4>
<ul>
<li><strong>低精度</strong>（Low precision）：可能是最通用的概念。常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；低精度则表示 FP16（半精度浮点），INT8（8位的定点整数）等等数值格式。不过目前低精度往往指代 INT8。</li>
<li><strong>混合精度</strong>（Mixed precision）在模型中使用 FP32 和 FP16 。 FP16 减少了一半的内存大小，但有些参数或操作符必须采用 FP32 格式才能保持准确度。如果您对该主题感兴趣，请查看 <a href="https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/" rel="external nofollow noreferrer">Mixed-Precision Training of Deep Neural Networks</a> 。</li>
<li><strong>量化</strong>一般指 INT8 。不过，根据存储一个权重元素所需的位数，还可以包括：
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.02830" rel="external nofollow noreferrer">二值神经网络</a>：在运行时权重和激活只取两种值（例如 +1，-1）的神经网络，以及在训练时计算参数的梯度。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.04711" rel="external nofollow noreferrer">三元权重网络</a>：权重约束为+1,0和-1的神经网络。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05279" rel="external nofollow noreferrer">XNOR网络</a>：过滤器和卷积层的输入是二进制的。 XNOR 网络主要使用二进制运算来近似卷积。</li>
</ul></li>
</ul>
<h4 id="正文">正文</h4>
<p>目前，很多技术用在了量化神经网络方面。粗略地来看可以分成确定性量化和随机量化。在确定量化中，在量化值和真实值之间有一一对应的映射，而随机量化权重，激活和梯度则是离散分布。量化值是从离散分布中采样得到的。</p>
<p>在神经网络中有三个部分是可以进行量化的：权重、激活和梯度。量化这些部分的动机和方法是不同的。量化权重和激活层，我们得到更小的模型尺寸。在分布式训练的花镜中，我们能够通过量化梯度的方式节省通信消耗。一般来说，量化梯度比量化权重和激活更加困难，因为训练往往需要精度更高的梯度来保证算法的收敛。</p>
<p>我们通常采用编码本（codebook）来表示代表真实值的离散值。从密码本的表示来看，现有的工作可以将量化神经网络粗略的分成两类：固定编码本量化和自适应编码本量化。</p>
<p>在固定编码本量化中，权重经常被量化成提前定义好的编码，二自适应编码本是从数据中学习而来。一些普遍应用的密码本包括<span class="math inline">\(\{-1,1\}\)</span>，<span class="math inline">\(\{-1,0,1\}\)</span>或者二数幂或者二进制网络和三元权重网络等。</p>
<p>训练量化模型需要不断调整，而且量化网络并不容易理解，寻找新的量化方法以及配合理论分析是量化神经网络非常重要的一点。</p>
<h2 id="量化技术">量化技术</h2>
<h3 id="确定性量化">确定性量化</h3>
<h4 id="取整rounding">取整（Rounding）</h4>
<h5 id="主要内容">主要内容</h5>
<p>取证可能是对真实值最简单的量化，例如[Courbariaux et al., 2015]提出下面这种取整方法： <span class="math display">\[
x^{b}=\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1 &amp; x \geq 0 \\
-1 &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中<span class="math inline">\(x^b\)</span>表示二进制量，<span class="math inline">\(x\)</span>是真实量。这个方法可以应用在量化权重，激活和梯度中。在前向传播中，真实值权重能够产生输出。然而，在反向传播过程中，我们不能够通过<code>Sign(x)</code>来进行，因为它是离散的，到处都是梯度为零。通常采用的方法是“直通估计（straight through estimator）”（STE）[Hinton et al., 2012b]，它采用启发式的方法估计随机神经元的梯度。假设<span class="math inline">\(E\)</span>是损失函数，STE的前向和反向计算可以看成如下方式： <span class="math display">\[
\begin{array}{l}
\text { Forward: } \quad x^{b}=\operatorname{Sign}(x) \\
\text { Backward: } \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}} \mathrm{I}_{|x| \leq 1}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{I}_{|x| \leq 1}\)</span>是定义如下的指示函数： <span class="math display">\[
\mathrm{I}_{|x| \leq 1}=\left\{\begin{array}{ll}
1 &amp; |x| \leq 1 \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 为了对双精度进行取整，[Gupta et al., 2015]作者提出了如下的取整方式： <span class="math display">\[
\operatorname{Round}(x,[\mathrm{IL}, \mathrm{FL}])=\left\{\begin{array}{ll}
\lfloor x\rfloor &amp; \text { if }\lfloor x\rfloor \leq x \leq\lfloor x\rfloor+\frac{\epsilon}{2} \\
\lfloor x\rfloor+\epsilon &amp; \text { if }\lfloor x\rfloor+\frac{\epsilon}{2}&lt;x \leq\lfloor x\rfloor+\epsilon
\end{array}\right.
\]</span> 在固定点表达中，IL代表整数位的个数，FL表示分数位的个数。<span class="math inline">\(\epsilon\)</span>表示在固定点表达中能够表达的最小正数。<span class="math inline">\(\lfloor x\rfloor\)</span>被定义为<span class="math inline">\(\epsilon\)</span>的最大整数倍。对于超出此固定点格式范围的值，作者将它们规范化为固定点表示的下界或上界[Rastegari et al., 2016]。将上式扩展： <span class="math display">\[
\begin{array}{ll}
\text { Forward: } &amp; x^{b}=\operatorname{Sign}(x) \times \mathrm{E}_{F}(|x|) \\
\text { Backward: } &amp; \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{E}_{F}(|x|)\)</span>表示每个输出通道的权值绝对值的平均值。</p>
<p>近期[Polino et al., 2018]提出了更加普遍的舍入函数： <span class="math display">\[
Q(x)=s c^{-1}(\hat{Q}(s c(x)))
\]</span> 其中<span class="math inline">\(sc(x)\)</span>是将值从任意范围缩放到<span class="math inline">\([0,1]\)</span>的缩放函数。<span class="math inline">\(\hat{Q}(x)\)</span>是实际的量化函数。给出量化等级参数<span class="math inline">\(s\)</span>，有<span class="math inline">\(s+1\)</span>等级的统一量化函数可以定义为： <span class="math display">\[
\hat{Q}(x, s)=\frac{\lfloor x s\rfloor}{s}+\frac{\xi}{s}
\]</span> 其中 <span class="math display">\[
\xi=\left\{\begin{array}{ll}
1 &amp; x s-\lfloor x s\rfloor&gt;\frac{1}{2} \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 这个量化函数的直觉是将<span class="math inline">\(x\)</span>分配到在<span class="math inline">\([0,1]\)</span>范围内<span class="math inline">\(s-1\)</span>个等间隔最接近的量化点。这是符号<span class="math inline">\(Sign(x)\)</span>函数的广义版本，能够将实值量化成多层。在[Shuang et al., 2018]中，作者提出了启发式摄入函数来量化一个市值为<span class="math inline">\(k\)</span>位的整数。 <span class="math display">\[
Q(x, k)=\operatorname{Clip}\left\{\sigma(k) \cdot \operatorname{round}\left[\frac{x}{\sigma(k)}\right],-1+\sigma(k), 1-\sigma(k)\right\}
\]</span> 想法是将真实值利用统一的距离<span class="math inline">\(\sigma(k)\)</span>进行量化，其中<span class="math inline">\(\sigma(k)=2^{1-k}\)</span>。<span class="math inline">\(Clip\)</span>将量化限制在<span class="math inline">\([-1+\sigma(k), 1-\sigma(k)]\)</span>范围内，<span class="math inline">\(round\)</span>用最近的离散点替换连续值。</p>
<h5 id="挑战">挑战</h5>
<p>挑战:使用四舍五入函数是将实值转换为量化值的简单方法。然而，每次四舍五入操作之后，网络性能可能会急剧下降。在训练过程中需要保持真实值作为参考，这会增加记忆开销。同时，由于使用离散值时参数空间要小得多，训练过程难以收敛。最后，舍入运算不能充分利用网络中权值的结构信息。</p>
<h4 id="向量量化">向量量化</h4>
<p>[Gong et al., 2014]是第一篇将向量量化考虑到神经网络压缩和量化中的。他主要的思想是将权重分组聚类，在推理时采用聚类中心代表每个组实际的权重。</p>
<p>[Han et al., 2015]，[Gong et al., 2014]等都对这种方式进行了改进，[Choi et al., 2016]指出这种方法有两个缺点，第一是不能够控制由于<code>k-means</code>算法造成的损失；第二是<code>k-means</code>算法不施加任何压缩比约束。为了解决这些问题，作者提出了一种Hessian加权k均值聚类方法。其基本思想是使用Hessian Weighted失真来测量因权值量化而导致的性能退化。这样可以防止那些对网络性能有较大影响的权值与原始值偏离太多。</p>
<p>有很多对向量量化的扩展方法，乘积量化[Gong et al., 2014]是一种将权重矩阵划分为许多不相交的子矩阵，并对每个子矩阵进行量化的方法。在[Wu et al., 2016]中，作者采用带误差修正的产品量化方法对网络参数进行量化，实现快速训练和测试。残差量化[Gong et al., 2014]将向量量化到k个聚类中，然后递归量化残差。在[Park et al., 2017]中，作者采用了类似于矢量量化的方法。他们使用了一个基于权重熵的想法[Guias¸u, 1971]来将权重分组到N个簇中。对于重要的权重范围有更多的簇。从而实现了自动灵活的多比特量化。</p>
<h5 id="挑战-1">挑战</h5>
<p>由于网络中权值的数量，k-means聚类的计算量很大。与四舍五入法相比，用向量化方法来实现二值权值比较困难。向量量化通常用于对预先训练的模型进行量化。因此，如果任务是从头训练量化网络，最好使用精心设计的四舍五入函数。向量量化忽略了网络的局部信息。</p>
<h4 id="量化最优化">量化最优化</h4>
<p>简单来说就是将量化作为最优化问题进行，此处暂时省略...</p>
<h3 id="随机量化">随机量化</h3>
<h4 id="随机舍入法random-rounding">随机舍入法（Random Rounding）</h4>
<p>在随机舍入法中，真实值和量化值有着一对一的对应。典型地，量化值的权重是从离散分布中采样而来，它是通过真实值进行参数化的。例如，[Courbariaux et al., 2015]提出了以下的随机近似方法： <span class="math display">\[
x^{b}=\left\{\begin{array}{ll}+1 &amp; \text { with probability } p=\sigma(x) \\ -1 &amp; \text { with probability } 1-p\end{array}\right.
\]</span> 其中<span class="math inline">\(\sigma\)</span>表示“hard sigmoid”函数 <span class="math display">\[
\sigma(x)=\operatorname{clip}\left(\frac{x+1}{2}, 0,1\right)=\max \left(0, \min \left(1, \frac{x+1}{2}\right)\right)
\]</span> 直观上来说，<span class="math inline">\(x\)</span>是一个正值，我们将以很高的概率量化到<span class="math inline">\(+1\)</span>，其他情况量化到<span class="math inline">\(-1\)</span>。这就给我们更加灵活的量化模式。在[Muller and Indi-veri, 2015]中，作者在整数规划中使用了这种思想。提出的随机四舍五入函数将每个实值概率映射到最近的离散点或第二最近的离散点，这取决于到对应点的距离。在[Lin et al., 2015]中，将二进随机四舍五入扩展到三元情形。</p>
<h5 id="挑战-2">挑战</h5>
<p>随机舍入提供了一种将噪声注入训练过程的方法。它可以作为一个正则化器和可提供条件计算。然而，使用随机的舍入方法，我们需要估计离散神经元的梯度。这样的估计往往有很高的方差。这一事实可能会导致训练过程中损失函数的振荡。[Bengio等人，2013]的工作提供了估计离散中性电子梯度的可能解决方案的概述。</p>
<h4 id="概率量化">概率量化</h4>
<p>暂无</p>
<h4 id="讨论">讨论</h4>
<p>上述量化技术使我们能够从不同的角度对网络进行量化。这些技术的优缺点可以指导我们在不同的情况下选择合适的技术。一般来说，如果我们想为硬件加速量化神经网络，确定性量化应该是首选，因为我们可以预先指定适当的量化级别，以便在专用硬件上运行量化的网络工作。这可以提高硬件的预测性能。四舍五入使我们能够以数据依赖的方式量化权重。这导致了条件计算[Bengio et al.， 2013]，可以增加神经网络的容量。概率量化与确定性量化的不同之处在于量化后的权重更具有可解释性。我们可以用概率量化的方法来理解权值的分布，并对网络的工作原理有更深入的了解。在概率量化的情况下，由于贝叶斯方法的正则化效果，我们也可以得到更稀疏的模型。</p>
<h2 id="量化对象分类">量化对象分类</h2>
<h3 id="权重量化">权重量化</h3>
<p>在[Zhou et al., 2017a]中，作者提出了增量式网络量化(INQ)，它包括三个步骤：权重划分、分组量化和再训练。他们以组的方式对权重进行组化，以允许某些权重组补偿由于其他组的量化而造成的准确性损失。[Gudovskiy and Rigazio, 2017]的工作将这种方法扩展到2次幂设置。</p>
<p>在[Lin et al., 2016]中，作者试图找到最优的不动点位宽跨层分配。他们研究了通过量化不同的层可以引入多少噪音。[Lin et al., 2017]使用多个二进制基的线性组合近似全精度权重。结果表明，二值神经网络在ImageNet数据集上首次取得了与全精度神经网络相当的预测精度。在[Moons et al., 2017]中，作者研究了如何发展节能量化神经网络。在[Guo et al., 2017]的工作中引入了网络素描来量化预先训练好的模型。其思想是使用二进制基来近似预先训练过的滤波器。他们首先提出了一种启发式算法来寻找二进制基础，然后提供了一个改进版本，以更好的近似。在[Mohamed Amer, 2018]中，作者提出了一种端到端训练框架来同时优化原始损失函数、量化误差和总比特数。然而，其精度无法与其他量化神经网络相比。</p>
<h3 id="梯度量化">梯度量化</h3>
<p>梯度量化的应用场景一般是为了在分布式训练过程中减小通信消耗。如下图就是为神经网络的并行训练：</p>
<p><img src="/archives/2932556/image-20200729203114640.png" alt="image-20200729203114640" style="zoom: 80%;"></p>
<p>[Seide et al., 2014<a href="#refer-anchor-5"><sup>5</sup></a>]提出了一种1-bit表示各个节点计算梯度。和常规的方法相比，它得到了10倍的加速比。[Strom, 2015<a href="#refer-anchor-6"><sup>6</sup></a>]作者提出了一种阈值量化方法。提前选定一个阈值，如果梯度大于这个阈值就量化为<span class="math inline">\(+1\)</span>，如果小于这个阈值就量化为<span class="math inline">\(0\)</span>。[Alistarh et al., 2016<a href="#refer-anchor-7"><sup>7</sup></a>]提出了QSGD方法，允许每个节点在精度、梯度和模型的精度之间进行权衡。QSGD利用随机四舍五入的思想将梯度量化为一组离散值，并利用无损编码产生高效的编码。[Dryden et al., 2016<a href="#refer-anchor-8"><sup>8</sup></a>]]作者提出一种简单的自适应量化方法来选择合适的梯度进行量化并发送。</p>
<p>[Wen et al., 2017<a href="#refer-anchor-9"><sup>9</sup></a>]]通过提出了TernGrad解决了并行过程中水平扩展的问题。他将梯度量化为<span class="math inline">\(\{-1,0,1\}\)</span>。在发送给中心参数服务器之前，每个梯度都将被如下量化： <span class="math display">\[
\tilde{\Delta}_{t}=\operatorname{ternarize}\left(\Delta_{t}\right)=s_{t} \cdot \operatorname{sign}\left(\Delta_{t}\right) \circ b_{t}
\]</span> 其中<span class="math inline">\(s_{t}=\max \left(\operatorname{abs}\left(\Delta_{t}\right)\right)\)</span>，其中<span class="math inline">\(\circ\)</span>是<a href="https://www.baidu.com/link?url=w-LVD0IIl4PbHY-Vzc-UEKgvX7_8m_uRUWXH56DVxRjY77fZbsptVsF2pO7Gmxx3cRBReXg7sz3JTvIakfzCprx8m2RSMUNzT_E2Ppx3wyfA6RMWuAA7E66zF4btnsYK&amp;wd=&amp;eqid=8509435200010e68000000055f216602" target="_blank" rel="noopener external nofollow noreferrer">哈达玛积</a>，<span class="math inline">\(b_t\)</span>是遵循如下伯努利分布的随机二进制向量： <span class="math display">\[
\left\{\begin{array}{l}
P\left(b_{t k}=1 \mid \Delta_{t}\right)=\left|\Delta_{t k}\right| / s_{t} \\
P\left(b_{t k}=0 \mid \Delta_{t}\right)=1-\left|\Delta_{t k}\right| / s_{t}
\end{array}\right.
\]</span> 采用这种方法，服务器和工作节点之间的通信开销可以降低接近20倍.</p>
<p>在单一节点的环境中，我们也能够通过量化梯度获得益处。为了减小反向传播的计算开销，[Rastegari et al., 2016<a href="#refer-anchor-11"><sup>11</sup></a>]将梯度量化为2-bits来进行高性能通信。[Zhou et al., 2016<a href="#refer-anchor-10"><sup>10</sup></a>]他还量化了反向传播过程中的梯度。他们发现使用随机四舍五入的方法是非常重要的，使量程梯度工作得很好。他们设计了如下的k比特量化函数， <span class="math display">\[
\tilde{f}_{\gamma}^{k}(d r)=2 \max _{0}(|d r|)\left[\text { quantize }_{k}\left(\frac{d r}{2 \max _{0}(|d r|)+\frac{1}{2}}\right)-\frac{1}{2}\right]
\]</span> 其中<span class="math inline">\(d r=\frac{\partial c}{\partial r}\)</span>是在某些层输出<span class="math inline">\(r\)</span>的梯度，<span class="math inline">\(quantize_k\)</span>被用于量化一个实数输入<span class="math inline">\(r_{i} \in[0,1]\)</span>到k-bit输出值<span class="math inline">\(r_{0} \in[0,1]\)</span>， <span class="math display">\[
r_{o}=\frac{1}{2^{k}-1} \operatorname{round}\left(\left(2^{k}-1\right) r_{i}\right)
\]</span> 他们还在训练过程中加入额外的噪声，以弥补量化造成的准确性损失。</p>
<h5 id="挑战-3">挑战</h5>
<ul>
<li><p>梯度的大小和符号对于更新权重都很重要。为了量化梯度，我们必须解决如何将这两个因素纳入计算的问题。</p></li>
<li><p>一种简单的量化梯度的方法可能在实践中并不奏效，因为它可能违反随机梯度下降算法的收敛条件。在这种情况下需要更复杂的方法。</p></li>
</ul>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener external nofollow noreferrer">Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener external nofollow noreferrer">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="noopener external nofollow noreferrer">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" target="_blank" rel="noopener external nofollow noreferrer">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-7">

</div></li>
<li>[7] <a href="https://www.isca-speech.org/archive/interspeech_2015/i15_1488.html" target="_blank" rel="noopener external nofollow noreferrer">Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. 2015.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="https://ieeexplore.ieee.org/abstract/document/7835789/" target="_blank" rel="noopener external nofollow noreferrer">Dryden N, Moon T, Jacobs S A, et al. Communication quantization for data-parallel training of deep neural networks[C]//2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC). IEEE, 2016: 1-8.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/pdf/1606.06160" target="_blank" rel="noopener external nofollow noreferrer">Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients[J]. arXiv preprint arXiv:1606.06160, 2016.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="https://link.springer.com/chapter/10.1007/978-3-319-46493-0_32" target="_blank" rel="noopener external nofollow noreferrer">Rastegari M, Ordonez V, Redmon J, et al. Xnor-net: Imagenet classification using binary convolutional neural networks[C]//European conference on computer vision. Springer, Cham, 2016: 525-542.</a></li>
</ul>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章推荐</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\archives\88211b35.html" rel="bookmark">分布式机器学习量化通信综述</a></div>
        <div class="popular-posts-excerpt"><p>此文加密，请输入密码</p></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\archives\fcc7e450.html" rel="bookmark">Error Compensated Quantized SGD and its Applications to Large-scale</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\archives\d06ef2e3.html" rel="bookmark">QSGD Communication-Efficient SGD via Gradient Quantization and Encoding</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\archives\493c0bc7.html" rel="bookmark">DOUBLESQUEEZE Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\archives\5cf57987.html" rel="bookmark">分布式学习通信优化综述（2020香港大学）</a></div>
    </li>
  </ul>

        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/img/wechatpay.jpg" alt="Jiyang 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/img/alipay.jpg" alt="Jiyang 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Jiyang
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://wjykl22.github.io/archives/2932556.html" title="A Survey on Methods and Theories of Quantized Neural Networks">https://wjykl22.github.io/archives/2932556.html</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 分布式机器学习</a>
              <a href="/tags/%E9%80%9A%E4%BF%A1%E4%BC%98%E5%8C%96/" rel="tag"># 通信优化</a>
              <a href="/tags/%E6%A2%AF%E5%BA%A6%E5%8E%8B%E7%BC%A9/" rel="tag"># 梯度压缩</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/archives/fcc7e450.html" rel="prev" title="Error Compensated Quantized SGD and its Applications to Large-scale">
      <i class="fa fa-chevron-left"></i> Error Compensated Quantized SGD and its Applications to Large-scale
    </a></div>
      <div class="post-nav-item">
    <a href="/archives/1b1e994e.html" rel="next" title="1 Bits SGD and its Application to Data Parallel Distributed Training of Speach DNNs">
      1 Bits SGD and its Application to Data Parallel Distributed Training of Speach DNNs <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
    

            <ul class="sidebar-nav motion-element">
              <li class="sidebar-nav-toc">
                文章目录
              </li>
              <li class="sidebar-nav-overview">
                站点概览
              </li>
            </ul>
    



      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-text">介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络"><span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#前馈神经网络"><span class="nav-text">前馈神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#卷积神经网络"><span class="nav-text">卷积神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#循环神经网络和lstm"><span class="nav-text">循环神经网络和LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#量化神经网络"><span class="nav-text">量化神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#术语介绍"><span class="nav-text">术语介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#正文"><span class="nav-text">正文</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#量化技术"><span class="nav-text">量化技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#确定性量化"><span class="nav-text">确定性量化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#取整rounding"><span class="nav-text">取整（Rounding）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#主要内容"><span class="nav-text">主要内容</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#挑战"><span class="nav-text">挑战</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#向量量化"><span class="nav-text">向量量化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#挑战-1"><span class="nav-text">挑战</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#量化最优化"><span class="nav-text">量化最优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机量化"><span class="nav-text">随机量化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#随机舍入法random-rounding"><span class="nav-text">随机舍入法（Random Rounding）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#挑战-2"><span class="nav-text">挑战</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#概率量化"><span class="nav-text">概率量化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#讨论"><span class="nav-text">讨论</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#量化对象分类"><span class="nav-text">量化对象分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#权重量化"><span class="nav-text">权重量化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度量化"><span class="nav-text">梯度量化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#挑战-3"><span class="nav-text">挑战</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiyang"
      src="/img/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiyang</p>
  <div class="site-description" itemprop="description">世界上有两样东西不可直视，一是太阳，二是人心</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wjykl22" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wjykl22" rel="noopener external nofollow noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wjykl22@gmail.com" title="E-Mail → mailto:wjykl22@gmail.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



    <div class="links-of-blogroll motion-element links-of-blogroll-block">
      <div class="links-of-blogroll-title">
        <!-- modify icon to fire by szw -->
        <i class="fa fa-history fa-" aria-hidden="true"></i>
        近期文章
      </div>
      <ul class="links-of-blogroll-list">
        
        
          <li>
            <a href="/archives/6fa57430.html" title="2020.08.26小组汇报" target="_blank">2020.08.26小组汇报</a>
          </li>
        
          <li>
            <a href="/archives/5cf57987.html" title="分布式学习通信优化综述（2020香港大学）" target="_blank">分布式学习通信优化综述（2020香港大学）</a>
          </li>
        
          <li>
            <a href="/archives/d05bdbcf.html" title="Ray参数服务器性能测评实验" target="_blank">Ray参数服务器性能测评实验</a>
          </li>
        
          <li>
            <a href="/archives/71d33514.html" title="Konga前端修改设计调研" target="_blank">Konga前端修改设计调研</a>
          </li>
        
          <li>
            <a href="/archives/d34fba27.html" title="LAQ和LAG代码研读" target="_blank">LAQ和LAG代码研读</a>
          </li>
        
      </ul>
    </div>


      </div>
      <div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-eye"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiyang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">248k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">3:45</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'owd5pfJvVjoBkwE0F3w7Oqc5-gzGzoHsz',
      appKey     : 'pQbK5JId2AmEfxG3oSiFXAFP',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
