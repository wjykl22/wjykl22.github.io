<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>分布式机器学习量化通信综述</title>
    <url>/archives/88211b35.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">请输入密码</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="5098132751387beeb702d7f8cbedaf254f7d2d3547cb07a2335ef39e880225e2">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>A Survey on Methods and Theories of Quantized Neural Networks</title>
    <url>/archives/2932556.html</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>当下流行的深度神经网络具有非常复杂的结构，训练时需要消耗大量的内存和电源；在移动端和边缘设备等资源限制的情况下难以发挥作用，而量化则是解决上述问题的办法之一。原来的神经网络权重、激活和梯度都需要采用<span class="math inline">\(32bit\)</span>精度的浮点表示，但是采用量化表示只需要整型或者二进制即可，大大减少了模型尺寸和资源消耗。这是一篇综述，从不同方面给出了量化神经网络的一些方法，同时也罗列了目前在这些方面遇到的挑战。</p>
<h2 id="介绍">介绍</h2>
<h3 id="神经网络">神经网络</h3>
<p>本文介绍了下面几种神经网络，比较基础，这里就不做赘述</p>
<h4 id="前馈神经网络">前馈神经网络</h4>
<h4 id="卷积神经网络">卷积神经网络</h4>
<p>值得一提的是以下这些卷积神经网络结构：</p>
<ul>
<li>AlexNet[Krizhevsky et al., 2012<a href="#refer-anchor-1"><sup>1</sup></a>]</li>
<li>VGGNet[Simonyan and Zisserman, 2014<a href="#refer-anchor-2"><sup>2</sup></a>]</li>
<li>GoogleNet[Szegedy et al., 2015<a href="#refer-anchor-3"><sup>3</sup></a>]</li>
<li>ResNet[He et al., 2016a<a href="#refer-anchor-4"><sup>4</sup></a>]</li>
</ul>
<p>这四个架构非常广泛地用在比较不同压缩和量化方法性能比较实验过程中，常常作为基准（baseline）。</p>
<h4 id="循环神经网络和lstm">循环神经网络和LSTM</h4>
<h3 id="量化神经网络">量化神经网络</h3>
<h4 id="术语介绍">术语介绍</h4>
<ul>
<li><strong>低精度</strong>（Low precision）：可能是最通用的概念。常规精度一般使用 FP32（32位浮点，单精度）存储模型权重；低精度则表示 FP16（半精度浮点），INT8（8位的定点整数）等等数值格式。不过目前低精度往往指代 INT8。</li>
<li><strong>混合精度</strong>（Mixed precision）在模型中使用 FP32 和 FP16 。 FP16 减少了一半的内存大小，但有些参数或操作符必须采用 FP32 格式才能保持准确度。如果您对该主题感兴趣，请查看 <a href="https://link.zhihu.com/?target=https%3A//devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/" rel="external nofollow noreferrer">Mixed-Precision Training of Deep Neural Networks</a> 。</li>
<li><strong>量化</strong>一般指 INT8 。不过，根据存储一个权重元素所需的位数，还可以包括：
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1602.02830" rel="external nofollow noreferrer">二值神经网络</a>：在运行时权重和激活只取两种值（例如 +1，-1）的神经网络，以及在训练时计算参数的梯度。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1605.04711" rel="external nofollow noreferrer">三元权重网络</a>：权重约束为+1,0和-1的神经网络。</li>
<li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1603.05279" rel="external nofollow noreferrer">XNOR网络</a>：过滤器和卷积层的输入是二进制的。 XNOR 网络主要使用二进制运算来近似卷积。</li>
</ul></li>
</ul>
<h4 id="正文">正文</h4>
<p>目前，很多技术用在了量化神经网络方面。粗略地来看可以分成确定性量化和随机量化。在确定量化中，在量化值和真实值之间有一一对应的映射，而随机量化权重，激活和梯度则是离散分布。量化值是从离散分布中采样得到的。</p>
<p>在神经网络中有三个部分是可以进行量化的：权重、激活和梯度。量化这些部分的动机和方法是不同的。量化权重和激活层，我们得到更小的模型尺寸。在分布式训练的花镜中，我们能够通过量化梯度的方式节省通信消耗。一般来说，量化梯度比量化权重和激活更加困难，因为训练往往需要精度更高的梯度来保证算法的收敛。</p>
<p>我们通常采用编码本（codebook）来表示代表真实值的离散值。从密码本的表示来看，现有的工作可以将量化神经网络粗略的分成两类：固定编码本量化和自适应编码本量化。</p>
<p>在固定编码本量化中，权重经常被量化成提前定义好的编码，二自适应编码本是从数据中学习而来。一些普遍应用的密码本包括<span class="math inline">\(\{-1,1\}\)</span>，<span class="math inline">\(\{-1,0,1\}\)</span>或者二数幂或者二进制网络和三元权重网络等。</p>
<p>训练量化模型需要不断调整，而且量化网络并不容易理解，寻找新的量化方法以及配合理论分析是量化神经网络非常重要的一点。</p>
<h2 id="量化技术">量化技术</h2>
<h3 id="确定性量化">确定性量化</h3>
<h4 id="取整rounding">取整（Rounding）</h4>
<h5 id="主要内容">主要内容</h5>
<p>取证可能是对真实值最简单的量化，例如[Courbariaux et al., 2015]提出下面这种取整方法： <span class="math display">\[
x^{b}=\operatorname{sign}(x)=\left\{\begin{array}{ll}
+1 &amp; x \geq 0 \\
-1 &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中<span class="math inline">\(x^b\)</span>表示二进制量，<span class="math inline">\(x\)</span>是真实量。这个方法可以应用在量化权重，激活和梯度中。在前向传播中，真实值权重能够产生输出。然而，在反向传播过程中，我们不能够通过<code>Sign(x)</code>来进行，因为它是离散的，到处都是梯度为零。通常采用的方法是“直通估计（straight through estimator）”（STE）[Hinton et al., 2012b]，它采用启发式的方法估计随机神经元的梯度。假设<span class="math inline">\(E\)</span>是损失函数，STE的前向和反向计算可以看成如下方式： <span class="math display">\[
\begin{array}{l}
\text { Forward: } \quad x^{b}=\operatorname{Sign}(x) \\
\text { Backward: } \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}} \mathrm{I}_{|x| \leq 1}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{I}_{|x| \leq 1}\)</span>是定义如下的指示函数： <span class="math display">\[
\mathrm{I}_{|x| \leq 1}=\left\{\begin{array}{ll}
1 &amp; |x| \leq 1 \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 为了对双精度进行取整，[Gupta et al., 2015]作者提出了如下的取整方式： <span class="math display">\[
\operatorname{Round}(x,[\mathrm{IL}, \mathrm{FL}])=\left\{\begin{array}{ll}
\lfloor x\rfloor &amp; \text { if }\lfloor x\rfloor \leq x \leq\lfloor x\rfloor+\frac{\epsilon}{2} \\
\lfloor x\rfloor+\epsilon &amp; \text { if }\lfloor x\rfloor+\frac{\epsilon}{2}&lt;x \leq\lfloor x\rfloor+\epsilon
\end{array}\right.
\]</span> 在固定点表达中，IL代表整数位的个数，FL表示分数位的个数。<span class="math inline">\(\epsilon\)</span>表示在固定点表达中能够表达的最小正数。<span class="math inline">\(\lfloor x\rfloor\)</span>被定义为<span class="math inline">\(\epsilon\)</span>的最大整数倍。对于超出此固定点格式范围的值，作者将它们规范化为固定点表示的下界或上界[Rastegari et al., 2016]。将上式扩展： <span class="math display">\[
\begin{array}{ll}
\text { Forward: } &amp; x^{b}=\operatorname{Sign}(x) \times \mathrm{E}_{F}(|x|) \\
\text { Backward: } &amp; \frac{\partial E}{\partial x}=\frac{\partial E}{\partial x^{b}}
\end{array}
\]</span> 其中<span class="math inline">\(\mathrm{E}_{F}(|x|)\)</span>表示每个输出通道的权值绝对值的平均值。</p>
<p>近期[Polino et al., 2018]提出了更加普遍的舍入函数： <span class="math display">\[
Q(x)=s c^{-1}(\hat{Q}(s c(x)))
\]</span> 其中<span class="math inline">\(sc(x)\)</span>是将值从任意范围缩放到<span class="math inline">\([0,1]\)</span>的缩放函数。<span class="math inline">\(\hat{Q}(x)\)</span>是实际的量化函数。给出量化等级参数<span class="math inline">\(s\)</span>，有<span class="math inline">\(s+1\)</span>等级的统一量化函数可以定义为： <span class="math display">\[
\hat{Q}(x, s)=\frac{\lfloor x s\rfloor}{s}+\frac{\xi}{s}
\]</span> 其中 <span class="math display">\[
\xi=\left\{\begin{array}{ll}
1 &amp; x s-\lfloor x s\rfloor&gt;\frac{1}{2} \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 这个量化函数的直觉是将<span class="math inline">\(x\)</span>分配到在<span class="math inline">\([0,1]\)</span>范围内<span class="math inline">\(s-1\)</span>个等间隔最接近的量化点。这是符号<span class="math inline">\(Sign(x)\)</span>函数的广义版本，能够将实值量化成多层。在[Shuang et al., 2018]中，作者提出了启发式摄入函数来量化一个市值为<span class="math inline">\(k\)</span>位的整数。 <span class="math display">\[
Q(x, k)=\operatorname{Clip}\left\{\sigma(k) \cdot \operatorname{round}\left[\frac{x}{\sigma(k)}\right],-1+\sigma(k), 1-\sigma(k)\right\}
\]</span> 想法是将真实值利用统一的距离<span class="math inline">\(\sigma(k)\)</span>进行量化，其中<span class="math inline">\(\sigma(k)=2^{1-k}\)</span>。<span class="math inline">\(Clip\)</span>将量化限制在<span class="math inline">\([-1+\sigma(k), 1-\sigma(k)]\)</span>范围内，<span class="math inline">\(round\)</span>用最近的离散点替换连续值。</p>
<h5 id="挑战">挑战</h5>
<p>挑战:使用四舍五入函数是将实值转换为量化值的简单方法。然而，每次四舍五入操作之后，网络性能可能会急剧下降。在训练过程中需要保持真实值作为参考，这会增加记忆开销。同时，由于使用离散值时参数空间要小得多，训练过程难以收敛。最后，舍入运算不能充分利用网络中权值的结构信息。</p>
<h4 id="向量量化">向量量化</h4>
<p>[Gong et al., 2014]是第一篇将向量量化考虑到神经网络压缩和量化中的。他主要的思想是将权重分组聚类，在推理时采用聚类中心代表每个组实际的权重。</p>
<p>[Han et al., 2015]，[Gong et al., 2014]等都对这种方式进行了改进，[Choi et al., 2016]指出这种方法有两个缺点，第一是不能够控制由于<code>k-means</code>算法造成的损失；第二是<code>k-means</code>算法不施加任何压缩比约束。为了解决这些问题，作者提出了一种Hessian加权k均值聚类方法。其基本思想是使用Hessian Weighted失真来测量因权值量化而导致的性能退化。这样可以防止那些对网络性能有较大影响的权值与原始值偏离太多。</p>
<p>有很多对向量量化的扩展方法，乘积量化[Gong et al., 2014]是一种将权重矩阵划分为许多不相交的子矩阵，并对每个子矩阵进行量化的方法。在[Wu et al., 2016]中，作者采用带误差修正的产品量化方法对网络参数进行量化，实现快速训练和测试。残差量化[Gong et al., 2014]将向量量化到k个聚类中，然后递归量化残差。在[Park et al., 2017]中，作者采用了类似于矢量量化的方法。他们使用了一个基于权重熵的想法[Guias¸u, 1971]来将权重分组到N个簇中。对于重要的权重范围有更多的簇。从而实现了自动灵活的多比特量化。</p>
<h5 id="挑战-1">挑战</h5>
<p>由于网络中权值的数量，k-means聚类的计算量很大。与四舍五入法相比，用向量化方法来实现二值权值比较困难。向量量化通常用于对预先训练的模型进行量化。因此，如果任务是从头训练量化网络，最好使用精心设计的四舍五入函数。向量量化忽略了网络的局部信息。</p>
<h4 id="量化最优化">量化最优化</h4>
<p>简单来说就是将量化作为最优化问题进行，此处暂时省略...</p>
<h3 id="随机量化">随机量化</h3>
<h4 id="随机舍入法random-rounding">随机舍入法（Random Rounding）</h4>
<p>在随机舍入法中，真实值和量化值有着一对一的对应。典型地，量化值的权重是从离散分布中采样而来，它是通过真实值进行参数化的。例如，[Courbariaux et al., 2015]提出了以下的随机近似方法： <span class="math display">\[
x^{b}=\left\{\begin{array}{ll}+1 &amp; \text { with probability } p=\sigma(x) \\ -1 &amp; \text { with probability } 1-p\end{array}\right.
\]</span> 其中<span class="math inline">\(\sigma\)</span>表示“hard sigmoid”函数 <span class="math display">\[
\sigma(x)=\operatorname{clip}\left(\frac{x+1}{2}, 0,1\right)=\max \left(0, \min \left(1, \frac{x+1}{2}\right)\right)
\]</span> 直观上来说，<span class="math inline">\(x\)</span>是一个正值，我们将以很高的概率量化到<span class="math inline">\(+1\)</span>，其他情况量化到<span class="math inline">\(-1\)</span>。这就给我们更加灵活的量化模式。在[Muller and Indi-veri, 2015]中，作者在整数规划中使用了这种思想。提出的随机四舍五入函数将每个实值概率映射到最近的离散点或第二最近的离散点，这取决于到对应点的距离。在[Lin et al., 2015]中，将二进随机四舍五入扩展到三元情形。</p>
<h5 id="挑战-2">挑战</h5>
<p>随机舍入提供了一种将噪声注入训练过程的方法。它可以作为一个正则化器和可提供条件计算。然而，使用随机的舍入方法，我们需要估计离散神经元的梯度。这样的估计往往有很高的方差。这一事实可能会导致训练过程中损失函数的振荡。[Bengio等人，2013]的工作提供了估计离散中性电子梯度的可能解决方案的概述。</p>
<h4 id="概率量化">概率量化</h4>
<p>暂无</p>
<h4 id="讨论">讨论</h4>
<p>上述量化技术使我们能够从不同的角度对网络进行量化。这些技术的优缺点可以指导我们在不同的情况下选择合适的技术。一般来说，如果我们想为硬件加速量化神经网络，确定性量化应该是首选，因为我们可以预先指定适当的量化级别，以便在专用硬件上运行量化的网络工作。这可以提高硬件的预测性能。四舍五入使我们能够以数据依赖的方式量化权重。这导致了条件计算[Bengio et al.， 2013]，可以增加神经网络的容量。概率量化与确定性量化的不同之处在于量化后的权重更具有可解释性。我们可以用概率量化的方法来理解权值的分布，并对网络的工作原理有更深入的了解。在概率量化的情况下，由于贝叶斯方法的正则化效果，我们也可以得到更稀疏的模型。</p>
<h2 id="量化对象分类">量化对象分类</h2>
<h3 id="权重量化">权重量化</h3>
<p>在[Zhou et al., 2017a]中，作者提出了增量式网络量化(INQ)，它包括三个步骤：权重划分、分组量化和再训练。他们以组的方式对权重进行组化，以允许某些权重组补偿由于其他组的量化而造成的准确性损失。[Gudovskiy and Rigazio, 2017]的工作将这种方法扩展到2次幂设置。</p>
<p>在[Lin et al., 2016]中，作者试图找到最优的不动点位宽跨层分配。他们研究了通过量化不同的层可以引入多少噪音。[Lin et al., 2017]使用多个二进制基的线性组合近似全精度权重。结果表明，二值神经网络在ImageNet数据集上首次取得了与全精度神经网络相当的预测精度。在[Moons et al., 2017]中，作者研究了如何发展节能量化神经网络。在[Guo et al., 2017]的工作中引入了网络素描来量化预先训练好的模型。其思想是使用二进制基来近似预先训练过的滤波器。他们首先提出了一种启发式算法来寻找二进制基础，然后提供了一个改进版本，以更好的近似。在[Mohamed Amer, 2018]中，作者提出了一种端到端训练框架来同时优化原始损失函数、量化误差和总比特数。然而，其精度无法与其他量化神经网络相比。</p>
<h3 id="梯度量化">梯度量化</h3>
<p>梯度量化的应用场景一般是为了在分布式训练过程中减小通信消耗。如下图就是为神经网络的并行训练：</p>
<p><img src="/archives/2932556/image-20200729203114640.png" alt="image-20200729203114640" style="zoom: 80%;"></p>
<p>[Seide et al., 2014<a href="#refer-anchor-5"><sup>5</sup></a>]提出了一种1-bit表示各个节点计算梯度。和常规的方法相比，它得到了10倍的加速比。[Strom, 2015<a href="#refer-anchor-6"><sup>6</sup></a>]作者提出了一种阈值量化方法。提前选定一个阈值，如果梯度大于这个阈值就量化为<span class="math inline">\(+1\)</span>，如果小于这个阈值就量化为<span class="math inline">\(0\)</span>。[Alistarh et al., 2016<a href="#refer-anchor-7"><sup>7</sup></a>]提出了QSGD方法，允许每个节点在精度、梯度和模型的精度之间进行权衡。QSGD利用随机四舍五入的思想将梯度量化为一组离散值，并利用无损编码产生高效的编码。[Dryden et al., 2016<a href="#refer-anchor-8"><sup>8</sup></a>]]作者提出一种简单的自适应量化方法来选择合适的梯度进行量化并发送。</p>
<p>[Wen et al., 2017<a href="#refer-anchor-9"><sup>9</sup></a>]]通过提出了TernGrad解决了并行过程中水平扩展的问题。他将梯度量化为<span class="math inline">\(\{-1,0,1\}\)</span>。在发送给中心参数服务器之前，每个梯度都将被如下量化： <span class="math display">\[
\tilde{\Delta}_{t}=\operatorname{ternarize}\left(\Delta_{t}\right)=s_{t} \cdot \operatorname{sign}\left(\Delta_{t}\right) \circ b_{t}
\]</span> 其中<span class="math inline">\(s_{t}=\max \left(\operatorname{abs}\left(\Delta_{t}\right)\right)\)</span>，其中<span class="math inline">\(\circ\)</span>是<a href="https://www.baidu.com/link?url=w-LVD0IIl4PbHY-Vzc-UEKgvX7_8m_uRUWXH56DVxRjY77fZbsptVsF2pO7Gmxx3cRBReXg7sz3JTvIakfzCprx8m2RSMUNzT_E2Ppx3wyfA6RMWuAA7E66zF4btnsYK&amp;wd=&amp;eqid=8509435200010e68000000055f216602" target="_blank" rel="noopener external nofollow noreferrer">哈达玛积</a>，<span class="math inline">\(b_t\)</span>是遵循如下伯努利分布的随机二进制向量： <span class="math display">\[
\left\{\begin{array}{l}
P\left(b_{t k}=1 \mid \Delta_{t}\right)=\left|\Delta_{t k}\right| / s_{t} \\
P\left(b_{t k}=0 \mid \Delta_{t}\right)=1-\left|\Delta_{t k}\right| / s_{t}
\end{array}\right.
\]</span> 采用这种方法，服务器和工作节点之间的通信开销可以降低接近20倍.</p>
<p>在单一节点的环境中，我们也能够通过量化梯度获得益处。为了减小反向传播的计算开销，[Rastegari et al., 2016<a href="#refer-anchor-11"><sup>11</sup></a>]将梯度量化为2-bits来进行高性能通信。[Zhou et al., 2016<a href="#refer-anchor-10"><sup>10</sup></a>]他还量化了反向传播过程中的梯度。他们发现使用随机四舍五入的方法是非常重要的，使量程梯度工作得很好。他们设计了如下的k比特量化函数， <span class="math display">\[
\tilde{f}_{\gamma}^{k}(d r)=2 \max _{0}(|d r|)\left[\text { quantize }_{k}\left(\frac{d r}{2 \max _{0}(|d r|)+\frac{1}{2}}\right)-\frac{1}{2}\right]
\]</span> 其中<span class="math inline">\(d r=\frac{\partial c}{\partial r}\)</span>是在某些层输出<span class="math inline">\(r\)</span>的梯度，<span class="math inline">\(quantize_k\)</span>被用于量化一个实数输入<span class="math inline">\(r_{i} \in[0,1]\)</span>到k-bit输出值<span class="math inline">\(r_{0} \in[0,1]\)</span>， <span class="math display">\[
r_{o}=\frac{1}{2^{k}-1} \operatorname{round}\left(\left(2^{k}-1\right) r_{i}\right)
\]</span> 他们还在训练过程中加入额外的噪声，以弥补量化造成的准确性损失。</p>
<h5 id="挑战-3">挑战</h5>
<ul>
<li><p>梯度的大小和符号对于更新权重都很重要。为了量化梯度，我们必须解决如何将这两个因素纳入计算的问题。</p></li>
<li><p>一种简单的量化梯度的方法可能在实践中并不奏效，因为它可能违反随机梯度下降算法的收敛条件。在这种情况下需要更复杂的方法。</p></li>
</ul>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener external nofollow noreferrer">Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener external nofollow noreferrer">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html" target="_blank" rel="noopener external nofollow noreferrer">Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" target="_blank" rel="noopener external nofollow noreferrer">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-7">

</div></li>
<li>[7] <a href="https://www.isca-speech.org/archive/interspeech_2015/i15_1488.html" target="_blank" rel="noopener external nofollow noreferrer">Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. 2015.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="https://ieeexplore.ieee.org/abstract/document/7835789/" target="_blank" rel="noopener external nofollow noreferrer">Dryden N, Moon T, Jacobs S A, et al. Communication quantization for data-parallel training of deep neural networks[C]//2016 2nd Workshop on Machine Learning in HPC Environments (MLHPC). IEEE, 2016: 1-8.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/pdf/1606.06160" target="_blank" rel="noopener external nofollow noreferrer">Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients[J]. arXiv preprint arXiv:1606.06160, 2016.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="https://link.springer.com/chapter/10.1007/978-3-319-46493-0_32" target="_blank" rel="noopener external nofollow noreferrer">Rastegari M, Ordonez V, Redmon J, et al. Xnor-net: Imagenet classification using binary convolutional neural networks[C]//European conference on computer vision. Springer, Cham, 2016: 525-542.</a></li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>Error Compensated Quantized SGD and its Applications to Large-scale</title>
    <url>/archives/fcc7e450.html</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文提出了一种错误补偿的随机梯度量化方法来提高训练效率。量化本地梯度来减小通信负担，但是累积的量化误差会影响收敛的速度。此外，本文对收敛行为进行了理论分析，并证明了其相对于其他算法的优势。实验表明该算法能够再不降级影响收敛性能的情况下，将梯度压缩两个数量级。</p>
<h2 id="介绍">介绍</h2>
<p>一些方法注重将梯度量化为固定值，这样可以用更少的bits来进行通信传输（Zhou et al 2016<a href="#refer-anchor-1"><sup>1</sup></a>）；还有更加激进的方法，比如二进制或者三元组的表达（Seide et al., 2014<a href="#refer-anchor-2"><sup>2</sup></a>;Strom, 2015<a href="#refer-anchor-3"><sup>3</sup></a>; Wen et al., 2017<a href="#refer-anchor-4"><sup>4</sup></a>）；其他方法是在通信的过程中进行稀疏化，其中，每次迭代只有梯度的一小部分在节点之间通信传输（Wangni et al., 2017<a href="#refer-anchor-5"><sup>5</sup></a>; Lin et al., 2018<a href="#refer-anchor-6"><sup>6</sup></a>）</p>
<p>本文提出了误差补偿随机梯度量化方法，称为EC-SGD。该算法和<span class="math inline">\(1Bits\)</span>算法<a href="#refer-anchor-2"><sup>2</sup></a>不太相同，它将之前量化的所有梯度误差都考虑在内，并不只是使用最新一轮的量化误差。</p>
<p>在（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）中，作者证明了所提出的QSGD算法达到某次最优间隙所需的迭代次数与随机量化梯度的方差界成正比。然而这不能够解释我们方法的收敛行为，因为我们量化梯度是有偏估计，并不像QSGD那样。事实上，量化梯度的方差边界比在QSGD当中的更大，因为量化误差的累计会更大。为了解决这个问题，我们从另一个角度给出了收敛性分析，并且证明了我们的算法比QSGD算法有着更加严格的最坏情况的错误边界。结果表明，我们提出的误差反馈方案可以很好地抑制量化误差对误差界的影响，我们在实验中观察到，与QSGD相比，次最优性间隙更小。</p>
<h2 id="相关工作">相关工作</h2>
<h3 id="异步sgd">异步SGD</h3>
<p>Hogwild!（Recht et al. 2011）（其他工作相对来说时间比较久远，这里就不罗列了）</p>
<h3 id="梯度量化">梯度量化</h3>
<p>（Seide et al., 2014<a href="#refer-anchor-2"><sup>2</sup></a>）<span class="math inline">\(1Bit-SGD\)</span>用于对梯度的量化，将<span class="math inline">\(0\)</span>作为阈值，量化为<span class="math inline">\(1\)</span>或者<span class="math inline">\(-1\)</span>。在量化过程中引入上一轮的量化误差作为反馈。相似的想法在（Strom, 2015<a href="#refer-anchor-3"><sup>3</sup></a>）中被采纳，它通过迭代累计局部梯度并且仅传输超过预先选择的阈值的梯度分量。（Wen et al., 2017<a href="#refer-anchor-4"><sup>4</sup></a>）扩展了这一想法，并且将梯度压缩至了三元组来保证其无偏性。QSGD（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）采用均匀分布的方式随机量化梯度，更加细致地分析其收敛性。ZipML（Zhang et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>）介绍了一种优化的量化策略，在分布式状态下动态选择量化节点。（Zhou et al 2016<a href="#refer-anchor-1"><sup>1</sup></a>）提出了DoReFa-Net来训练卷积神经网络，将输入、权重和梯度都进行定点的量化。</p>
<h3 id="梯度稀疏化">梯度稀疏化</h3>
<p>梯度丢弃方法是由（Aji &amp; Heafield, 2017<a href="#refer-anchor-10"><sup>10</sup></a>）将稀疏化方法引入到梯度当中，来减小通信误差。在（Wangni et al., 2017<a href="#refer-anchor-11"><sup>11</sup></a>）将梯度量化抽象成了线性规划问题，目的就是最小化量化梯度的方差增长。（Lin et al., 2018<a href="#refer-anchor-6"><sup>6</sup></a>）提出了深度梯度压缩算法，利用动量校正，梯度剪裁，动量因子掩饰，热身训练，以实现更高的稀疏性而不失去准确性。</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>（比较容易理解，暂时不翻译）</p>
<h2 id="误差压缩量化sgd方法">误差压缩量化SGD方法</h2>
<p>在每一轮迭代，之前每一轮的累计的量化误差都会对当前本地梯度进行补偿，再通过随机量化函数进行压缩。</p>
<p>令<span class="math inline">\(Q: \mathbb{R}^{d} \rightarrow \mathcal{C}^{d}\)</span>是一个无偏的随机量化函数，它将每部分的<span class="math inline">\(d\)</span>个维度向量映射到量化密码本<span class="math inline">\(\mathcal{C}\)</span>中。密码本通常只包含有限数量的元素，因此量化向量能够高效的编码。在每次迭代中，每个节点在广播之前量化他们的本地梯度： <span class="math display">\[
\tilde{\mathbf{g}}_{p}^{(t)}=Q\left(\mathbf{g}_{p}^{(t)}\right)
\]</span> 其中<span class="math inline">\(\mathbf{g}_{p}^{(t)}\)</span>是第<span class="math inline">\(p\)</span>个节点和第<span class="math inline">\(t\)</span>次迭代的本地梯度，<span class="math inline">\(\tilde{\mathbf{g}}_{p}^{(t)}\)</span>表示他的量化产物。</p>
<p>当节点接收到所有从其他节点发送过来的本地梯度之后，它将计算全局梯度以及更新它本地的模型，采用以下式子： <span class="math display">\[
\mathbf{w}^{(t+1)}=\mathbf{w}^{(t)}-\eta \cdot \tilde{\mathbf{g}}^{(t)}=\mathbf{w}^{(t)}-\frac{\eta}{P} \sum_{p=1}^{P} \tilde{\mathbf{g}}_{p}^{(t)}
\]</span></p>
<p>其中<span class="math inline">\(\eta&gt;0\)</span>表示学习率。</p>
<p>ECQ-SGD的核心思想是当量化本地梯度时，当前梯度和之前累加的量化误差都会考虑在内。特别的，我们使用<span class="math inline">\(\mathbf{h}_{p}^{(t)}\)</span>代表第<span class="math inline">\(p\)</span>个节点的第<span class="math inline">\(t\)</span>次迭代的累计量化误差： <span class="math display">\[
\mathbf{h}_{p}^{(t)}=\sum_{t^{\prime}=0}^{t-1} \beta^{t-1-t^{\prime}}\left(\mathbf{g}_{p}^{\left(t^{\prime}\right)}-\tilde{\mathbf{g}}_{p}^{\left(t^{\prime}\right)}\right)
\]</span> 其中<span class="math inline">\(\beta\)</span>是时间减弱因子。注意到累计量化误差的更新式如下： <span class="math display">\[
\mathbf{h}_{p}^{(t)}=\beta \mathbf{h}_{p}^{(t-1)}+\left(\mathbf{g}_{p}^{(t-1)}-\tilde{\mathbf{g}}_{p}^{(t-1)}\right)
\]</span> 其中<span class="math inline">\(\mathbf{h}_{p}^{(0)}=\mathbf{0}\)</span>。量化本地梯度将会通过量化函数计算出误差补偿梯度： <span class="math display">\[
\tilde{\mathbf{g}}_{p}^{(t)}=Q\left(\mathbf{g}_{p}^{(t)}+\alpha \mathbf{h}_{p}^{(t)}\right)
\]</span> 其中<span class="math inline">\(\alpha\)</span>代表补偿系数。</p>
<p>这里我们采用服从均匀分布的随机量化函数，类似于QSGD算法（Alistarh et al., 2017<a href="#refer-anchor-8"><sup>8</sup></a>），其中第<span class="math inline">\(i\)</span>个维度将会量化为： <span class="math display">\[
\tilde{g}_{i}=\|\mathbf{g}\| \cdot \operatorname{sgn}\left(g_{i}\right) \cdot \xi\left(\left|g_{i}\right| ;\|\mathbf{g}\|\right)
\]</span> 其中$|| <span class="math inline">\(表示扩展因子（可以选择\)</span>l_2<span class="math inline">\(或者\)</span>l_{}<span class="math inline">\(这两种方式），\)</span>()<span class="math inline">\(是随机函数，是映射到如下元素中\)</span>{0, , , 1}$： <span class="math display">\[
\xi\left(\left|g_{i}\right| ;\|\mathbf{g}\|\right)=\left\{\begin{array}{ll}
\frac{l}{s}, &amp; \text { with probability } l+1-s \cdot \frac{\left|g_{i}\right|}{\|\mathbf{g}\|} \\
\frac{l+1}{s}, &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中<span class="math inline">\(\left|g_{i}\right| /\|\mathbf{g}\|\)</span>是落在<span class="math inline">\(\left[\frac{l}{s}, \frac{l+1}{s}\right)\)</span>范围之内。超参数<span class="math inline">\(s\)</span>表示非零量化等级：更大的<span class="math inline">\(s\)</span>会导致更加细粒度的量化，同时导致更多的通信消耗。我们采用<span class="math inline">\(Q_{s}(\cdot)\)</span>代表量化函数，<span class="math inline">\(s\)</span>表示非零量化等级。</p>
<p>在量化之后，我们只需要使用<span class="math inline">\(r=\left\lceil\log _{2}(2 s+1)\right\rceil\)</span>bits来编码每个梯度<span class="math inline">\(\tilde{g}_{i}\)</span>，一个完整的精度表示扩展因子$ ||$。全部的通信需要花费<span class="math inline">\(32+d\)</span>bits（<span class="math inline">\(r \ll 32\)</span>）,原始的梯度需要每个维度32-bit的全精度来表示。更加有效的编码模式，例如Huffman编码，能够进一步地减小通信量。令<span class="math inline">\(d_k\)</span>表示分配各<span class="math inline">\(k\)</span>个量化级别的维数，之后整个编码长度最多只需要<span class="math inline">\(\sum_{k=1}^{2 s+1} d_{k} \log _{2} \frac{d}{d_{k}}\)</span>个bits。</p>
<p>算法1概括了上述的整个过程：</p>
<p><img src="/archives/fcc7e450/image-20200728190425444.png" alt="image-20200728190425444" style="zoom:67%;"></p>
<h2 id="理论分析">理论分析</h2>
<p>（暂时省略）</p>
<h2 id="实验分析">实验分析</h2>
<h3 id="线性模型">线性模型</h3>
<p>使用三个人造数据集：Syn-256、Syn-512和Syn-1024。每个数据集包含10k个训练样本，后缀表示特征维度<span class="math inline">\(d\)</span>。训练样本通过<span class="math inline">\(y_{i}=\mathbf{w}^{* T} \mathbf{x}_{i}+\epsilon_{i}\)</span>生成，其中<span class="math inline">\(\mathbf{w}^{*} \in \mathbb{R}^{d}\)</span>是我们希望获得的潜在模型参数，<span class="math inline">\(\left\{\epsilon_{i}\right\}\)</span>是服从独立同分布的随机噪声。学习率是0.02，QSGD和ECQ-SGD都采用<span class="math inline">\(l_2\)</span>作为扩展因子，采用<span class="math inline">\(4\)</span>作为量化等级。</p>
<p>下图我们比较了损失函数值（上方）和距离最优解的距离（下方）。对于这三个数据集，ECQ-SGD损失函数的收敛性更加接近于<span class="math inline">\(32-bit\)</span>全精度的SGD算法，比<span class="math inline">\(QSGD\)</span>的训练速度明显更快。另一方面，QSGD(或ECQ-SGD)与32Bit-FP在最优解距离上的差距度量了量化误差对(21)中定义的误差界的贡献。ECQ-SGD的距离差明显小于QSGD，说明量化误差对误差界的贡献得到了很好的抑制。</p>
<p>下面我们比较了在大数据集上，QSGD和ECQ-SGD运行时训练速率，Syn-20k，它包括了50k训练样本和20k维度的特征。在图三中，我们多维度展示了在1k轮迭代之后各种方法的时间消耗和测试误差。我们发现ECQ-SGD达到和32Bit-FP相似的测试误差，比32Bit-FP和QSGD所用的时间更短。虽然ECQ-SGD需要额外的编码好解码时间，由于通信量的降低，整个训练速度依然得到了提高：</p>
<p><img src="/archives/fcc7e450/image-20200728192202073.png" alt="image-20200728192202073" style="zoom:67%;"></p>
<p>其次，本文还对两个公开的数据集<code>YearPredictionMSD</code>（回归）和<code>gisette</code>分类。在不同的量化方法下对这两个数据集的逻辑回归和线性回归做了测评：</p>
<p><img src="/archives/fcc7e450/image-20200728192421592.png" alt="image-20200728192421592" style="zoom: 80%;"></p>
<h3 id="卷积神经网络">卷积神经网络</h3>
<p>本文还在卷积神经网络上做了测试。CIFAR-10和ResNet-20模型，采用不同的量化方法，结果如图所示：</p>
<figure>
<img src="/archives/fcc7e450/image-20200728192625094.png" alt="image-20200728192625094"><figcaption>image-20200728192625094</figcaption>
</figure>
<p>在第一列中，比较了各种方法的整体通信负担和损失函数值的情况。与32位全精度基准方法相比，每个模型的超参数被分离，以达到可以忽略的精度损失。我们发现所有的方法都以相似的速度收敛，但ECQ-SGD在通信成本上降低了80倍以上，并且显著优于其他梯度量化方法。</p>
<p>在第二列和第三列中，我们比较了它和QSGD的细节，因为它和我们的方法最为相关。我们采用了不同的扩展因子：第二列是<span class="math inline">\(l_2\)</span>，第三列是<span class="math inline">\(l_{\infty}\)</span>。我们发现我们观察到，ECQ-SGD在收敛速度和分类精度方面始终优于QSGD，而在相同的超参数设置下，这两种方法在降低通信成本方面是相似的。</p>
<h3 id="性能模型">性能模型</h3>
<p>我们采用（Yan et al., 2015）提出的性能评估模型对ECQ-SGD算法进行评估。对计算量和通信时间进行轻量级分析，以估计较大集群的学习效率。主要硬件规格如下:Intel Xeon E5-2680 CPU, Nvidia Tesla P40 GPU(每个节点8个单元)，Mellanox ConnectX-3 Pro网卡(40Gb/s连通性)。</p>
<p>下图中，我们展示了采用ResNet-50模型训练ILSVRC-12数据集。训练512个GPU，ECQ-SGD先比于普通的SGD达到了143.5%的加速比。</p>
<p><img src="/archives/fcc7e450/image-20200728193508865.png" alt="image-20200728193508865" style="zoom:67%;"></p>
<h2 id="结论">结论</h2>
<p>为了提高大规模分布式优化的学习效率，本文提出了误差补偿量化SGD算法。通过引入误差反馈机制，ECQ-SGD算法可以有效地抑制量化误差对误差界的贡献。我们从理论的角度分析了它的收敛行为，并证明了它比最先进的QSGD算法的优势。在线性模型和非凸卷积神经网络上的实验证明了该算法的有效性。</p>
<h2 id="部分参考文献">部分参考文献</h2>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="https://arxiv.org/pdf/1606.06160" target="_blank" rel="noopener external nofollow noreferrer">Zhou S, Wu Y, Ni Z, et al. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients[J]. arXiv preprint arXiv:1606.06160, 2016.</a></li>
</ul>
<div id="refer-anchor-2">

</div>
<ul>
<li>[2] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a></li>
</ul>
<div id="refer-anchor-3">

</div>
<ul>
<li>[3] <a href="https://www.isca-speech.org/archive/interspeech_2015/papers/i15_1488.pdf" target="_blank" rel="noopener external nofollow noreferrer">Strom N. Scalable distributed DNN training using commodity GPU cloud computing[C]//Sixteenth Annual Conference of the International Speech Communication Association. 2015.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-5">

</div></li>
<li>[5] <a href="http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wangni J, Wang J, Liu J, et al. Gradient sparsification for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2018: 1299-1309.</a>
<div id="refer-anchor-6">

</div></li>
<li>[6] <a href="https://arxiv.org/pdf/1712.01887" target="_blank" rel="noopener external nofollow noreferrer">Lin Y, Han S, Mao H, et al. Deep gradient compression: Reducing the communication bandwidth for distributed training[J]. arXiv preprint arXiv:1712.01887, 2017.</a>
<div id="refer-anchor-7">

</div></li>
<li>[7] <a href="http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf" target="_blank" rel="noopener external nofollow noreferrer">Recht B, Re C, Wright S, et al. Hogwild: A lock-free approach to parallelizing stochastic gradient descent[C]//Advances in neural information processing systems. 2011: 693-701.</a>
<div id="refer-anchor-8">

</div></li>
<li>[8] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-9">

</div></li>
<li>[9] <a href="http://proceedings.mlr.press/v70/zhang17e.html" target="_blank" rel="noopener external nofollow noreferrer">Zhang H, Li J, Kara K, et al. ZipML: Training linear models with end-to-end low precision, and a little bit of deep learning[C]//International Conference on Machine Learning. 2017: 4035-4043.</a>
<div id="refer-anchor-10">

</div></li>
<li>[10] <a href="https://arxiv.org/pdf/1704.05021" target="_blank" rel="noopener external nofollow noreferrer">Aji A F, Heafield K. Sparse communication for distributed gradient descent[J]. arXiv preprint arXiv:1704.05021, 2017.</a>
<div id="refer-anchor-11">

</div></li>
<li>[11] <a href="http://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wangni J, Wang J, Liu J, et al. Gradient sparsification for communication-efficient distributed optimization[C]//Advances in Neural Information Processing Systems. 2018: 1299-1309.</a>
<div id="refer-anchor-12">

</div></li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>QSGD Communication-Efficient SGD via Gradient Quantization and Encoding</title>
    <url>/archives/d06ef2e3.html</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文提出了量化SGD（QSGD）方法，可以帮助使用者平滑地权衡通信带宽和收敛时间：在方差成本可能较高的情况下，节点可以调节每轮迭代发送的比特数。本文证明了这种权衡是内在的，某种意义上，提高它超过某个阈值将违反信息理论的下界。QSGD保证了在凸和非凸目标函数上的收敛性，在异步条件下，能够使用随机方差减小的方法进行拓展。</p>
<h2 id="简介">简介</h2>
<p>比较流行的减少通信量通常采用有损压缩的方法<a href="#refer-anchor-1"><sup>1</sup></a><a href="#refer-anchor-2"><sup>2</sup></a><a href="#refer-anchor-3"><sup>3</sup></a>。一种方式是简单地降低表示精度，在凸和稀疏的条件下证明了收敛性<a href="#refer-anchor-4"><sup>4</sup></a>。<span class="math inline">\(1BitSGD\)</span>量化方法为梯度的量化压缩奠定了基础。</p>
<h3 id="本文贡献">本文贡献</h3>
<p>本文重点关注数据并行SGD算法的通信开销和收敛速率保证。我们提出有损的梯度压缩方法：QSGD算法，它可以在每一次迭代中在通信量和方差之间做平衡。</p>
<p>QSGD建立在两种算法思想上。第一种是直观的随机量化模式：在某进程中给出一个梯度向量，我们通过四舍五入到一组离散的值来量化每个部分，有原则地保持原始数据的统计性质。第二种是量化梯度的有效无损代码，利用他们的统计属性来生成有效的编码。我们分析给出了QSGD引起的精度-方差权衡的严格界限。</p>
<p>QSGD非常普遍，在假设条件下，对于非凸目标和异步迭代，能够收敛到局部最小。在非平凡扩展的情况下，我们创新了一种随机方差减小的QSGD算法，称为QSVRG，它具有指数级的收敛速率。</p>
<p>一个关键的问题是QSGD的压缩方差平衡是否是固有的：例如，任何算法保证最多恒定方差放大每次迭代需要传输<span class="math inline">\(\Omega(n)\)</span>bits。回答是肯定的：在此基础上的渐近改进将打破分布式平均估计的通信复杂度下限（不是很懂这块内容）。</p>
<p>与1BitSGD相比，QSGD具有渐近高压缩性能，在标准假设下可证明收敛性，在某些情况下具有较好的实际性能。</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>（暂时省略）</p>
<h2 id="量化随机梯度下降qsgd">量化随机梯度下降（QSGD）</h2>
<h3 id="一般性的随机量化和编码">一般性的随机量化和编码</h3>
<h4 id="随机量化">随机量化</h4>
<p>我们现在思考随机梯度向量一般参数有损压缩方案。压缩函数用<span class="math inline">\(Q_s(v)\)</span>表示，其中<span class="math inline">\(s \geq 1\)</span>调节参数，代表我们所要实现的量化的程度。直观上来看，我们定义<span class="math inline">\(s\)</span>服从<span class="math inline">\(0-1\)</span>的均匀分布，每个值都以一种保持预期值并引入最小方差的方式进行量化，例如下图：</p>
<p><img src="/archives/d06ef2e3/image-20200728104606218.png" alt="image-20200728104606218" style="zoom:67%;"></p>
<p>上图是5级的广义随机量化的示例</p>
<p>对于任意的<span class="math inline">\(\boldsymbol{v} \in \mathbb{R}^{n} \text { with } \boldsymbol{v} \neq \mathbf{0})\)</span>，<span class="math inline">\(Q_{s}(\boldsymbol{v})\)</span>定义如下： <span class="math display">\[
Q_{s}\left(v_{i}\right)=\|\boldsymbol{v}\|_{2} \cdot \operatorname{sgn}\left(v_{i}\right) \cdot \xi_{i}(\boldsymbol{v}, s)
\]</span> 其中<span class="math inline">\(\xi_{i}(\boldsymbol{v}, s)\)</span>是一个独立随机变量，定义如下。令<span class="math inline">\(0 \leq \ell&lt;s\)</span>是一个整数，例如<span class="math inline">\(\left|v_{i}\right| /\|\boldsymbol{v}\|_{2} \in[\ell / s,(\ell+1) / s]\)</span>。<span class="math inline">\([\ell / s,(\ell+1) / s]\)</span>是与$|v_{i}| /||_{2} $相符的量化间隔。定义： <span class="math display">\[
\xi_{i}(\boldsymbol{v}, s)=\left\{\begin{array}{ll}
\ell / s &amp; \text { 以如下概率 } 1-p\left(\frac{\left|v_{i}\right|}{\|\boldsymbol{v}\|_{2}}, s\right) \\
(\ell+1) / s &amp; \text { 其他情况. }
\end{array}\right.
\]</span> 这里<span class="math inline">\(p(a, s)=a s-\ell\)</span>对于任何的<span class="math inline">\(a \in[0,1]\)</span>。如果<span class="math inline">\(v=0\)</span>，我们定义<span class="math inline">\(Q(v,s)=0\)</span>。</p>
<p><span class="math inline">\(\xi_{i}(\boldsymbol{v}, s)\)</span>的分布具有最小的方差，它的期望满足<span class="math inline">\(\mathbb{E}\left[\xi_{i}(\boldsymbol{v}, s)\right]=\left|v_{i}\right| /\|\boldsymbol{v}\|_{2}\)</span>。我们可以证明如下：</p>
<p><strong>引理</strong> 对于任意<span class="math inline">\(\boldsymbol{v} \in \mathbb{R}^{n}\)</span>，我们有（1）<span class="math inline">\(\mathbb{E}\left[Q_{s}(\boldsymbol{v})\right]=\boldsymbol{v} \text { (无偏) }\)</span>；（2）<span class="math inline">\(\mathbb{E}\left[\left\|Q_{s}(\boldsymbol{v})-\boldsymbol{v}\right\|_{2}^{2}\right] \leq \min \left(n / s^{2}, \sqrt{n} / s\right)\|\boldsymbol{v}\|_{2}^{2}\)</span>（方差最小边界）；（3）<span class="math inline">\(\mathbb{E}\left[\left\|Q_{s}(\boldsymbol{v})\right\|_{0}\right] \leq s(s+\sqrt{n})\)</span>稀疏性</p>
<h4 id="梯度的高效编码">梯度的高效编码</h4>
<p>对于任意的向量<span class="math inline">\(v\)</span>，输出<span class="math inline">\(Q_s(v)\)</span>可以表达为三元组<span class="math inline">\(\left(\|v\|_{2}, \sigma, \zeta\right)\)</span>，其中<span class="math inline">\(\sigma\)</span>是向量中<span class="math inline">\(v_i\)</span>的符号，<span class="math inline">\(\zeta\)</span>是整数<span class="math inline">\(s \cdot \xi_{i}(\boldsymbol{v}, s)\)</span>的向量。我们可以发现，大的整数出现的频率很低。我们将通过专门的<code>Eilias</code>整数编码来利用这一点。</p>
<p>直觉上，对于任何正整数<span class="math inline">\(k\)</span>，他的编码用<span class="math inline">\(Elias(k)\)</span>表示<span class="math inline">\(k\)</span>的二进制，并且在它的前面加上长度表示。然后递归地编码这个前缀。我们发现对于任何整数<span class="math inline">\(k\)</span>，结果编码的长度为<span class="math inline">\(\operatorname{Elias}(k) \mid=\log k+\log \log k+\ldots+1 \leq(1+o(1)) \log k+1\)</span>。编码和解码过程非常高效。</p>
<p>给定的梯度向量可以表示为三元组<span class="math inline">\(\left(\|\boldsymbol{v}\|_{2}, \boldsymbol{\sigma}, \boldsymbol{\zeta}\right)\)</span>，具有<span class="math inline">\(s\)</span>压缩等级，我们的编码输出为字符串<span class="math inline">\(S\)</span>，定义如下：第一使用<span class="math inline">\(32bits\)</span>编码<span class="math inline">\(\|\boldsymbol{v}\|_{2}\)</span>。它继续使用<span class="math inline">\(Elias\)</span>递归编码读第一个非零项<span class="math inline">\(\boldsymbol{\zeta}\)</span>的位置进行编码。接下来附加一个bit代表<span class="math inline">\(\boldsymbol{\sigma}_i\)</span>，遵循<span class="math inline">\(s \cdot \xi_{i}(\boldsymbol{v}, s)\)</span>。迭代进行，它继续从当前的写入坐标到下一个非零的距离，并以同样的方式对坐标的<span class="math inline">\(\boldsymbol{\sigma}_i\)</span>和<span class="math inline">\(\boldsymbol{\zeta}_i\)</span>进行编码。解码方案很简单：我们首先读取<span class="math inline">\(32bits\)</span>来构建<span class="math inline">\(\|\boldsymbol{v}\|_{2}\)</span>，接下来，迭代使用<span class="math inline">\(Elias\)</span>递归编码的译码方案读取非零项<span class="math inline">\(\boldsymbol{\zeta}\)</span>和<span class="math inline">\(\boldsymbol{\sigma}\)</span>的位置和值。</p>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf" target="_blank" rel="noopener external nofollow noreferrer">Dean J, Corrado G, Monga R, et al. Large scale distributed deep networks[C]//Advances in neural information processing systems. 2012: 1223-1231.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1603.04467.pdf" target="_blank" rel="noopener external nofollow noreferrer">Abadi M, Agarwal A, Barham P, et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems[J]. arXiv preprint arXiv:1603.04467, 2016.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="http://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf" target="_blank" rel="noopener external nofollow noreferrer">Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</a>
<div id="refer-anchor-4">

</div></li>
<li>[4] <a href="http://papers.nips.cc/paper/5717-taming-the-wild-a-unified-analysis-of-hogwild-style-algorithms.pdf" target="_blank" rel="noopener external nofollow noreferrer">De Sa C M, Zhang C, Olukotun K, et al. Taming the wild: A unified analysis of hogwild-style algorithms[C]//Advances in neural information processing systems. 2015: 2674-2682.</a></li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>粉笔公考-判断推理-图形推理</title>
    <url>/archives/487a2936.html</url>
    <content><![CDATA[<h2 id="判断推理题型">判断推理题型</h2>
<ul>
<li>图形推理</li>
<li>类比推理</li>
<li>定义判断</li>
<li>逻辑判断</li>
</ul>
<h2 id="图形推理">图形推理</h2>
<h3 id="命题形式">命题形式</h3>
<h4 id="一组图">一组图</h4>
<p>大多数：从左到右整体去看</p>
<p>有时候：跳着看，考地相对较少</p>
<p><img src="/archives/487a2936/image-20200727205210815.png" alt="image-20200727205210815" style="zoom:67%;"></p>
<h4 id="两组图">两组图</h4>
<p>第一组用于找规律，第二组用规律（模仿第一组的规律即可，细节变化以第二组图为准）</p>
<p><img src="/archives/487a2936/image-20200727205147584.png" alt="image-20200727205147584" style="zoom:67%;"></p>
<h4 id="九宫格">九宫格</h4>
<p>优先横着看，其次再是竖着看（很少斜着、S型和米子型）</p>
<h4 id="分组分类">分组分类</h4>
<p>一般分成两组，在组内找出各自的规律（找出两个规律）</p>
<p><img src="/archives/487a2936/image-20200727205400400.png" alt="image-20200727205400400" style="zoom:67%;"></p>
<h4 id="空间类折纸盒">空间类：折纸盒</h4>
<p>六面体为主，转化成平面</p>
<p><img src="/archives/487a2936/image-20200727205533123.png" alt="image-20200727205533123" style="zoom:67%;"></p>
<h5 id="特殊题型">特殊题型</h5>
<ul>
<li>截面图</li>
<li>三视图</li>
<li>立体拼合</li>
</ul>
<h2 id="六大规律">六大规律</h2>
<p>重点通过识别图形特征，来识别考察什么规律</p>
<ol type="1">
<li>位置规律</li>
<li>样式规律</li>
<li>属性规律</li>
<li>特殊规律</li>
<li>数量规律</li>
<li>空间规律</li>
</ol>
<h3 id="位置规律">位置规律</h3>
<h4 id="特征">特征</h4>
<p>位置类识别特征：各图元素组成相同</p>
<p><img src="/archives/487a2936/image-20200727210022937.png" alt="image-20200727210022937" style="zoom:67%;"></p>
<h4 id="考点">考点</h4>
<ul>
<li>平移</li>
<li>旋转、翻转（常结合考察）</li>
</ul>
<h5 id="考点一平移">考点一：平移</h5>
<ol type="1">
<li><p>方向：直线（上下、左右、对角线）、绕圈（顺/逆时针）</p>
<p><img src="/archives/487a2936/image-20200727210259617.png" alt="image-20200727210259617" style="zoom:67%;"></p></li>
<li><p>步数：恒定、递增（等差）、周期（考的少）</p></li>
</ol>
<h6 id="宫格形黑块平移">宫格形黑块平移</h6>
<ol type="1">
<li>个别黑块重合
<ul>
<li>题干和选项大部分元素组成完全一致，个别一两副图少黑块</li>
<li>题干第一幅图的黑块一般不会重合</li>
</ul></li>
</ol>
<p><img src="/archives/487a2936/image-20200727210936304.png" alt="image-20200727210936304" style="zoom:67%;"></p>
<ol start="2" type="1">
<li><p>黑块走到头后</p>
<ul>
<li><p>循环走：从头开始</p>
<p><img src="/archives/487a2936/image-20200727211118319.png" alt="image-20200727211118319" style="zoom:67%;"></p></li>
<li><p>折返走：直接弹回</p>
<p><img src="/archives/487a2936/image-20200727211135729.png" alt="image-20200727211135729" style="zoom:67%;"></p></li>
<li><p>“双胞胎”黑块如何分辨：就近走原则</p>
<p><img src="/archives/487a2936/image-20200727211331662.png" alt="image-20200727211331662" style="zoom:67%;"></p></li>
</ul></li>
</ol>
<h6 id="多宫格方向判定">多宫格方向判定</h6>
<p>题型特征：16宫格图形多个黑块平移</p>
<ol type="1">
<li><p>直线走：</p>
<ul>
<li>横行黑块数量相同（左右走）</li>
<li>竖行黑块数量相同（上下走）</li>
</ul>
<p><img src="/archives/487a2936/image-20200727211823656.png" alt="image-20200727211823656" style="zoom:67%;"></p></li>
<li><p>绕圈走：</p>
<ul>
<li>中间颜色数量相同，有限考虑内外圈分开看</li>
</ul>
<p><img src="/archives/487a2936/image-20200727212331968.png" alt="image-20200727212331968" style="zoom:67%;"></p></li>
</ol>
<h5 id="考点二旋转与翻转">考点二：旋转与翻转</h5>
<h6 id="旋转">旋转</h6>
<ol type="1">
<li>方向：顺时针、逆时针</li>
<li>常见角度：45°、60°、90°、180°等</li>
</ol>
<blockquote>
<p>TIPS：难题可以采用两两相邻比较</p>
<p>钟表类：麦面一个框，中间有一个点，饶了一圈线——常考旋转</p>
</blockquote>
<h6 id="翻转">翻转</h6>
<ol type="1">
<li>左右翻转</li>
<li>上下翻转（可能存在视觉误差，需要警觉）</li>
</ol>
<blockquote>
<p>TIPS：先看容易看懂的</p>
</blockquote>
<p>这道题目需要注意上下翻转，很难看出来（第二张图到第三张图）</p>
<p><img src="/archives/487a2936/image-20200727214018551.png" alt="image-20200727214018551" style="zoom:67%;"></p>
<h6 id="区分旋转和翻转">区分旋转和翻转</h6>
<ul>
<li><p>只有左右互换（上下不变）——左右翻</p>
<p><img src="/archives/487a2936/image-20200727214319546.png" alt="image-20200727214319546" style="zoom:67%;"></p></li>
<li><p>只有上下互换（左右不变）——上下翻</p>
<p><img src="/archives/487a2936/image-20200727214353319.png" alt="image-20200727214353319" style="zoom:67%;"></p></li>
<li><p>上下、左右都互换——旋转180°</p>
<p><img src="/archives/487a2936/image-20200727214429063.png" alt="image-20200727214429063" style="zoom:67%;"></p></li>
</ul>
<h3 id="样式规律">样式规律</h3>
<h4 id="特征-1">特征</h4>
<p>样式识别特征：元素组成相似</p>
<h4 id="考点-1">考点</h4>
<ol type="1">
<li>遍历</li>
<li>加减同异</li>
<li>黑白运算</li>
</ol>
<h5 id="考点一遍历">考点一：遍历</h5>
<p>图形特征：小元素重复出现</p>
<p>解题思路：缺啥补啥（遍历包括空白/阴影等很多方面）</p>
<p><img src="/archives/487a2936/image-20200727215015137.png" alt="image-20200727215015137" style="zoom:67%;"></p>
<h5 id="考点二加减同异"><strong>考点二：加减同异</strong></h5>
<p>识别特征：相同线条重复出现</p>
<ol type="1">
<li>相加、相减</li>
<li>求异（保留不同）</li>
<li>求同（保留相同）</li>
</ol>
<blockquote>
<p>对比选项，从特殊线条入手（横线、竖线、最长最短线）</p>
</blockquote>
]]></content>
      <categories>
        <category>公务员考试</category>
        <category>判断推理</category>
      </categories>
      <tags>
        <tag>国考</tag>
        <tag>行测</tag>
        <tag>判断推理</tag>
        <tag>图形推理</tag>
        <tag>类比推理</tag>
        <tag>定义判断</tag>
        <tag>逻辑判断</tag>
      </tags>
  </entry>
  <entry>
    <title>DOUBLESQUEEZE Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression</title>
    <url>/archives/493c0bc7.html</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>目前已有类似于QSGD和稀疏化SGD的通信优化算法，但参数服务器在实际应用中在收到工作节点量化梯度并聚合后，需要将聚合梯度从新分发给工作节点。本论文同时对工作节点和参数服务器梯度，采用误差补偿的方式进行梯度压缩。该算法有三大优势：</p>
<ol type="1">
<li>它兼容众多“粗暴”的压缩技术</li>
<li>它与没有误差补偿的压缩算法（例如QSGD和稀疏化SGD）相比，收敛性更好</li>
<li>达到了线性收敛</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(<span class="string">"111"</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>dfdfadf</p>
</blockquote>
<h2 id="背景介绍">背景介绍</h2>
<p>提高分布式机器学习性能的三个方向：</p>
<p>（1）高通信效率的学习</p>
<ul>
<li>QSGD: Communication-efficient SGD via gradient quantization and encoding<a href="#refer-anchor-1"><sup>1</sup></a>（量化为三元组表示）</li>
<li>signSGD: Compressed optimisation for non-convex problems<a href="#refer-anchor-2"><sup>2</sup></a></li>
<li>1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns<a href="#refer-anchor-3"><sup>3</sup></a>（提出一种误差补偿的量化方法）</li>
</ul>
<p>（2）去中心化学习；</p>
<ul>
<li>He L, Bian A, Jaggi M. Cola: Decentralized linear learning[C]//Advances in Neural Information Processing Systems. 2018: 4536-4546.</li>
<li>Lian X, Zhang C, Zhang H, et al. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent[C]//Advances in Neural Information Processing Systems. 2017: 5330-5340.</li>
</ul>
<p>（3）异步学习</p>
<ul>
<li>Agarwal A, Duchi J C. Distributed delayed stochastic optimization[C]//Advances in Neural Information Processing Systems. 2011: 873-881.</li>
<li>Lian X, Huang Y, Li Y, et al. Asynchronous parallel stochastic gradient for nonconvex optimization[C]//Advances in Neural Information Processing Systems. 2015: 2737-2745.</li>
<li>Recht B, Re C, Wright S, et al. Hogwild: A lock-free approach to parallelizing stochastic gradient descent[C]//Advances in neural information processing systems. 2011: 693-701.</li>
</ul>
<h3 id="量化压缩基本模型">量化压缩基本模型</h3>
<p>作者对分布式机器学习（特别是参数服务器架构）和量化压缩数学模型简单做了介绍</p>
<h4 id="分布式机器学习基本模型">分布式机器学习基本模型</h4>
<p><span class="math display">\[
\min _{\boldsymbol{x}} f(\boldsymbol{x})=\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}_{\boldsymbol{\zeta} \sim \mathcal{D}_{i}} F(\boldsymbol{x} ; \boldsymbol{\zeta})
\]</span></p>
<p>其中<span class="math inline">\(n\)</span>表示工作节点数量，<span class="math inline">\(\mathcal{D}_{i}\)</span>本地节点<span class="math inline">\(i\)</span>的数据分布，<span class="math inline">\(F(\boldsymbol{x} ; \boldsymbol{\zeta})\)</span>为本地损失函数。 <span class="math display">\[
\boldsymbol{g}^{(i)}=\nabla F\left(\boldsymbol{x} ; \boldsymbol{\zeta}^{(i)}\right)
\]</span> 各工作节点计算梯度 <span class="math display">\[
\boldsymbol{g}=\frac{1}{n} \sum_{i=1}^{n} \boldsymbol{g}^{(i)}
\]</span> 参数服务器对梯度进行聚合，以上是对分布式SGD算法的简单建模</p>
<h4 id="量化压缩">量化压缩</h4>
<p><span class="math inline">\(Q_{\omega}[\cdot]\)</span>代表压缩操作，以<span class="math inline">\(1Bits\)</span>方法为例，利用递归的方法更新压缩误差： <span class="math display">\[
\boldsymbol{\delta}^{(i)}=\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}-Q_{\omega}\left[\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}\right]
\]</span> 其中<span class="math inline">\(\left[\boldsymbol{g}^{(i)}+\boldsymbol{\delta}^{(i)}\right]\)</span>表示本轮计算得到的梯度<span class="math inline">\(g^{(i)}\)</span>和上一轮压缩误差<span class="math inline">\(\boldsymbol{\delta}^{(i)}\)</span>的和，上式子是对本轮量化误差的重新计算，这也是误差补偿的由来。</p>
<h4 id="主要贡献">主要贡献</h4>
<ol type="1">
<li>比其他没有错误补偿的压缩方法具有更好的收敛性</li>
<li>进一步优化了通信效率</li>
<li>第一次给出了误差补偿SGD相关算法的速率分析</li>
<li>在非凸情况下的加速证明</li>
</ol>
<h2 id="相关工作">相关工作</h2>
<h4 id="分布式学习">分布式学习</h4>
<h5 id="中心化并行训练">中心化并行训练</h5>
<h6 id="参数服务器架构">参数服务器架构</h6>
<ol type="1">
<li>Abadi M, Barham P, Chen J, et al. Tensorflow: A system for large-scale machine learning[C]//12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16). 2016: 265-283.</li>
<li>Li M, Andersen D G, Park J W, et al. Scaling distributed machine learning with the parameter server[C]//11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 2014: 583-598.</li>
</ol>
<h6 id="去中心化训练">去中心化训练</h6>
<ol type="1">
<li>固定拓扑
<ul>
<li>Jin P H, Yuan Q, Iandola F, et al. How to scale distributed deep learning?[J]. arXiv preprint arXiv:1611.04581, 2016.</li>
<li>Lian X, Zhang C, Zhang H, et al. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent[C]//Advances in Neural Information Processing Systems. 2017: 5330-5340.</li>
<li>Shen Z, Mokhtari A, Zhou T, et al. Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication[J]. arXiv preprint arXiv:1805.09969, 2018.</li>
<li>Tang H, Lian X, Yan M, et al. D <span class="math inline">\(^ 2\)</span>: Decentralized Training over Decentralized Data[J]. arXiv preprint arXiv:1803.07068, 2018.</li>
</ul></li>
<li>随机拓扑
<ul>
<li>Lian X, Zhang W, Zhang C, et al. Asynchronous decentralized parallel stochastic gradient descent[C]//International Conference on Machine Learning. 2018: 3043-3052.</li>
<li>Nedić A, Olshevsky A. Distributed optimization over time-varying directed graphs[J]. IEEE Transactions on Automatic Control, 2014, 60(3): 601-615.</li>
<li>Nedic A, Olshevsky A, Shi W. Achieving geometric convergence for distributed optimization over time-varying graphs[J]. SIAM Journal on Optimization, 2017, 27(4): 2597-2633.</li>
</ul></li>
</ol>
<h6 id="不同角度实现分布式训练">不同角度实现分布式训练</h6>
<ol type="1">
<li>隐私分布式优化
<ul>
<li>Jayaraman B, Wang L, Evans D, et al. Distributed learning without distress: Privacy-preserving empirical risk minimization[C]//Advances in Neural Information Processing Systems. 2018: 6343-6354.</li>
</ul></li>
<li>自适应分布式ADMM
<ul>
<li>Xu Z, Taylor G, Li H, et al. Adaptive consensus ADMM for distributed optimization[J]. arXiv preprint arXiv:1706.02869, 2017.</li>
</ul></li>
<li>非平滑分布式优化
<ul>
<li>Scaman K, Bach F, Bubeck S, et al. Optimal algorithms for non-smooth distributed optimization in networks[C]//Advances in Neural Information Processing Systems. 2018: 2740-2749.</li>
</ul></li>
<li>分布式近端原对称对偶算法
<ul>
<li>Hong M, Hajinezhad D, Zhao M M. Prox-PDA: The proximal primal-dual algorithm for fast distributed nonconvex optimization and learning over networks[C]//International Conference on Machine Learning. 2017: 1529-1538.</li>
</ul></li>
<li>投影-free的分布式在线学习
<ul>
<li>Zhang W, Zhao P, Zhu W, et al. Projection-free distributed online learning in networks[C]//International Conference on Machine Learning. 2017: 4054-4062.</li>
</ul></li>
<li>平行倒推
<ul>
<li>Huo Z, Gu B, Yang Q, et al. Decoupled parallel backpropagation with convergence guarantee[J]. arXiv preprint arXiv:1804.10574, 2018.</li>
</ul></li>
</ol>
<h5 id="压缩通信学习">压缩通信学习</h5>
<ol type="1">
<li><p>稀疏化模型</p>
<ul>
<li>Wang J, Kolar M, Srebro N, et al. Efficient distributed learning with sparsity[C]//International Conference on Machine Learning. 2017: 3636-3645.</li>
</ul></li>
<li><p>梯度量化</p>
<ul>
<li>Shen Z, Mokhtari A, Zhou T, et al. Towards more efficient stochastic decentralized learning: Faster convergence and sparse communication[J]. arXiv preprint arXiv:1805.09969, 2018.</li>
</ul>
<p>QSGD</p>
<ul>
<li>Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</li>
</ul>
<p>PCA压缩</p>
<ul>
<li>Garber D, Shamir O, Srebro N. Communication-efficient algorithms for distributed stochastic principal component analysis[J]. arXiv preprint arXiv:1702.08169, 2017.</li>
</ul>
<p><span class="math inline">\(1Bits\)</span>量化</p>
<ul>
<li>Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</li>
<li>Wen W, Xu C, Yan F, et al. Terngrad: Ternary gradients to reduce communication in distributed deep learning[C]//Advances in neural information processing systems. 2017: 1509-1519.</li>
</ul></li>
</ol>
<h5 id="错误补偿压缩">错误补偿压缩</h5>
<h6 id="bits量化"><span class="math inline">\(1Bits\)</span>量化</h6>
<ul>
<li>Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</li>
</ul>
<h6 id="二次优化">二次优化</h6>
<ul>
<li>Wu J, Huang W, Huang J, et al. Error compensated quantized SGD and its applications to large-scale distributed optimization[J]. arXiv preprint arXiv:1806.08054, 2018.</li>
</ul>
<h6 id="signsgd">SignSGD</h6>
<ul>
<li>Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed optimisation for non-convex problems[J]. arXiv preprint arXiv:1802.04434, 2018.</li>
<li>Alistarh D, Hoefler T, Johansson M, et al. The convergence of sparsified gradient methods[C]//Advances in Neural Information Processing Systems. 2018: 5973-5983.</li>
</ul>
<h2 id="算法介绍">算法介绍</h2>
<h3 id="算法描述">算法描述</h3>
<p>本文采用参数服务器架构描述该算法，但是算法的应用场景不仅限于参数服务器架构，在第<span class="math inline">\(t\)</span>次迭代，我们将该算法的关键步骤描述如下：</p>
<ul>
<li><strong>工作节点计算</strong></li>
</ul>
<p>每个节点<span class="math inline">\(i\)</span>计算本地随机梯度<span class="math inline">\(\nabla F\left(\boldsymbol{x}_{t} ; \boldsymbol{\zeta}_{t}^{(i)}\right)\)</span>，该梯度基于全局模型<span class="math inline">\(x_t\)</span>以及本地样本<span class="math inline">\(\boldsymbol{\zeta}_{t}^{(i)}\)</span>。这里的<span class="math inline">\(i\)</span>代表工作节点<span class="math inline">\(i\)</span>的索引，<span class="math inline">\(t\)</span>表示本轮的迭代次数</p>
<ul>
<li><strong>工作节点压缩</strong></li>
</ul>
<p>每个工作节点<span class="math inline">\(i\)</span>计算误差补偿随机梯度 <span class="math display">\[
\boldsymbol{\delta}_{t}^{(i)}=\boldsymbol{v}_{t}^{(i)}-Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]
\]</span> 其中<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>表示压缩误差补偿随机梯度</p>
<ul>
<li><strong>参数服务器压缩</strong></li>
</ul>
<p>所有节点将计算所得的<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>量化梯度发送给参数服务器，参数服务器聚合所有量化梯度<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>，并且更新全局误差补偿随机梯度<span class="math inline">\(v_t\)</span>，根据以下式子对梯度误差<span class="math inline">\(\boldsymbol{\delta}_{t}\)</span>进行更新 <span class="math display">\[
\begin{array}{l}
\boldsymbol{v}_{t}=\boldsymbol{\delta}_{t-1}+\frac{1}{n} \sum_{i=1}^{n} Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right] \\
\boldsymbol{\delta}_{t}=\boldsymbol{v}_{t}-Q_{\omega_{t}}\left[\boldsymbol{v}_{t}\right]
\end{array}
\]</span></p>
<ul>
<li><strong>工作节点更新</strong></li>
</ul>
<p>参数服务器将<span class="math inline">\(Q_{\omega_{t}^{(i)}}\left[\boldsymbol{v}_{t}^{(i)}\right]\)</span>发送给所有工作节点，所有工作节点更新本地模型： <span class="math display">\[
\boldsymbol{x}_{t+1}=\boldsymbol{x}_{t}-\gamma Q_{\omega_{t}}\left[\boldsymbol{v}_{t}\right]
\]</span> 其中<span class="math inline">\(\gamma\)</span>表示学习率</p>
<h3 id="压缩选择">压缩选择</h3>
<p>该方法不像当前存在的方法，并不需要无偏压缩的限制（也就是<span class="math inline">\(\mathbb{E}_{\omega} Q_{\omega}[\boldsymbol{x}]=\boldsymbol{x}\)</span>）。所以选择压缩的方法是非常灵活的。论文例举了多种较为常用的压缩选项：</p>
<h4 id="随机量化">随机量化</h4>
<p>对于任意真实值<span class="math inline">\(z \in[a, b]\)</span>，其中<span class="math inline">\((a,b)\)</span>是定义好的低bit数值，<span class="math inline">\(z\)</span>会有<span class="math inline">\(\frac{b-z}{b-a}\)</span>的概率被压缩到<span class="math inline">\(a\)</span>,有<span class="math inline">\(\frac{z-a}{b-a}\)</span>的概率压缩到<span class="math inline">\(b\)</span>。这种压缩操作是无偏的。</p>
<h4 id="bits量化-1"><span class="math inline">\(1Bits\)</span>量化</h4>
<p>将<span class="math inline">\(x\)</span>向量压缩到<span class="math inline">\(\|x\| \operatorname{sign}(x)\)</span>，其中<span class="math inline">\(sign(x)\)</span>是其中<span class="math inline">\(x\)</span>向量对应元素的符号。这种压缩是有偏的</p>
<h4 id="clipping">Clipping</h4>
<p>对于真实值<span class="math inline">\(z\)</span>，直接设置低于<span class="math inline">\(k\)</span>bis的部分压缩到<span class="math inline">\(0\)</span>。例如，将<span class="math inline">\(1.23456\)</span>压缩为d<span class="math inline">\(1.2\)</span>，直接将其较低的四位变成<span class="math inline">\(0\)</span>。这种压缩是有偏的。</p>
<h4 id="top-k稀疏化">Top-k稀疏化</h4>
<p>对于向量<span class="math inline">\(x\)</span>，将其最大的<span class="math inline">\(k\)</span>个元素进行保留，其余的设置为<span class="math inline">\(0\)</span>。这种操作是有偏的。</p>
<h4 id="随机稀疏化">随机稀疏化</h4>
<p>对于真实值<span class="math inline">\(z\)</span>，有<span class="math inline">\(p\)</span>的概率将<span class="math inline">\(z\)</span>设置为<span class="math inline">\(0\)</span>，以及<span class="math inline">\(p\)</span>的概率设置为<span class="math inline">\(z/p\)</span>。这样的方法是无偏的</p>
<h2 id="数学证明和收敛性分析">数学证明和收敛性分析</h2>
<p>待补充...</p>
<h2 id="实验">实验</h2>
<h3 id="实验设置">实验设置</h3>
<h4 id="数据集和模型">数据集和模型</h4>
<ul>
<li>ResNet-18以及CIFAR-10</li>
</ul>
<h4 id="实现对照组">实现对照组</h4>
<h5 id="doublesqueeze">DOUBLESQUEEZE</h5>
<h6 id="bit压缩"><span class="math inline">\(1-bit\)</span>压缩</h6>
<p>将梯度压缩到<span class="math inline">\(1-bit\)</span>，只包含符号。基于向量考虑，它的比例因子表示为： <span class="math display">\[
\frac{\text { magnitude of compensated gradient }}{\text { magnitude of quantized gradient }}
\]</span></p>
<h6 id="top-k压缩">Top-k压缩</h6>
<h5 id="qsgd">QSGD</h5>
<p>工作节点将梯度压缩成三元表示，其中每个元素用<span class="math inline">\(\{-1,0,1\}\)</span>表示。假设在这个梯度向量各个元素中的最大绝对值为<span class="math inline">\(m\)</span>，对于任意一个元素<span class="math inline">\(e\)</span>，它都以<span class="math inline">\(|e| /|m|\)</span>的可能性压缩到<span class="math inline">\(sign(e)\)</span>，以<span class="math inline">\(1-|e| /|m|\)</span>的可能性压缩到<span class="math inline">\(0\)</span>。扩展因子可以记为： <span class="math display">\[
\frac{\text { magnitude of compensated gradient }}{\text { magnitude of quantized gradient }}
\]</span> 采用这种方法时，参数服务器将梯度分发的时候不会讲梯度再次压缩</p>
<h5 id="vanilla-sgd">Vanilla SGD</h5>
<p>并不采用任何压缩处理</p>
<h5 id="mem-sgd">MEM-SGD</h5>
<p>和DEOUBLESQUEEZE的区别是从参数服务器进行分发的梯度不进行压缩，对于此种方法，本文也去使用了<span class="math inline">\(1-bit\)</span>二和<span class="math inline">\(top-k\)</span>这两中压缩方法。</p>
<h5 id="top-k-sgd">Top-k SGD</h5>
<p>该方法不涉及误差补偿机制</p>
<h3 id="实验结果">实验结果</h3>
<ol type="1">
<li>将<span class="math inline">\(1-bit\)</span>压缩作为DEUBLESQUEEZE的压缩方法，与MEM-SGD, QSGD这些压缩方法做对比</li>
</ol>
<p><img src="/archives/493c0bc7/image-20200724191830893.png" alt="image-20200724191830893" style="zoom:67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192014337.png" alt="image-20200724192014337" style="zoom:67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192043966.png" alt="image-20200724192043966" style="zoom:67%;"></p>
<ol start="2" type="1">
<li>将Top-k压缩作为DEUBLESQUEEZE的压缩方法，与MEM-SGD, QSGD这些压缩方法做对比</li>
</ol>
<p><img src="/archives/493c0bc7/image-20200724192139291.png" alt="image-20200724192139291" style="zoom: 67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192150444.png" alt="image-20200724192150444" style="zoom: 67%;"></p>
<p><img src="/archives/493c0bc7/image-20200724192202959.png" alt="image-20200724192202959" style="zoom: 67%;"></p>
<div id="refer-anchor-1">

</div>
<ul>
<li>[1] <a href="http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf" target="_blank" rel="noopener external nofollow noreferrer">Alistarh D, Grubic D, Li J, et al. QSGD: Communication-efficient SGD via gradient quantization and encoding[C]//Advances in Neural Information Processing Systems. 2017: 1709-1720.</a>
<div id="refer-anchor-2">

</div></li>
<li>[2] <a href="https://arxiv.org/pdf/1802.04434.pdf" target="_blank" rel="noopener external nofollow noreferrer">Bernstein J, Wang Y X, Azizzadenesheli K, et al. signSGD: Compressed Optimisation for Non-Convex Problems[C]//International Conference on Machine Learning. 2018: 560-569.</a>
<div id="refer-anchor-3">

</div></li>
<li>[3] <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/IS140694.pdf" target="_blank" rel="noopener external nofollow noreferrer">Seide F, Fu H, Droppo J, et al. 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns[C]//Fifteenth Annual Conference of the International Speech Communication Association. 2014.</a>
<div id="refer-anchor-4">

</div>
<div id="refer-anchor-5">

</div>
<div id="refer-anchor-6">

</div>
<div id="refer-anchor-7">

</div>
<div id="refer-anchor-8">

</div>
<div id="refer-anchor-9">

</div></li>
</ul>
]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>Kong和Keycloak第三方认证和权限管理</title>
    <url>/archives/fbb6b1b9.html</url>
    <content><![CDATA[<h2 id="介绍">介绍</h2>
<p>使用Keycloak作为第三方认证服务，Kong作为API网关与Keycloak进行对接，打通两者用户数据库，并使用Kong当中的ACL插件进行接口权限的设计。主要涉及到如下框架：</p>
<ul>
<li>Kong——开源API网关</li>
<li>Keycloak——一个OpenID认证服务</li>
<li>Konga——API网关后台管理可视化界面</li>
</ul>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-17.29.03-1595831384730.png" alt="Screen-Shot-2018-11-15-at-17.29.03"><figcaption>Screen-Shot-2018-11-15-at-17.29.03</figcaption>
</figure>
<p>下文将记录Kong、Konga和Keycloak三者的安装和对接过程，并介绍权限打通的设计思路。以下是主要步骤：</p>
<ol type="1">
<li>创建Dockerfile，创建带有kong-oidc插件的镜像</li>
<li>构建上述镜像</li>
<li>创建<code>docker-compose.yml</code>文件，配置Kong，Konga和Keycloak的相关信息</li>
<li>启动kong-db服务</li>
<li>运行<code>migrations</code></li>
<li>启动Kong服务</li>
<li>验证kong-oidc插件是否可用</li>
<li>使用StreamSet打通Keycloak和Kong用户数据库，进行数据同步</li>
<li>添加kong-oidc插件和ACL插件</li>
<li>测试认证是否可行</li>
</ol>
<h2 id="安装过程">安装过程</h2>
<h3 id="创建dockerfile文件">创建Dockerfile文件</h3>
<p>第一，我们需要创建有关于Kong的镜像。我们还需要在这个镜像的基础上安装kong-oidc插件。我们可以采用以下两种方法进行:</p>
<ol type="1">
<li>修改现有的、正在运行的容器并提交更改</li>
<li>创建Dockerfile文件，并镜像构建</li>
</ol>
<p>我们将采用第二种方法，以下是Dockerfile文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p docker/kong</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> touch docker/kong/Dockerfile</span></span><br></pre></td></tr></table></figure>
<p>使用vim创建并打开文件，写入以下内容：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> kong:<span class="number">1.4</span>.<span class="number">2</span>-centos</span><br><span class="line"></span><br><span class="line"><span class="keyword">LABEL</span><span class="bash"> description=<span class="string">"Centos 7 + Kong 1.4.2 + kong-oidc plugin"</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum install -y git unzip &amp;&amp; yum clean all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> luarocks install kong-oidc</span></span><br></pre></td></tr></table></figure>
<p>以上代码将会将安装kong1.4.2版本，以及在此基础上安装kong-oidc插件，接下来构建该文件：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker build -t kong:1.4.2-centos-oidc docker/kong/</span></span><br></pre></td></tr></table></figure>
<p>如果遇到<code>Warning: The directory '/root/.cache/luarocks' or its parent directory is not owned by the current user</code>就忽略。</p>
<h3 id="安装及配置kong">安装及配置Kong</h3>
<p>接下来创建<code>docker-compose.yml</code>文件</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> touch docker-compose.yml</span></span><br></pre></td></tr></table></figure>
<p>打开这个文件，并进行如下配置</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">version: <span class="string">'3.4'</span></span><br><span class="line"></span><br><span class="line">networks: </span><br><span class="line">  kong-net:</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  kong-datastore:</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  kong-db:</span><br><span class="line">    image: postgres:<span class="number">9.6</span></span><br><span class="line">    volumes:</span><br><span class="line">      - kong-datastore:/var/lib/postgresql/data</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"15432:5432"</span></span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_DB:       api-gw</span><br><span class="line">      POSTGRES_USER:     kong</span><br><span class="line">      POSTGRES_PASSWORD: kong</span><br><span class="line"></span><br><span class="line">  kong:</span><br><span class="line">    image: kong:<span class="number">1.4</span>.<span class="number">2</span>-centos-oidc</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-db</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8000:8000"</span> <span class="comment"># Listener</span></span><br><span class="line">      - <span class="string">"8001:8001"</span> <span class="comment"># Admin API</span></span><br><span class="line">      - <span class="string">"8443:8443"</span> <span class="comment"># Listener  (SSL)</span></span><br><span class="line">      - <span class="string">"8444:8444"</span> <span class="comment"># Admin API (SSL)</span></span><br><span class="line">    environment:</span><br><span class="line">      KONG_DATABASE:         postgres</span><br><span class="line">      KONG_PG_HOST:          kong-db</span><br><span class="line">      KONG_PG_PORT:          <span class="number">5432</span></span><br><span class="line">      KONG_PG_DATABASE:      api-gw</span><br><span class="line">      KONG_PROXY_ACCESS_LOG: /dev/stdout</span><br><span class="line">      KONG_ADMIN_ACCESS_LOG: /dev/stdout</span><br><span class="line">      KONG_PROXY_ERROR_LOG:  /dev/stderr</span><br><span class="line">      KONG_ADMIN_ERROR_LOG:  /dev/stderr</span><br><span class="line">      KONG_PROXY_LISTEN:     <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8000</span>, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8443</span> ssl</span><br><span class="line">      KONG_ADMIN_LISTEN:     <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8001</span>, <span class="number">0.0</span>.<span class="number">0.0</span>:<span class="number">8444</span> ssl</span><br><span class="line">      KONG_PLUGINS:          bundled,oidc</span><br></pre></td></tr></table></figure>
<p>接下来，采用下面的命令启动kong-db服务，其中<code>-d</code>是告诉docker在后台运行Docker Compose进程</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d kong-db</span></span><br></pre></td></tr></table></figure>
<p>验证服务是否已经启动</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>接下来将迁移kong-db数据库，采用<code>migrations</code>命令。下面这个命令将驱动一个kong服务，<code>-rm</code>表示该服务将在命令运行之后关闭。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose run --rm kong kong migrations up</span></span><br></pre></td></tr></table></figure>
<p>最后，我们将Kong启动，并且检查其运行状态</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d kong</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>检查Kong的管理API，检查OIDC插件是否可以在服务器上使用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://localhost:8001 | jq .plugins.available_on_server.oidc</span></span><br></pre></td></tr></table></figure>
<p>将返回<code>true</code>。但是，虽然OIDC插件在Kong上可用，但是还没有配置Keycloak，实际上还不能进行API的保护。</p>
<p>至此为止，就完成了Kong的安装</p>
<hr>
<h3 id="安装konga可视化终端">安装Konga可视化终端</h3>
<p>Konga是开源的Kong可视化界面，方面API网关管理人员对Kong进行可视化管理，安装方法和Kong类似。</p>
<p>配置<code>docker-compose.yml</code>文件，添加如下内容：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">konga-prepare:</span><br><span class="line">     image: pantsel/konga:next</span><br><span class="line">     command: <span class="string">"-c prepare -a postgres -u postgresql://kong:kong@kong-db:5432/konga_db"</span></span><br><span class="line">     networks:</span><br><span class="line">       - kong-net</span><br><span class="line">     restart: on-failure</span><br><span class="line">     links:</span><br><span class="line">       - kong-db</span><br><span class="line">     depends_on:</span><br><span class="line">       - kong-db</span><br></pre></td></tr></table></figure>
<p>首先将konga的postgresql数据启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d konga-prepare</span></span><br></pre></td></tr></table></figure>
<p>检查是否启动成功：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>接下来在<code>docker-compose.yml</code>再配置Konga镜像：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">konga:</span><br><span class="line">    image: pantsel/konga:latest</span><br><span class="line">    networks:</span><br><span class="line">      - kong-net</span><br><span class="line">    environment:</span><br><span class="line">      DB_ADAPTER: postgres</span><br><span class="line">      DB_HOST: kong-db</span><br><span class="line">      DB_USER: kong</span><br><span class="line">      DB_DATABASE: konga_db</span><br><span class="line">      NODE_ENV: production</span><br><span class="line">      DB_PASSWORD: kong</span><br><span class="line">    depends_on: </span><br><span class="line">      - kong-db</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"1337:1337"</span></span><br></pre></td></tr></table></figure>
<p>再次启动konga</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d konga</span></span><br></pre></td></tr></table></figure>
<p>检查是否启动成功：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>用浏览器访问“IP地址:1337”端口，注册后台管理用户名密码，检查是否能够正常访问，进入后台管理界面后，配置和Kong的对接：</p>
<p><img src="/archives/fbb6b1b9/image-20200727141443389.png" alt="image-20200727141443389" style="zoom:67%;"></p>
<p>对接完成后，可以用Konga查看并配置Kong网关的信息了。</p>
<h3 id="安装keycloak">安装Keycloak</h3>
<p>本节欸将重点配置Keycloak的安装。我们还是采用docker对Keycloak进行安装。</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-19.38.47.png" alt="Screen-Shot-2018-11-15-at-19.38.47"><figcaption>Screen-Shot-2018-11-15-at-19.38.47</figcaption>
</figure>
<p>在本节中，我们将运行以下步骤：</p>
<ol type="1">
<li>修改<code>docker-compose.yml</code>来添加Keycloak和它的可视化终端服务</li>
<li>注册Keycloak数据库服务</li>
<li>注册Keycloak服务</li>
<li>登录Keycloak</li>
<li>为Kong添加Keycloak终端</li>
<li>添加新用户</li>
</ol>
<h4 id="使用docker安装keycloak">使用Docker安装Keycloak</h4>
<p>重新打开<code>docker-compose.yml</code>.为Keycloak添加网络：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">networks: </span><br><span class="line">  kong-net:</span><br><span class="line">  keycloak-net:</span><br><span class="line"></span><br><span class="line">volumes:</span><br><span class="line">  kong-datastore:</span><br><span class="line">  keycloak-datastore:</span><br></pre></td></tr></table></figure>
<p>接下来将Keycloak数据库添加到<code>docker-compose.yml</code>中：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">services:</span><br><span class="line">  ...</span><br><span class="line">  keycloak-db:</span><br><span class="line">    image: postgres:<span class="number">9.6</span></span><br><span class="line">    volumes: </span><br><span class="line">      - keycloak-datastore:/var/lib/postresql/data</span><br><span class="line">    networks:</span><br><span class="line">      - keycloak-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"25432:5432"</span></span><br><span class="line">    environment:</span><br><span class="line">      POSTGRES_DB:       keycloak</span><br><span class="line">      POSTGRES_USER:     keycloak</span><br><span class="line">      POSTGRES_PASSWORD: password</span><br></pre></td></tr></table></figure>
<p>启动服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d keycloak-db</span></span><br></pre></td></tr></table></figure>
<p>验证是否可以使用（确保它的状态是启动的）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<p>下一步，将Keycloak添加到<code>docker-compose.yml</code>的services中：</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line">services:</span><br><span class="line">  ...</span><br><span class="line">  keycloak:</span><br><span class="line">    image: jboss/keycloak:<span class="number">4.5</span>.<span class="number">0</span>.Final</span><br><span class="line">    depends_on:</span><br><span class="line">      - keycloak-db</span><br><span class="line">    networks:</span><br><span class="line">      - keycloak-net</span><br><span class="line">    ports:</span><br><span class="line">      - <span class="string">"8180:8080"</span></span><br><span class="line">    environment:</span><br><span class="line">      DB_VENDOR:   POSTGRES</span><br><span class="line">      DB_ADDR:     keycloak-db</span><br><span class="line">      DB_PORT:     <span class="number">5432</span></span><br><span class="line">      DB_DATABASE: keycloak</span><br><span class="line">      DB_USER:     keycloak</span><br><span class="line">      DB_PASSWORD: password</span><br><span class="line">      KEYCLOAK_USER:     admin</span><br><span class="line">      KEYCLOAK_PASSWORD: admin</span><br></pre></td></tr></table></figure>
<p>最后启动Keycloak服务：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose up -d keycloak</span></span><br></pre></td></tr></table></figure>
<p>验证服务是否可用</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker-compose ps</span></span><br></pre></td></tr></table></figure>
<h4 id="添加客户端到keycloak中">添加客户端到Keycloak中</h4>
<p>点击Administration Console进入到控制台当中，此时需要输入用户名和密码：</p>
<p>在<code>Master</code>域下，创建客户端：</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.38.29.png" alt="Screen-Shot-2018-11-15-at-20.38.29"><figcaption>Screen-Shot-2018-11-15-at-20.38.29</figcaption>
</figure>
<p>在添加客户端页面，填写“客户端ID”为kong，点击保存按钮。</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.43.23.png" alt="Screen-Shot-2018-11-15-at-20.43.23"><figcaption>Screen-Shot-2018-11-15-at-20.43.23</figcaption>
</figure>
<p>在详情页面中，可以看到“Access Type”，我们需要选择的是“Confidential”，其中“Root URL”是“IP地址:8000”（Kong所接管的端口），以及“Valid Redirect URIs”填写“/*”就可以了，我们的设置如下：</p>
<figure>
<img src="/archives/fbb6b1b9/image-20200727135258550.png" alt="image-20200727135258550"><figcaption>image-20200727135258550</figcaption>
</figure>
<p>点击保存之后，在“Credentials”页面中会有对应的“Secret”，这个密钥是需要在Kong当中进行配置的</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.52.51.png" alt="Screen-Shot-2018-11-15-at-20.52.51"><figcaption>Screen-Shot-2018-11-15-at-20.52.51</figcaption>
</figure>
<h4 id="为keycloak添加用户">为Keycloak添加用户</h4>
<p>要添加用户，单击左侧侧边栏的“Users”选项卡，然后单击右侧的“add user”按钮</p>
<figure>
<img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-20.58.07.png" alt="Screen-Shot-2018-11-15-at-20.58.07"><figcaption>Screen-Shot-2018-11-15-at-20.58.07</figcaption>
</figure>
<p>在下一页，将“Username”设置为“用户”，并将“Email Enable”开关设置为“On”。然后，点击“Save”按钮。</p>
<p><img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-21.00.07.png" alt="Screen-Shot-2018-11-15-at-21.00.07" style="zoom:67%;"></p>
<p>点击“Credentials”选项卡，输入密码，确认，确保“Temporary”开关设置为“关闭”。然后，点击“Save”按钮。</p>
<p>至此，在Keycloak当中的配置已经完成了。</p>
<hr>
<h3 id="将kong和keycloak进行对接">将Kong和Keycloak进行对接</h3>
<p>接下来将配置Keycloak和Kong的对接部分</p>
<p><img src="/archives/fbb6b1b9/Screen-Shot-2018-11-15-at-21.06.44.png" alt="Screen-Shot-2018-11-15-at-21.06.44" style="zoom:67%;"></p>
<ol type="1">
<li><p>创建服务和路由（此处省略）</p></li>
<li><p>在全局范围内安装oidc插件</p>
<p>点击左侧“Plugin”后，再点击“ADD GLOBAL PLUGINS”，在Other当中，又OIDC插件，找到并点击“ADD PLUGIN”，会弹出如下表单，表单当中的条目信息可以参考：https://github.com/nokia/kong-oidc在GitHub上的首页。</p>
<p>其中以下几个需要重点关注：</p>
<ul>
<li>client_id：需要和Keycloak中创建的client_id对应一致</li>
<li>client_secret：需要需要和Keycloak中自动生成的secret一致</li>
<li>realm：默认就是Keycloak中的master域</li>
<li>redirect after logout uri：/</li>
<li>discovery：根据OIDC填写：http://XXXXXX:8180/auth/realms/master/.well-known/openid-configuration</li>
</ul></li>
<li><p>配置完成后，访问Kong接管的API接口，会自动跳转到Keycloak的登录界面，说明Keycloak和Kong对接完成</p>
<p><img src="/archives/fbb6b1b9/image-20200727142400435.png" alt="image-20200727142400435" style="zoom:67%;"></p></li>
</ol>
<h2 id="存在问题">存在问题</h2>
<p>官方给出了以上配置虽然打通了Kong和Keycloak，但是由于登录是Keycloak进行管理，采用的是Keycloak中的用户，而这些用户和Kong当中的“Consumer”是分离的，而且此时不能够通过Keycloak进行用户权限的管理。</p>
<p>如果将用户管理交给Keycloak，就会架空Kong当中的Consumer，最好是有办法将Kong中的Consumer和Keycloak中的User打通</p>
<h3 id="目标">目标</h3>
<ul>
<li>打通Keycloak和Kong中的用户表</li>
<li>能够使得ACL插件进行白名单和黑名单的访问控制</li>
</ul>
<h3 id="方法">方法</h3>
<ol type="1">
<li>使用ETL工具两者数据表，将ID和用户名进行同步</li>
<li>修改ACL插件使其能够获取到认证过后的<code>authenticated_consumer</code></li>
</ol>
<h3 id="操作步骤">操作步骤</h3>
<h4 id="etl工具同步">ETL工具同步</h4>
<p>此处过程省略</p>
<h4 id="修改acl插件">修改ACL插件</h4>
<p>在kong-oidc插件官方文档中，给出了<code>X-Userinfo</code>是被注入在请求头当中的，例如：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">X-Userinfo: &#123;"preferred_username":"alice","id":"60f65308-3510-40ca-83f0-e9c0151cc680","sub":"60f65308-3510-40ca-83f0-e9c0151cc680"&#125;</span><br></pre></td></tr></table></figure>
<p>而该插件同样在<code>ngx.ctx.authenticated_consumer</code>中设置了变量，他能够支持其他插件对认证通过用户进行操作，因此，可以让ACL获取该信息，利用该信息和现有“Consumer”做匹配。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">ngx.ctx.authenticated_credential = &#123;</span><br><span class="line">    id = "60f65308-3510-40ca-83f0-e9c0151cc680",   -- sub field from Userinfo</span><br><span class="line">    username = "alice"                             -- preferred_username from Userinfo</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改如下：</p>
<ul>
<li><p>进入docker容器当中</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo docker <span class="built_in">exec</span> -it  容器ID /bin/bash</span></span><br></pre></td></tr></table></figure></li>
<li><p>进入Kong插件所在目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/share/lua/5.1/kong/plugins/acl</span></span><br></pre></td></tr></table></figure></li>
<li><p>打开group.lua文件，对<code>get_current_consumer_id</code>做修改</p>
<figure class="highlight diff"><table><tr><td class="code"><pre><span class="line">local function get_current_consumer_id()</span><br><span class="line">  kong.log.err("oidc info:", ngx.ctx.authenticated_credential.id)</span><br><span class="line">  return (kong.client.get_consumer() or EMPTY).id or</span><br><span class="line">         (kong.client.get_credential() or EMPTY).consumer_id or</span><br><span class="line"><span class="addition">+        (ngx.ctx.authenticated_credential or EMPTY).id</span></span><br></pre></td></tr></table></figure></li>
</ul>
<p>这样就可以做到Kong和Keycloak真正打通。</p>
]]></content>
      <categories>
        <category>API网关</category>
        <category>Kong&amp;Konga</category>
        <category>权限认证</category>
      </categories>
      <tags>
        <tag>Kong</tag>
        <tag>keycloak</tag>
        <tag>权限认证</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>1 Bits SGD and its Application to Data Parallel Distributed Training of Speach DNNs</title>
    <url>/archives/1b1e994e.html</url>
    <content><![CDATA[<h2 id="摘要">摘要</h2>
<p>本文主要提出了一种比较激进的梯度量化方法，它可以将梯度中的每一位都量化为1bit，而且每次的量化误差都会补偿进下一轮量化梯度当中，因此称为带有误差补偿的1bit梯度量化方法。</p>
<blockquote>
<p>值得一提的是，本文是将量化引入分布式机器学习的开山之作，同时，这种非常激进（因为量化为1bit）的量化方式，极大减小了通信成本，而且误差补偿的方法非常具有启发性。</p>
</blockquote>
<p>本文主要针对并行SGD的AdaGrad、自动最小批量大小选择，双缓冲机制以及模型并行等多个方面进行探究。在实验上证实了量化和AdaGrad相结合会有一定的准确率提升。</p>
<h2 id="数据并行的确定分布式sgd训练">数据并行的确定分布式SGD训练</h2>
<p>错误的反向传播方法是训练DNN模型常用的方法，它形成了一种随机梯度下降算法。随机梯度下降算法通用形式如下： <span class="math display">\[
\begin{aligned}
\lambda(t+N) &amp;=\lambda(t)+\epsilon(t) \cdot G(t) \\
G(t) &amp;=\left.\sum_{\tau=t}^{t+N-1} \frac{\partial \mathcal{F}_{\lambda}(o(\tau))}{\partial \lambda}\right|_{\lambda=\lambda(t)}
\end{aligned}
\]</span> 其中<span class="math inline">\(\lambda(t)\)</span>代表当前索引<span class="math inline">\(t\)</span>采样下的模型，它的增长步长是<span class="math inline">\(N\)</span>，也就是数据采样的批量大小为<span class="math inline">\(N\)</span>。<span class="math inline">\(\mathcal{F}_{\lambda}\)</span>是样本向量为<span class="math inline">\(o(\tau)\)</span>的函数的部分梯度。<span class="math inline">\(\epsilon(t)\)</span>表示学习率。</p>
<h3 id="数据并行的分布式sgd">数据并行的分布式SGD</h3>
<p>对于上述数据并行训练来说，最佳节点数量<span class="math inline">\(\hat{K}\)</span>能够使得节点中计算和数据通信完全重叠，此时可以保证通信和计算资源是饱和的，也就是能够达到并行的最优化： <span class="math display">\[
T_{\text {calc }}(\hat{K})=T_{\text {comm }}(\hat{K})
\]</span> <span class="math inline">\(T_{\text {calc }}\)</span>和<span class="math inline">\(T_{\text {comm }}\)</span>分别是节点上每个小批量所需要的的计算和通信时间。如果我们将计算和通信开销继续细分，我们就能够求出最优节点数量 <span class="math display">\[
\hat{K}=\frac{N / 2 \cdot T_{\mathrm{calc}}^{\mathrm{frm}}+C \cdot T_{\mathrm{calc}}^{\mathrm{post}}}{\frac{1}{Z} \cdot T_{\mathrm{comm}}^{\mathrm{float}}-T_{\mathrm{calc}}^{\mathrm{udd}}}
\]</span> <img src="/archives/1b1e994e/image-20200731151547791.png" alt="image-20200731151547791" style="zoom:67%;"></p>
<p>其中：</p>
<p>事实上，将每个维度精度为<span class="math inline">\(Z=32\)</span>的数据压缩为1bit</p>
<ul>
<li><p><span class="math inline">\(T_{\mathrm{calc}}^{\mathrm{frm}}\)</span>表示处理数据的时间，这部分是可以并行的</p>
<p>与参数M的关系：<span class="math inline">\(\propto \frac{M}{\text { FLOPS }}\)</span></p></li>
<li><p><span class="math inline">\(T_{\mathrm{calc}}^{\mathrm{post}}\)</span>表示对梯度后处理的时间，比如momentum+AdaGrad</p>
<p>与参数M的关系：<span class="math inline">\(\propto \frac{M}{\text { RAM bindwidth }}\)</span></p></li>
<li><p><span class="math inline">\(T_{\mathrm{comm}}^{\mathrm{float}}\)</span>传输梯度（由单精度浮点数表示）的时间</p>
<p>与参数M的关系：<span class="math inline">\(\propto \frac{M}{\text { Network bindwidth }}\)</span></p></li>
<li><p><span class="math inline">\(T_{\mathrm{calc}}^{\mathrm{udd}}\)</span>模型更新时间，相对于<span class="math inline">\(K\)</span>来说是固定的</p>
<p>与参数M的关系：<span class="math inline">\(\propto \frac{M}{\text { RAM bindwidth }}\)</span></p></li>
</ul>
<h3 id="双缓冲机制">双缓冲机制</h3>
<p>为了实现更高的并行度，作者提出了双缓冲（double buffering）的概念，即在每个节点上把一个小批量分成两部分在交换其中一部分的梯度时进行另一部分的计算。然而，双缓冲机制会引入额外的更新开销（该开销是进行梯度聚合过程中产生的），当通信开销小于更新开销（即<span class="math inline">\(T_{\text {comm}}^{\text {flat}}&lt;T_{\text {calc}}^{\text {upd}}\)</span>）公式不在成立，双缓冲机制失去作用。</p>
<blockquote>
<p>双缓冲机制的方式很好，可以用于参数服务器架构，使得通信和梯度计算之间过程的重叠，有助于更好地并行。</p>
</blockquote>
<h3 id="误差补偿的1bit量化">误差补偿的1Bit量化</h3>
<p>本文最核心的部分就是减小在数据并行过程中，节点之间进行梯度传输过程中所需要的带宽消耗。（1）本文中的数据并行默认只交换荼毒，并不交换模型；（2）在数据交换的时候对梯度进行量化。</p>
<p>为了降低量化误差带来的负面影响，作者使用了误差补偿技术：每次量化时，把上一次迭代的量化误差加到本次迭代的梯度上，然后再进行量化，接着求出本次量化操作的误差。这种误差补偿机制可以确保所有的梯度都会再一定程度上对模型更新产生作用，只不过这种作用分散在不同的迭代中——类似于一种延迟更新的形式。作者指出，使用误差补偿后，就可以在几乎不损失模型精度的情况下将梯度由32位量化成1位。 <span class="math display">\[
\begin{aligned}
G_{i j \ell}^{\text {quant }}(t) &amp;=\mathcal{Q}\left(G_{i j \ell}(t)+\Delta_{i j \ell}(t-N)\right) \\
\Delta_{i j \ell}(t) &amp;=G_{i j \ell}(t)-\mathcal{Q}^{-1}\left(G_{i j \ell}^{\text {quant }}(t)\right)
\end{aligned}
\]</span> 其中<span class="math inline">\(\mathcal{Q}(\cdot)\)</span>表示量化函数，<span class="math inline">\(G_{i j \ell}^{\text {quant }}(t)\)</span>表示量化之后的整型数值。我们在量化过程中会保证<span class="math inline">\(\Delta_{i j \ell}(t)\)</span>被加到下一轮的梯度过程中（也称为了误差补偿机制）。</p>
<p>举个例子，在具体的实现上，比较简单的方法是将大于<span class="math inline">\(0\)</span>的梯度值编码成为<span class="math inline">\(1\)</span>，小于等于<span class="math inline">\(0\)</span>的梯度值编码为<span class="math inline">\(0\)</span>。在解码的时候，将<span class="math inline">\(1\)</span>编码为<span class="math inline">\(+1\)</span>，将<span class="math inline">\(0\)</span>解码为<span class="math inline">\(-1\)</span>，在进行聚合操作。</p>
<h3 id="系统描述">系统描述</h3>
<p>作者从最优节点数的公式当中总结出如下提高并行度的方法：</p>
<ol type="1">
<li>增加<span class="math inline">\(N\)</span>，也就是尽可能使得小批量的规模更大</li>
<li>增加<span class="math inline">\(Z\)</span>，尽可能大得压缩通信数据量</li>
<li>减少固定消耗<span class="math inline">\(T_{\mathrm{calc}}^{\mathrm{udd}}\)</span></li>
</ol>
]]></content>
      <categories>
        <category>科研</category>
        <category>分布式机器学习</category>
        <category>通信优化</category>
        <category>梯度压缩</category>
      </categories>
      <tags>
        <tag>分布式机器学习</tag>
        <tag>通信优化</tag>
        <tag>梯度压缩</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/archives/4a17b156.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener external nofollow noreferrer">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener external nofollow noreferrer">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener external nofollow noreferrer">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener external nofollow noreferrer">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener external nofollow noreferrer">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener external nofollow noreferrer">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener external nofollow noreferrer">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener external nofollow noreferrer">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>test</title>
    <url>/archives/d87f7e0c.html</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">请输入密码</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="5c31251553eaba323c3ef346aadd1c63f2f0bc339a1ee60ff0cc34731a2c43f4">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>你好</category>
      </categories>
      <tags>
        <tag>hello</tag>
      </tags>
  </entry>
</search>
