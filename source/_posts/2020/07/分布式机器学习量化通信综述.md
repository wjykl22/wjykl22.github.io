---
title: 分布式机器学习量化通信综述
mathjax: true
abbrlink: 88211b35
date: 2020-07-31 16:03:49
top: 10
password: wjykl22
abstract: 此文加密，请输入密码
message: 请输入密码
tags:
	- 分布式机器学习
	- 通信优化
	- 梯度压缩
categories:
	- 科研
	- 分布式机器学习
	- 通信优化
	- 梯度压缩

---

# 摘要

随着物联网技术和边缘计算的发展，人工智能的应用场景不断朝着去中心化、边缘化和异构化的方向发展，使得分布式机器学习的设计需要适应异构环境，满足边缘移动环境设备性能较弱，网络通信较差的情况。因此越来越多的研究开始对分布式机器学习以及联邦学习的通信进行优化，优化的方向主要可以分为以下三个方向：（1）优化分布式拓扑结构和通信步调；（2）减少通信带宽消耗以及（3）减少通信次数。
		本文将主要针对第二个方向：从减少通信带宽消耗角度，对现有研究中通信量化的相关技术做了梳理，主要涉及梯度量化压缩和稀疏化这两大方面，并总结各类方法在研究和实际应用过程中所遇到的机遇和挑战。

# 背景介绍

## 分布式学习

随着机器学习和深度学习模型复杂程度和规模的扩大，单一部件和单一节点以无法满足机器学习算法的最低要求，我们以最简单的最优化损失函数为例：
$$
\min_w\frac{1}{n}\sum_{i=1}^n{\mathbb{E}_{\boldsymbol{\zeta }\sim \mathcal{D}_i}F_i}\left( w;\boldsymbol{\zeta } \right) \
$$
其中，$F\left( \cdot \right) $是以$w$作为模型参数的损失函数，$n$为节点数量，$i$是节点的索引，$\mathcal{D}_i$是本地节点$i$的数据分布。以大规模的深度神经网络为例，学习过程可能会遇到如下问题：（1）$\mathcal{D}$数据量过大，单一节点无法存储，因此$\mathcal{D}_i$的分布式存储是解决方式之一；（2）$w$过于复杂，模型无法在单一节点进行存储，因此需要对模型进行划分，可以采用模型并行的方式进行分布式训练；（3）训练过程中速度较慢，单节点训练复杂模型可能需要几周的时间，因此也需要集合多节点的算力资源，实现算法的并行化。

机器学习模型的分布式训练各个节点之间需要进行通讯，通信的拓扑结构对训练的精度和性能会产生非常大的影响，目前主流拓扑结构主要分为：MapReduce、AllReduce、参数服务器、数据流以及去中心化架构等，下文将对这几种拓扑模型做简单介绍。

### MapReduce

### AllReduce

### 参数服务器架构

### 去中心化架构



## 通信优化







## 通信压缩

在参数服务器架构中，参数服务器和工作节点之间需要进行频繁通信，但是在带宽受限的边缘环境中，无法负担深度神经网络训练过程中庞大的梯度和带宽通信和传输，现在考虑以下问题：
$$
\min_{\boldsymbol{\theta }\in \mathbb{R}^d}\mathcal{L}\left( \boldsymbol{\theta } \right)
$$
其中
$$
\mathcal{L}\left( \boldsymbol{\theta } \right) :=\sum_{m\in \mathcal{M}}{\mathcal{L}_m}\left( \boldsymbol{\theta } \right)
$$
上式中$\theta \in \mathbb{R}^{d}$为需要通过学习得到的参数向量，$\mathcal{L}$和$\mathcal{L} \text { and }\left\{\mathcal{L}_{m}, m \in \mathcal{M}\right\}$是损$\mathcal{M}:=\{1, \ldots, M\}$个工作节点上的损失函数。

以参数服务器下数据并行的SGD算法第$k$轮迭代为例：（1）每个工作节点计算利用本地采样得到的样本$\mathcal{D}_i$计算损失函数的梯度，第$m$个节点计算所得梯度记为$\nabla \mathcal{L}_{m}\left(\boldsymbol{\theta}^{k}\right)$；（2）当所有工作节点计算完成后，需要将所有本地梯度上传参数服务器，通过梯度聚合算法计算得到全局聚合梯度$\nabla_{\mathrm{GD}}^{k}$，在此过程中会涉及到$m$次通信局部梯度$\nabla_{m}^{k}$的过程，假设梯度总共有$p$维，每个维度通常采用$32$位浮点数表示，此次通信量为$\sum_{m\in \mathcal{M}}{\nabla _{m}^{k}}:=32mp$bits。（3）参数服务器收到来自工作节点梯度后，将会将其聚合，获得全局梯度$\nabla_{\mathrm{GD}}^{k}:=\sum_{m \in \mathcal{M}} \nabla \mathcal{L}_{m}\left(\theta^{k}\right)$；（4）参数服务器将全局梯度重新发送给工作节点，此过程会涉及到模型权重参数的通信，假设模型大小为$B(\theta)$bits，由于存在$m$个工作节点，因此通信量为$mB(\theta)$bits；最后当工作节点接收到模型权重后，从头开始新一轮迭代。

从上述过程我们不难发现，一次迭代所需要的通信量为$Com:=(\sum_{m\in \mathcal{M}}{\nabla _{m}^{k}}+mB(\theta))$bits，它会随着模型和工作节点数量的增加而呈现线性增长。为了边缘环境下通信带宽的问题，我们可以采用量化压缩或者稀疏化的方式，对模型权重、激活层输出和梯度进行量化压缩或者稀疏化操作，记为：$Q(\cdot )$，目的是为了使得$Q\left( Com \right) \ll Com$。

到目前为止，大量学者对$Q(\cdot )$这一操作做了非常大量的研究，主要可以分成量化和稀疏化两类，量化主要通过压缩和编码的方式对梯度、权重和激活层进行降低精度；而稀疏化的方式则是将矩阵的部分维度置为0，从而降低整个矩阵向量的存储大小，下文将对这两种方式的研究做详细介绍。

### 量化压缩

目前，很多技术用在了量化神经网络方面。粗略地来看可以分成确定性量化和随机量化。在确定量化中，在量化值和真实值之间有一一对应的映射，而随机量化权重，激活和梯度则是离散分布。量化值是从离散分布中采样得到的。

在神经网络中有三个部分是可以进行量化的：权重、激活和梯度。量化这些部分的动机和方法是不同的。量化权重和激活层，我们得到更小的模型尺寸。在分布式训练的花镜中，我们能够通过量化梯度的方式节省通信消耗。一般来说，量化梯度比量化权重和激活更加困难，因为训练往往需要精度更高的梯度来保证算法的收敛。

我们通常采用编码本（codebook）来表示代表真实值的离散值。从密码本的表示来看，现有的工作可以将量化神经网络粗略的分成两类：固定编码本量化和自适应编码本量化。

在固定编码本量化中，权重经常被量化成提前定义好的编码，二自适应编码本是从数据中学习而来。一些普遍应用的密码本包括$\{-1,1\}$，$\{-1,0,1\}$或者二数幂或者二进制网络和三元权重网络等。

训练量化模型需要不断调整，而且量化网络并不容易理解，寻找新的量化方法以及配合理论分析是量化神经网络非常重要的一点。

#### 确定性量化

> 下文整理主要来自于综述[<sup>1</sup>](\#refer-anchor-1)

##### 舍入法（Rounding）

###### 主要内容

取整是对实际值最为基础的量化方式，例如[Courbariaux et al., 2015]提出了下面这种取整方法：
$$
Q_b(x)=\operatorname{Sign}(x)=\left\{\begin{array}{ll}
+1 & x \geq 0 \\
-1 & \text { otherwise }
\end{array}\right.
$$
其中$Q_b(x)$表示二进制向量，$x$为真实值，该方法可以用在量化量化权重和梯度中。但是在反向传播过程中，由于

$\operatorname{Sign}(x)$是离散值的缘故，会使梯度处处为零，因此[Hinton et al., 2012b]提出了一种启发式的方法，可以估计随机神经元的梯度，被称为直通估计（straight through estimator, STE）。假设$E$是损失函数，STE的前向传播和反向计算可以看成如下方式：
$$
\begin{array}{l}
\text { Forward: } \quad Q_b(x)=\operatorname{Sign}(x) \\
\text { Backward: } \frac{\partial E}{\partial x}=\frac{\partial E}{\partial Q_b(x)} \mathrm{I}_{|x| \leq 1}
\end{array}
$$
其中$\mathrm{I}_{|x| \leq 1}$是定义如下的指示函数：
$$
\mathrm{I}_{|x| \leq 1}=\left\{\begin{array}{ll}
1 & |x| \leq 1 \\
0 & \text { otherwise }
\end{array}\right.
$$
为了对双精度进行取整，[Gupta et al., 2015]作者提出了如下的取整方式：
$$
\operatorname{Round}(x,[\mathrm{IL}, \mathrm{FL}])=\left\{\begin{array}{ll}
\lfloor x\rfloor & \text { if }\lfloor x\rfloor \leq x \leq\lfloor x\rfloor+\frac{\epsilon}{2} \\
\lfloor x\rfloor+\epsilon & \text { if }\lfloor x\rfloor+\frac{\epsilon}{2}<x \leq\lfloor x\rfloor+\epsilon
\end{array}\right.
$$
在固定点表达中，IL代表整数位的个数，FL表示分数位的个数。$\epsilon$表示在固定点表达中能够表达的最小正数。$\lfloor x\rfloor$被定义为$\epsilon$的最大整数倍。对于超出此固定点格式范围的值，作者将它们规范化为固定点表示的下界或上界[Rastegari et al., 2016]。将上式扩展：
$$
\begin{array}{ll}
\text { Forward: } & Q_b(x)=\operatorname{Sign}(x) \times \mathrm{E}_{F}(|x|) \\
\text { Backward: } & \frac{\partial E}{\partial x}=\frac{\partial E}{\partial Q_b(x)}
\end{array}
$$
其中$\mathrm{E}_{F}(|x|)$表示每个输出通道的权值绝对值的平均值。

近期[Polino et al., 2018]提出了更加普遍的舍入函数：
$$
Q(x)=s c^{-1}(\hat{Q}(s c(x)))
$$


其中$sc(x)$是将值从任意范围缩放到$[0,1]$的缩放函数。$\hat{Q}(x)$是实际的量化函数。给出量化等级参数$s$，有$s+1$等级的统一量化函数可以定义为：
$$
\hat{Q}(x, s)=\frac{\lfloor x s\rfloor}{s}+\frac{\xi}{s}
$$
其中
$$
\xi=\left\{\begin{array}{ll}
1 & x s-\lfloor x s\rfloor>\frac{1}{2} \\
0 & \text { otherwise }
\end{array}\right.
$$
这个量化函数的直觉是将$x$分配到在$[0,1]$范围内$s-1$个等间隔最接近的量化点。这是符号$Sign(x)$函数的广义版本，能够将实值量化成多层。在[Shuang et al., 2018]中，作者提出了启发式摄入函数来量化一个市值为$k$位的整数。
$$
Q(x, k)=\operatorname{Clip}\left\{\sigma(k) \cdot \operatorname{round}\left[\frac{x}{\sigma(k)}\right],-1+\sigma(k), 1-\sigma(k)\right\}
$$
想法是将真实值利用统一的距离$\sigma(k)$进行量化，其中$\sigma(k)=2^{1-k}$。$Clip$将量化限制在$[-1+\sigma(k), 1-\sigma(k)]$范围内，$round$用最近的离散点替换连续值。

###### 存在问题

使用四舍五入函数是将实值转换为量化值的简单方法。然而，每次四舍五入操作之后，网络性能可能会急剧下降。在训练过程中需要保持真实值作为参考，这会增加记忆开销。同时，由于使用离散值时参数空间要小得多，训练过程难以收敛。最后，舍入运算不能充分利用网络中权值的结构信息。

##### 向量量化

###### 主要内容

[Gong et al., 2014]是第一篇将向量量化考虑到神经网络压缩和量化中的。他主要的思想是将权重分组聚类，在推理时采用聚类中心代表每个组实际的权重。

对于权重矩阵$W \in R^{m \times n}$，可以将其建模成为`k-means`聚类算法将其进行向量压缩：
$$
\min \sum_{i}^{m} \sum_{j}^{n} \sum_{k}^{l}\left\|w_{i j}-c_{k}\right\|_{2}^{2}
$$
其中$c_k$是质心，完成聚类后，每个权重都指向质心索引。

[Han et al., 2015]，[Gong et al., 2014]等都对这种方式进行了改进，[Choi et al., 2016]指出这种方法有两个缺点，第一是不能够控制由于`k-means`算法造成的损失；第二是`k-means`算法不施加任何压缩比约束。为了解决这些问题，作者提出了一种Hessian加权k均值聚类方法。其基本思想是使用Hessian Weighted失真来测量因权值量化而导致的性能退化。这样可以防止那些对网络性能有较大影响的权值与原始值偏离太多。

有很多对向量量化的扩展方法，乘积量化[Gong et al., 2014]是一种将权重矩阵划分为许多不相交的子矩阵，并对每个子矩阵进行量化的方法。在[Wu et al., 2016]中，作者采用带误差修正的产品量化方法对网络参数进行量化，实现快速训练和测试。残差量化[Gong et al., 2014]将向量量化到k个聚类中，然后递归量化残差。在[Park et al., 2017]中，作者采用了类似于矢量量化的方法。他们使用了一个基于权重熵的想法[Guias¸u, 1971]来将权重分组到N个簇中。对于重要的权重范围有更多的簇。从而实现了自动灵活的多比特量化。

###### 存在问题

由于网络中权值的数量，k-means聚类的计算量很大。与四舍五入法相比，用向量化方法来实现二值权值比较困难。向量量化通常用于对预先训练的模型进行量化。因此，如果任务是从头训练量化网络，最好使用精心设计的四舍五入函数。向量量化忽略了网络的局部信息。

##### 量化最优化

简单来说就是将量化作为最优化问题进行，例如[Rastegari et al., 2016]，作者将真实值过滤器的量化转化如下优化问题：
$$
J(B, \alpha)=\|W-\alpha B\|^{2}
$$
其中$W$代表真实值过滤器，$B$是二进制过滤器，$\alpha$是一个正扩展因子，最优化$B$和$\alpha$通过下式给出：
$$
B^{*}=\operatorname{Sign}(W), \quad \alpha^{*}=\frac{1}{n}\|W\|_{l_{1}}
$$
$n$表示过滤器中元素数量。

将量化作为最优化问题成果还包括如下研究[Li et al., 2016]，它改变了约束条件，将二元约束放宽为三元约束；[Zhu et al., 2016]，[Hou et al., 2016]，[Carreira-Perpin´an and Idelbayev, 2017]，[Hou et al., 2016]也都展开了这方面的研究。





